{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct way to open:\n",
    "1. ubuntu promt\n",
    "2. cd /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/python\\ code\n",
    "3. code ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapbox api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pk.eyJ1Ijoiam9obm55Ym95NjY5NyIsImEiOiJjbTgyMzZkbjUxZHF2MmlzYTByc3pxZmw0In0.7RASB6M_AczC7q8dvFPWBQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taipei_urban_masterplan.geojson:\n",
    "#{\n",
    "#\"type\": \"FeatureCollection\",\n",
    "#\"name\": \"Taipei_urban_masterplan\",\n",
    "#\"crs\": { \"type\": \"name\", \"properties\": { \"name\": \"urn:ogc:def:crs:OGC:1.3:CRS84\" } },\n",
    "#\"features\": [\n",
    "#{ \"type\": \"Feature\", \"properties\": { \"編號\": \"1\", \"圖層\": \"32\", \"顏色\": \"19\", \"街廓編號\": null, \"分區代碼\": \"PEA\", \"分區簡稱\": \"公\", \"使用分區\": \"公園用地\", \"分區說明\": null, \"原屬分區\": null, \"變更前代碼\": null, \"變更前簡稱\": null, \"變更前分區\": null, \"Category\": \"City_Open_Area\", \"Area\": 823.19 }, \"geometry\": { \"type\": \"MultiPolygon\", \"coordinates\": [ [ [ [ 121.528633581995919, 25.047796387544285 ], [ 121.528533470499838, 25.048123199000166 ], [ 121.528553569927254, 25.048141923941568 ], [ 121.528906514957143, 25.048059586583932 ], [ 121.528919131953018, 25.048015927027393 ], [ 121.528876127568978, 25.047978288611517 ], [ 121.528844065787951, 25.047950226128741 ], [ 121.528706848633234, 25.047828447317585 ], [ 121.528662044074167, 25.047789289499065 ], [ 121.528633581995919, 25.047796387544285 ] ] ] ] } },\n",
    "#the data I want to present in kepler are \"Category\" & \"Area\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, LineString\n",
    "from shapely.ops import unary_union, linemerge\n",
    "import cugraph\n",
    "import cudf  # Add this import\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import hashlib\n",
    "from keplergl import KeplerGl\n",
    "import json\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "CHECKPOINT_DIR = \"/home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/checkpoints\"\n",
    "SUBGRAPH_DIR = \"/home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/Neighborhood_subgraph\"\n",
    "CONFIG_PATH = \"/home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/kepler.gl.json\"\n",
    "TEMP_HTML_PATH = \"/home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/temp_walkability_map.html\"\n",
    "URBAN_MASTERPLAN_PATH = \"/home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/Taipei_urban_masterplan.geojson\"\n",
    "MAPBOX_ACCESS_TOKEN = \"pk.eyJ1Ijoiam9obm55Ym95NjY5NyIsImEiOiJjbTgyMzZkbjUxZHF2MmlzYTByc3pxZmw0In0.7RASB6M_AczC7q8dvFPWBQ\"\n",
    "WORKING_CRS = 'EPSG:3826'  # TWD97 / TM2 zone 121 (meters)\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(SUBGRAPH_DIR, exist_ok=True)\n",
    "\n",
    "def validate_geometry(gdf, name):\n",
    "    \"\"\"Validate and repair geometries in a GeoDataFrame.\"\"\"\n",
    "    print(f\"Validating geometries for {name}...\")\n",
    "    gdf['geometry'] = gdf.geometry.make_valid()\n",
    "    invalid = gdf[~gdf.geometry.is_valid]\n",
    "    if not invalid.empty:\n",
    "        print(f\"Found {len(invalid)} invalid geometries in {name}. Attempting to repair...\")\n",
    "        gdf = gdf[gdf.geometry.is_valid]\n",
    "    gdf = gdf[~gdf.geometry.is_empty]\n",
    "    print(f\"{name} after validation: {len(gdf)} rows\")\n",
    "    return gdf\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and preprocess all geospatial datasets.\"\"\"\n",
    "    print(\"Checkpoint 1: Loading and preparing data...\")\n",
    "\n",
    "    # Define file paths\n",
    "    paths = {\n",
    "        'neighborhoods': \"/home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/neighborhoods_with_ndvi_numerical.geojson\",\n",
    "        'buildings': \"/home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/Taipei_Buildings_fulldata.geojson\",\n",
    "        'roads': \"/home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/taipei_segments_cleaned_verified.geoparquet\",\n",
    "        'trees': \"/home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/taipei_land.geoparquet\",\n",
    "        'transit': \"/home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/taipei_infrastructure.geoparquet\",\n",
    "        'urban_masterplan': URBAN_MASTERPLAN_PATH\n",
    "    }\n",
    "\n",
    "    # Load datasets\n",
    "    data = {}\n",
    "    for name, path in tqdm(paths.items(), desc=\"Loading files\"):\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"File not found: {path}\")\n",
    "        if path.endswith('.geojson'):\n",
    "            gdf = gpd.read_file(path, encoding='utf-8-sig')\n",
    "        else:\n",
    "            gdf = gpd.read_parquet(path)\n",
    "        gdf = gdf.to_crs(WORKING_CRS)\n",
    "        gdf = validate_geometry(gdf, name)\n",
    "        data[name] = gdf\n",
    "\n",
    "    # Clean and preprocess specific datasets\n",
    "    # Trees: Filter for trees and convert polylines to points\n",
    "    trees_gdf = data['trees']\n",
    "    trees_gdf = trees_gdf[trees_gdf['subtype'] == 'tree']\n",
    "    tree_points = trees_gdf[trees_gdf.geometry.geom_type == 'Point']\n",
    "    tree_polylines = trees_gdf[trees_gdf.geometry.geom_type.isin(['LineString', 'MultiLineString'])]\n",
    "    \n",
    "    converted_points = []\n",
    "    for geom in tree_polylines.geometry:\n",
    "        if geom.geom_type == 'MultiLineString':\n",
    "            geom = linemerge(geom)\n",
    "        if geom.geom_type != 'LineString' or geom.length <= 0:\n",
    "            continue\n",
    "        distance = 0\n",
    "        while distance <= geom.length:\n",
    "            point = geom.interpolate(distance)\n",
    "            converted_points.append({'geometry': point})\n",
    "            distance += 7  # Spacing of 7 meters\n",
    "    if converted_points:\n",
    "        converted_points_gdf = gpd.GeoDataFrame(converted_points, crs=WORKING_CRS)\n",
    "        all_trees = pd.concat([tree_points.geometry, converted_points_gdf.geometry], ignore_index=True)\n",
    "    else:\n",
    "        all_trees = tree_points.geometry\n",
    "    data['trees'] = gpd.GeoDataFrame(geometry=all_trees, crs=WORKING_CRS)\n",
    "    print(f\"Total number of trees after processing: {len(data['trees'])}\")\n",
    "\n",
    "    # Transit: Filter for bus stops and points only\n",
    "    transit_gdf = data['transit']\n",
    "    transit_gdf = transit_gdf[transit_gdf['class'].isin(['stop_position', 'bus_stop'])]\n",
    "    transit_gdf = transit_gdf[transit_gdf.geometry.geom_type == 'Point']\n",
    "    transit_gdf = transit_gdf[~transit_gdf.geometry.isna()]\n",
    "    data['transit'] = transit_gdf\n",
    "    print(f\"Total number of transit points after filtering: {len(data['transit'])}\")\n",
    "\n",
    "    # Buildings: Clean building types\n",
    "    buildings_gdf = data['buildings']\n",
    "    buildings_gdf['area_m2'] = buildings_gdf.geometry.area.round(1)\n",
    "    buildings_gdf['building'] = buildings_gdf['building'].fillna('unknown').replace('yes', 'unknown')\n",
    "    data['buildings'] = buildings_gdf\n",
    "\n",
    "    # Roads: Add length\n",
    "    roads_gdf = data['roads']\n",
    "    roads_gdf['length_m'] = roads_gdf.geometry.length.round(1)\n",
    "    data['roads'] = roads_gdf\n",
    "\n",
    "    return data\n",
    "\n",
    "def compute_data_hash(gdf):\n",
    "    \"\"\"Compute a hash of a GeoDataFrame for caching.\"\"\"\n",
    "    df_hashable = gdf.drop(columns=['geometry']).astype(str).fillna('missing')\n",
    "    return hashlib.md5(pd.util.hash_pandas_object(df_hashable).values.tobytes()).hexdigest()\n",
    "\n",
    "def calculate_neighborhood_features(neighborhoods_gdf, trees_gdf, transit_gdf, urban_masterplan_gdf):\n",
    "    \"\"\"Calculate features for each neighborhood (tree count, transit count, land use).\"\"\"\n",
    "    print(\"Calculating neighborhood features...\")\n",
    "\n",
    "    # Initialize feature columns\n",
    "    neighborhoods_gdf['tree_count'] = 0\n",
    "    neighborhoods_gdf['transit_count'] = 0\n",
    "    unique_categories = urban_masterplan_gdf['Category'].unique()\n",
    "    for category in unique_categories:\n",
    "        attr_name_m2 = f\"land_use_{category.lower().replace('_', '_')}_m2\"\n",
    "        attr_name_percent = f\"land_use_{category.lower().replace('_', '_')}_percent\"\n",
    "        neighborhoods_gdf[attr_name_m2] = 0.0\n",
    "        neighborhoods_gdf[attr_name_percent] = 0.0\n",
    "\n",
    "    # Cache setup\n",
    "    cache_dir = CHECKPOINT_DIR\n",
    "    urban_seg_cache_path = os.path.join(cache_dir, \"urban_seg_selected_neighborhoods.pkl\")\n",
    "    urban_seg_dict_path = os.path.join(cache_dir, \"urban_seg_dict.pkl\")\n",
    "    urban_seg_hash_path = os.path.join(cache_dir, \"urban_seg_hash.txt\")\n",
    "\n",
    "    input_data_hash = compute_data_hash(urban_masterplan_gdf) + compute_data_hash(neighborhoods_gdf)\n",
    "\n",
    "    cached_results_exist = False\n",
    "    if os.path.exists(urban_seg_cache_path) and os.path.exists(urban_seg_dict_path) and os.path.exists(urban_seg_hash_path):\n",
    "        with open(urban_seg_hash_path, 'r') as f:\n",
    "            cached_hash = f.read()\n",
    "        if cached_hash == input_data_hash:\n",
    "            with open(urban_seg_cache_path, 'rb') as f:\n",
    "                neighborhoods_gdf = pickle.load(f)\n",
    "            with open(urban_seg_dict_path, 'rb') as f:\n",
    "                urban_plan_to_neighborhoods = pickle.load(f)\n",
    "            print(\"Loaded cached urban masterplan segregation results.\")\n",
    "            cached_results_exist = True\n",
    "\n",
    "    if not cached_results_exist:\n",
    "        # Calculate tree and transit counts using spatial join\n",
    "        print(\"Counting trees per neighborhood...\")\n",
    "        trees_joined = gpd.sjoin(trees_gdf, neighborhoods_gdf, how='left', predicate='within')\n",
    "        tree_counts = trees_joined.groupby('index_right').size()\n",
    "        neighborhoods_gdf['tree_count'] = neighborhoods_gdf.index.map(tree_counts).fillna(0).astype(int)\n",
    "\n",
    "        print(\"Counting transit points per neighborhood...\")\n",
    "        transit_joined = gpd.sjoin(transit_gdf, neighborhoods_gdf, how='left', predicate='within')\n",
    "        transit_counts = transit_joined.groupby('index_right').size()\n",
    "        neighborhoods_gdf['transit_count'] = neighborhoods_gdf.index.map(transit_counts).fillna(0).astype(int)\n",
    "\n",
    "        # Calculate land use areas\n",
    "        urban_plan_to_neighborhoods = {}\n",
    "        for idx in tqdm(range(len(urban_masterplan_gdf)), desc=\"Segregating urban masterplan\"):\n",
    "            urban_plan_to_neighborhoods[idx] = []\n",
    "            masterplan_row = urban_masterplan_gdf.iloc[idx]\n",
    "            masterplan_geom = masterplan_row.geometry\n",
    "            masterplan_gdf = gpd.GeoDataFrame([masterplan_row], geometry=[masterplan_geom], crs=WORKING_CRS)\n",
    "\n",
    "            intersections = gpd.overlay(neighborhoods_gdf, masterplan_gdf, how='intersection', keep_geom_type=False)\n",
    "            if intersections.empty:\n",
    "                continue\n",
    "\n",
    "            # Compute the masterplan area directly from the geometry (in square meters)\n",
    "            masterplan_area = masterplan_geom.area\n",
    "            if masterplan_area <= 0:\n",
    "                continue\n",
    "\n",
    "            for _, intersection_row in intersections.iterrows():\n",
    "                neigh_idx = intersection_row.name\n",
    "                intersection_geom = intersection_row.geometry\n",
    "                if intersection_geom.is_empty:\n",
    "                    continue\n",
    "\n",
    "                intersection_area = intersection_geom.area\n",
    "                if intersection_area > 0:\n",
    "                    category = masterplan_row['Category']\n",
    "                    # Prorate based on geometry areas (both in square meters)\n",
    "                    prorated_area = (intersection_area / masterplan_area) * masterplan_area\n",
    "                    attr_name_m2 = f\"land_use_{category.lower().replace('_', '_')}_m2\"\n",
    "                    neighborhoods_gdf.at[neigh_idx, attr_name_m2] += prorated_area\n",
    "                    urban_plan_to_neighborhoods[idx].append((neighborhoods_gdf['LIE_NAME'].iloc[neigh_idx], intersection_area))\n",
    "\n",
    "        # Calculate total allocated area per neighborhood to normalize percentages\n",
    "        total_allocated_area = pd.Series(0.0, index=neighborhoods_gdf.index)\n",
    "        for category in unique_categories:\n",
    "            attr_name_m2 = f\"land_use_{category.lower().replace('_', '_')}_m2\"\n",
    "            total_allocated_area += neighborhoods_gdf[attr_name_m2]\n",
    "\n",
    "        # Calculate percentages, normalizing to ensure they sum to 100%\n",
    "        neighborhood_areas = neighborhoods_gdf.geometry.area\n",
    "        for category in unique_categories:\n",
    "            attr_name_m2 = f\"land_use_{category.lower().replace('_', '_')}_m2\"\n",
    "            attr_name_percent = f\"land_use_{category.lower().replace('_', '_')}_percent\"\n",
    "            # First calculate raw percentage based on neighborhood area\n",
    "            raw_percent = (neighborhoods_gdf[attr_name_m2] / neighborhood_areas * 100).round(4)\n",
    "            # Normalize by the total allocated area\n",
    "            normalized_percent = (neighborhoods_gdf[attr_name_m2] / total_allocated_area * 100).round(4)\n",
    "            # Use raw percentage if total allocated area is less than neighborhood area, otherwise normalize\n",
    "            neighborhoods_gdf[attr_name_percent] = np.where(\n",
    "                total_allocated_area <= neighborhood_areas,\n",
    "                raw_percent,\n",
    "                normalized_percent\n",
    "            )\n",
    "            neighborhoods_gdf[attr_name_percent] = neighborhoods_gdf[attr_name_percent].where(neighborhood_areas > 0, 0)\n",
    "\n",
    "        # Cache results\n",
    "        with open(urban_seg_cache_path, 'wb') as f:\n",
    "            pickle.dump(neighborhoods_gdf, f)\n",
    "        with open(urban_seg_dict_path, 'wb') as f:\n",
    "            pickle.dump(urban_plan_to_neighborhoods, f)\n",
    "        with open(urban_seg_hash_path, 'w') as f:\n",
    "            f.write(input_data_hash)\n",
    "        print(\"Cached urban masterplan segregation results.\")\n",
    "\n",
    "    # Print results\n",
    "    print(\"Tree, transit, and urban masterplan counts per neighborhood:\")\n",
    "    for idx, row in neighborhoods_gdf.iterrows():\n",
    "        print(f\"- {row['LIE_NAME']}: {row['tree_count']} trees, {row['transit_count']} transit points\")\n",
    "        for category in unique_categories:\n",
    "            attr_name_m2 = f\"land_use_{category.lower().replace('_', '_')}_m2\"\n",
    "            attr_name_percent = f\"land_use_{category.lower().replace('_', '_')}_percent\"\n",
    "            print(f\"  - {category} Area: {row[attr_name_m2]:.2f}m² ({row[attr_name_percent]:.4f}%)\")\n",
    "\n",
    "    return neighborhoods_gdf, urban_plan_to_neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(data, urban_plan_to_neighborhoods):\n",
    "    print(\"Checkpoint 2: Building graph network...\")\n",
    "    neighborhoods_gdf = data['neighborhoods']\n",
    "    buildings_gdf = data['buildings']\n",
    "    roads_gdf = data['roads']\n",
    "    trees_gdf = data['trees']\n",
    "    transit_gdf = data['transit']\n",
    "    urban_masterplan_gdf = data['urban_masterplan']\n",
    "\n",
    "    subgraphs = {}\n",
    "    road_network = cugraph.Graph(directed=False)\n",
    "    road_network_nodes = []\n",
    "\n",
    "    # Define road class weights outside the loop\n",
    "    road_class_weights = {\n",
    "        'footway': 0.5, 'pedestrian': 0.5, 'cycleway': 0.6, 'steps': 0.6, 'living_street': 0.6,\n",
    "        'path': 0.7, 'track': 0.7, 'residential': 0.8, 'service': 1.0, 'unclassified': 1.2,\n",
    "        'tertiary': 1.5, 'secondary': 2.0, 'primary': 2.5, 'highway': 3.5, 'motorway': 2.5,\n",
    "        'trunk': 2.5, 'unknown': 1.0\n",
    "    }\n",
    "\n",
    "    # Checkpoint file to track progress\n",
    "    checkpoint_file = os.path.join(CHECKPOINT_DIR, \"build_graph_checkpoint.txt\")\n",
    "    completed_indices = set()\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            completed_indices = set(int(line.strip()) for line in f if line.strip().isdigit())\n",
    "        print(f\"Loaded {len(completed_indices)} completed neighborhoods from checkpoint.\")\n",
    "\n",
    "    # Check for existing .pkl files and determine which need processing\n",
    "    to_process = []\n",
    "    for idx in range(len(neighborhoods_gdf)):\n",
    "        lie_name = neighborhoods_gdf['LIE_NAME'].iloc[idx]\n",
    "        subgraph_path = os.path.join(SUBGRAPH_DIR, f\"subgraph_{lie_name}.pkl\")\n",
    "        if idx in completed_indices and os.path.exists(subgraph_path):\n",
    "            with open(subgraph_path, 'rb') as f:\n",
    "                subgraph_data = pickle.load(f)\n",
    "                subgraphs[lie_name] = subgraph_data\n",
    "                # Extract road nodes from loaded subgraph\n",
    "                nodes_df = subgraph_data['nodes'].to_pandas()\n",
    "                road_nodes_df = nodes_df[nodes_df['type'] == 'road']\n",
    "                for _, row in road_nodes_df.iterrows():\n",
    "                    road_network_nodes.append({\n",
    "                        'vertex': row['vertex'],\n",
    "                        'type': 'road',\n",
    "                        'road_class': row['road_class'],\n",
    "                        'length_m': row['length_m']\n",
    "                    })\n",
    "            print(f\"Loaded existing subgraph for {lie_name} from {subgraph_path}\")\n",
    "        else:\n",
    "            to_process.append(idx)\n",
    "\n",
    "    # Process missing or incomplete neighborhoods\n",
    "    for idx in tqdm(to_process, desc=\"Processing neighborhoods\", total=len(to_process)):\n",
    "        lie_name = neighborhoods_gdf['LIE_NAME'].iloc[idx]\n",
    "        G_sub = cugraph.Graph(directed=False)\n",
    "        all_nodes = []\n",
    "        all_edges = []\n",
    "\n",
    "        # Neighborhood node\n",
    "        node_id = f\"neighborhood_{lie_name}\"\n",
    "        node_data = {\n",
    "            'vertex': node_id,\n",
    "            'type': 'neighborhood',\n",
    "            'lie_name': lie_name,\n",
    "            'sect_name': neighborhoods_gdf['SECT_NAME'].iloc[idx],\n",
    "            'population': neighborhoods_gdf['2024population'].iloc[idx],\n",
    "            'land_use_residential_percent': neighborhoods_gdf['land_use_residential_percent'].iloc[idx],\n",
    "            'land_use_commercial_percent': neighborhoods_gdf['land_use_commercial_percent'].iloc[idx],\n",
    "            'land_use_education_percent': neighborhoods_gdf['land_use_education_percent'].iloc[idx],\n",
    "            'ndvi_mean': neighborhoods_gdf['ndvi_mean'].iloc[idx],\n",
    "            'ndvi_median': neighborhoods_gdf['ndvi_median'].iloc[idx],\n",
    "            'tree_count': neighborhoods_gdf['tree_count'].iloc[idx],\n",
    "            'transit_count': neighborhoods_gdf['transit_count'].iloc[idx]\n",
    "        }\n",
    "        unique_categories = urban_masterplan_gdf['Category'].unique()\n",
    "        for category in unique_categories:\n",
    "            attr_name_m2 = f\"land_use_{category.lower().replace('_', '_')}_m2\"\n",
    "            attr_name_percent = f\"land_use_{category.lower().replace('_', '_')}_percent\"\n",
    "            node_data[attr_name_m2] = neighborhoods_gdf[attr_name_m2].iloc[idx]\n",
    "            node_data[attr_name_percent] = neighborhoods_gdf[attr_name_percent].iloc[idx]\n",
    "        all_nodes.append(node_data)\n",
    "\n",
    "        # Buffer and spatial filtering\n",
    "        buffer_distance = 200\n",
    "        neigh_geom = neighborhoods_gdf.geometry.iloc[idx]\n",
    "        neigh_buffer = neigh_geom.buffer(buffer_distance)\n",
    "\n",
    "        relevant_buildings = buildings_gdf[buildings_gdf.geometry.within(neigh_buffer)]\n",
    "        relevant_roads = roads_gdf[roads_gdf.geometry.intersects(neigh_buffer)]\n",
    "        relevant_trees = trees_gdf[trees_gdf.geometry.within(neigh_buffer)]\n",
    "        relevant_transit = transit_gdf[transit_gdf.geometry.within(neigh_buffer)]\n",
    "\n",
    "        # Building nodes\n",
    "        building_nodes = {}\n",
    "        for b_idx, building in relevant_buildings.iterrows():\n",
    "            node_id = f\"building_{b_idx}\"\n",
    "            building_data = {\n",
    "                'vertex': node_id,\n",
    "                'type': 'building',\n",
    "                'building_type': building['building'],\n",
    "                'area_m2': building['area_m2'],\n",
    "                'age': building.get('屋齡', '<NA>'),\n",
    "                'height': building.get('建物高度', '<NA>'),\n",
    "                'floors': building.get('地上層數', '<NA>'),\n",
    "                'structure_type': building.get('構造種類', 'Unknown'),\n",
    "                'usage_zone': building.get('使用分區', 'Unknown')\n",
    "            }\n",
    "            all_nodes.append(building_data)\n",
    "            building_nodes[node_id] = building.geometry\n",
    "\n",
    "        # Road nodes\n",
    "        road_nodes = {}\n",
    "        road_points = []\n",
    "        for r_idx, road in relevant_roads.iterrows():\n",
    "            geom = road.geometry\n",
    "            if geom.geom_type == 'LineString':\n",
    "                start_point = Point(geom.coords[0])\n",
    "                end_point = Point(geom.coords[-1])\n",
    "                road_points.extend([(f\"road_start_{r_idx}\", start_point), (f\"road_end_{r_idx}\", end_point)])\n",
    "            elif geom.geom_type == 'MultiLineString':\n",
    "                for i, line in enumerate(geom.geoms):\n",
    "                    start_point = Point(line.coords[0])\n",
    "                    end_point = Point(line.coords[-1])\n",
    "                    road_points.extend([(f\"road_start_{r_idx}_{i}\", start_point), (f\"road_end_{r_idx}_{i}\", end_point)])\n",
    "\n",
    "        for node_id, geom in road_points:\n",
    "            r_idx = int(node_id.split('_')[2])\n",
    "            road_data = {\n",
    "                'vertex': node_id,\n",
    "                'type': 'road',\n",
    "                'road_class': roads_gdf['class'].iloc[r_idx] if pd.notna(roads_gdf['class'].iloc[r_idx]) else 'unknown',\n",
    "                'length_m': roads_gdf['length_m'].iloc[r_idx]\n",
    "            }\n",
    "            all_nodes.append(road_data)\n",
    "            road_nodes[node_id] = geom\n",
    "            road_network_nodes.append(road_data)\n",
    "\n",
    "        # Tree nodes\n",
    "        tree_nodes = {}\n",
    "        for t_idx, tree in relevant_trees.iterrows():\n",
    "            node_id = f\"tree_{t_idx}\"\n",
    "            tree_data = {'vertex': node_id, 'type': 'tree'}\n",
    "            all_nodes.append(tree_data)\n",
    "            tree_nodes[node_id] = tree.geometry\n",
    "\n",
    "        # Transit nodes\n",
    "        transit_nodes = {}\n",
    "        for t_idx, transit in relevant_transit.iterrows():\n",
    "            node_id = f\"transit_{t_idx}\"\n",
    "            transit_data = {\n",
    "                'vertex': node_id,\n",
    "                'type': 'transit',\n",
    "                'class': transit['class']\n",
    "            }\n",
    "            all_nodes.append(transit_data)\n",
    "            transit_nodes[node_id] = transit.geometry\n",
    "\n",
    "        # Urban plan nodes and edges\n",
    "        urban_plan_nodes = {}\n",
    "        for urban_idx in urban_plan_to_neighborhoods:\n",
    "            if idx in [neigh_idx for _, neigh_idx in urban_plan_to_neighborhoods[urban_idx]]:\n",
    "                node_id = f\"urban_plan_{urban_idx}\"\n",
    "                urban_plan_data = {\n",
    "                    'vertex': node_id,\n",
    "                    'type': 'urban_plan',\n",
    "                    'category': urban_masterplan_gdf['Category'].iloc[urban_idx],\n",
    "                    'area': urban_masterplan_gdf.geometry.iloc[urban_idx].area\n",
    "                }\n",
    "                all_nodes.append(urban_plan_data)\n",
    "                urban_plan_nodes[node_id] = urban_masterplan_gdf.geometry.iloc[urban_idx]\n",
    "                distance = neigh_geom.centroid.distance(urban_masterplan_gdf.geometry.iloc[urban_idx].centroid)\n",
    "                all_edges.append({'src': node_id, 'dst': f\"neighborhood_{lie_name}\", 'weight': distance, 'type': 'urban_plan'})\n",
    "                all_edges.append({'src': f\"neighborhood_{lie_name}\", 'dst': node_id, 'weight': distance, 'type': 'urban_plan'})\n",
    "\n",
    "                urban_plan_buffer = urban_masterplan_gdf.geometry.iloc[urban_idx].buffer(50)\n",
    "                for building_node, build_geom in building_nodes.items():\n",
    "                    if build_geom.within(urban_plan_buffer):\n",
    "                        distance = urban_masterplan_gdf.geometry.iloc[urban_idx].centroid.distance(build_geom)\n",
    "                        all_edges.append({'src': node_id, 'dst': building_node, 'weight': distance, 'type': 'urban_plan_to_building'})\n",
    "                        all_edges.append({'src': building_node, 'dst': node_id, 'weight': distance, 'type': 'urban_plan_to_building'})\n",
    "                for road_node, road_geom in road_nodes.items():\n",
    "                    if road_geom.within(urban_plan_buffer):\n",
    "                        distance = urban_masterplan_gdf.geometry.iloc[urban_idx].centroid.distance(road_geom)\n",
    "                        all_edges.append({'src': node_id, 'dst': road_node, 'weight': distance, 'type': 'urban_plan_to_road'})\n",
    "                        all_edges.append({'src': road_node, 'dst': node_id, 'weight': distance, 'type': 'urban_plan_to_road'})\n",
    "                for tree_node, tree_geom in tree_nodes.items():\n",
    "                    if tree_geom.within(urban_plan_buffer):\n",
    "                        distance = urban_masterplan_gdf.geometry.iloc[urban_idx].centroid.distance(tree_geom)\n",
    "                        all_edges.append({'src': node_id, 'dst': tree_node, 'weight': distance, 'type': 'urban_plan_to_tree'})\n",
    "                        all_edges.append({'src': tree_node, 'dst': node_id, 'weight': distance, 'type': 'urban_plan_to_tree'})\n",
    "                for transit_node, transit_geom in transit_nodes.items():\n",
    "                    if transit_geom.within(urban_plan_buffer):\n",
    "                        distance = urban_masterplan_gdf.geometry.iloc[urban_idx].centroid.distance(transit_geom)\n",
    "                        all_edges.append({'src': node_id, 'dst': transit_node, 'weight': distance, 'type': 'urban_plan_to_transit'})\n",
    "                        all_edges.append({'src': transit_node, 'dst': node_id, 'weight': distance, 'type': 'urban_plan_to_transit'})\n",
    "\n",
    "        # Neighborhood edges\n",
    "        neigh_node = f\"neighborhood_{lie_name}\"\n",
    "        for building_node, build_geom in building_nodes.items():\n",
    "            distance = neigh_geom.distance(build_geom)\n",
    "            all_edges.append({'src': neigh_node, 'dst': building_node, 'weight': distance, 'type': 'walk'})\n",
    "            all_edges.append({'src': building_node, 'dst': neigh_node, 'weight': distance, 'type': 'walk'})\n",
    "\n",
    "        for road_node, road_geom in road_nodes.items():\n",
    "            distance = neigh_geom.distance(road_geom)\n",
    "            all_edges.append({'src': neigh_node, 'dst': road_node, 'weight': distance, 'type': 'walk'})\n",
    "            all_edges.append({'src': road_node, 'dst': neigh_node, 'weight': distance, 'type': 'walk'})\n",
    "\n",
    "        for building_node, build_geom in building_nodes.items():\n",
    "            build_buffer = build_geom.buffer(50)\n",
    "            for road_node, road_geom in road_nodes.items():\n",
    "                if road_geom.within(build_buffer):\n",
    "                    distance = build_geom.distance(road_geom)\n",
    "                    all_edges.append({'src': building_node, 'dst': road_node, 'weight': distance, 'type': 'walk'})\n",
    "                    all_edges.append({'src': road_node, 'dst': building_node, 'weight': distance, 'type': 'walk'})\n",
    "\n",
    "        for tree_node, tree_geom in tree_nodes.items():\n",
    "            distance = neigh_geom.distance(tree_geom)\n",
    "            all_edges.append({'src': neigh_node, 'dst': tree_node, 'weight': distance, 'type': 'natural'})\n",
    "            all_edges.append({'src': tree_node, 'dst': neigh_node, 'weight': distance, 'type': 'natural'})\n",
    "\n",
    "        for transit_node, transit_geom in transit_nodes.items():\n",
    "            distance = neigh_geom.distance(transit_geom)\n",
    "            all_edges.append({'src': neigh_node, 'dst': transit_node, 'weight': distance, 'type': 'transit'})\n",
    "            all_edges.append({'src': transit_node, 'dst': neigh_node, 'weight': distance, 'type': 'transit'})\n",
    "\n",
    "        # Road network edges within the loop\n",
    "        tolerance = 10\n",
    "        road_node_list = list(road_nodes.items())\n",
    "        for i, (node1_id, geom1) in enumerate(road_node_list):\n",
    "            for j, (node2_id, geom2) in enumerate(road_node_list[i+1:], start=i+1):\n",
    "                if geom1.distance(geom2) <= tolerance:\n",
    "                    road1_idx = int(node1_id.split('_')[2])\n",
    "                    road2_idx = int(node2_id.split('_')[2])\n",
    "                    class1 = roads_gdf['class'].iloc[road1_idx] if pd.notna(roads_gdf['class'].iloc[road1_idx]) else 'unknown'\n",
    "                    class2 = roads_gdf['class'].iloc[road2_idx] if pd.notna(roads_gdf['class'].iloc[road2_idx]) else 'unknown'\n",
    "                    weight1 = road_class_weights.get(class1, road_class_weights['unknown'])\n",
    "                    weight2 = road_class_weights.get(class2, road_class_weights['unknown'])\n",
    "                    distance = geom1.distance(geom2)\n",
    "                    weight = distance * (weight1 + weight2) / 2\n",
    "                    all_edges.append({'src': node1_id, 'dst': node2_id, 'weight': weight, 'type': 'road'})\n",
    "                    all_edges.append({'src': node2_id, 'dst': node1_id, 'weight': weight, 'type': 'road'})\n",
    "\n",
    "        # Convert to DataFrames\n",
    "        nodes_df = cudf.from_pandas(pd.DataFrame(all_nodes))\n",
    "        edges_df = cudf.from_pandas(pd.DataFrame(all_edges))\n",
    "\n",
    "        print(f\"Edges DataFrame for {lie_name} before initializing graph:\")\n",
    "        print(f\"Shape: {edges_df.shape}\")\n",
    "        print(f\"Columns: {edges_df.columns.tolist()}\")\n",
    "        print(f\"Any NaN in edges_df:\\n{edges_df.isna().any()}\")\n",
    "        edges_df = edges_df.dropna()\n",
    "        edges_df = edges_df.drop_duplicates()\n",
    "        print(f\"Shape after cleaning: {edges_df.shape}\")\n",
    "\n",
    "        # Graph construction\n",
    "        if not edges_df.empty:\n",
    "            G_sub.from_cudf_edgelist(edges_df, source='src', destination='dst', edge_attr='weight', symmetrize=False)\n",
    "            nodes_from_graph = G_sub.nodes()\n",
    "            if isinstance(nodes_from_graph, cudf.Series):\n",
    "                nodes_from_graph = cudf.DataFrame({'vertex': nodes_from_graph})\n",
    "            nodes_from_graph_pandas = nodes_from_graph.to_pandas()\n",
    "            G_sub_nodes = nodes_from_graph_pandas.merge(pd.DataFrame(all_nodes), on='vertex', how='left')\n",
    "            G_sub._nodes = cudf.from_pandas(G_sub_nodes)\n",
    "        else:\n",
    "            G_sub._nodes = nodes_df\n",
    "\n",
    "        # Save subgraph data using original edges_df to preserve 'weight'\n",
    "        subgraph_data = {\n",
    "            'nodes': G_sub._nodes,\n",
    "            'edges': edges_df  # Use edges_df instead of G_sub.edgelist.edgelist_df\n",
    "        }\n",
    "        subgraphs[lie_name] = subgraph_data\n",
    "        print(f\"Subgraph for {lie_name}: {G_sub.number_of_nodes()} nodes, {G_sub.number_of_edges()} edges\")\n",
    "\n",
    "        subgraph_path = os.path.join(SUBGRAPH_DIR, f\"subgraph_{lie_name}.pkl\")\n",
    "        with open(subgraph_path, 'wb') as f:\n",
    "            pickle.dump(subgraph_data, f)\n",
    "        \n",
    "        # Update checkpoint\n",
    "        with open(checkpoint_file, 'a') as f:\n",
    "            f.write(f\"{idx}\\n\")\n",
    "        print(f\"Checkpoint saved for {lie_name} at index {idx}\")\n",
    "\n",
    "    # Road network construction with progress bar\n",
    "    print(\"Building road network for inter-neighborhood connections...\")\n",
    "    tolerance = 10\n",
    "    road_network_edges = []\n",
    "\n",
    "    # Deduplicate road_network_nodes based on 'vertex'\n",
    "    if road_network_nodes:\n",
    "        # Convert to DataFrame and drop duplicates\n",
    "        road_nodes_df = pd.DataFrame(road_network_nodes).drop_duplicates(subset='vertex')\n",
    "        print(f\"Number of unique road nodes after deduplication: {len(road_nodes_df)}\")\n",
    "        road_node_list = road_nodes_df.to_dict('records')\n",
    "        total_nodes = len(road_node_list)\n",
    "        # Use tqdm for the outer loop to show progress and estimated time\n",
    "        for i in tqdm(range(total_nodes), desc=\"Building road network edges\", total=total_nodes):\n",
    "            node1 = road_node_list[i]\n",
    "            for j in range(i + 1, total_nodes):\n",
    "                node2 = road_node_list[j]\n",
    "                geom1 = roads_gdf.geometry.iloc[int(node1['vertex'].split('_')[2])]\n",
    "                geom2 = roads_gdf.geometry.iloc[int(node2['vertex'].split('_')[2])]\n",
    "                if geom1.distance(geom2) <= tolerance:\n",
    "                    class1 = node1['road_class']\n",
    "                    class2 = node2['road_class']\n",
    "                    weight1 = road_class_weights.get(class1, road_class_weights['unknown'])\n",
    "                    weight2 = road_class_weights.get(class2, road_class_weights['unknown'])\n",
    "                    distance = geom1.distance(geom2)\n",
    "                    weight = distance * (weight1 + weight2) / 2\n",
    "                    road_network_edges.append({'src': node1['vertex'], 'dst': node2['vertex'], 'weight': weight, 'type': 'road'})\n",
    "                    road_network_edges.append({'src': node2['vertex'], 'dst': node1['vertex'], 'weight': weight, 'type': 'road'})\n",
    "    else:\n",
    "        road_node_list = []\n",
    "        print(\"No road nodes to process for inter-neighborhood connections.\")\n",
    "\n",
    "    road_nodes_df = cudf.from_pandas(pd.DataFrame(road_node_list) if road_node_list else pd.DataFrame())\n",
    "    if road_network_edges:\n",
    "        road_edges_df = cudf.from_pandas(pd.DataFrame(road_network_edges))\n",
    "        road_edges_df = road_edges_df.dropna()\n",
    "        road_edges_df = road_edges_df.drop_duplicates()\n",
    "        road_network.from_cudf_edgelist(road_edges_df, source='src', destination='dst', edge_attr='weight', symmetrize=False)\n",
    "        road_network._nodes = road_nodes_df\n",
    "    else:\n",
    "        if not road_nodes_df.empty:\n",
    "            road_network._nodes = road_nodes_df\n",
    "        else:\n",
    "            road_network._nodes = cudf.DataFrame()\n",
    "\n",
    "    road_network_data = {\n",
    "        'nodes': road_network._nodes,\n",
    "        'edges': cudf.from_pandas(pd.DataFrame(road_network_edges)) if road_network_edges else None\n",
    "    }\n",
    "    with open(os.path.join(CHECKPOINT_DIR, \"road_network.pkl\"), 'wb') as f:\n",
    "        pickle.dump(road_network_data, f)\n",
    "\n",
    "    # Safely print road network stats\n",
    "    num_nodes = road_network.number_of_nodes() if road_network._nodes is not None and not road_network._nodes.empty else 0\n",
    "    num_edges = len(road_network_edges) if road_network_edges else 0\n",
    "    print(f\"Total road network nodes: {num_nodes}, edges: {num_edges}\")\n",
    "    return subgraphs, road_network_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_walkability(subgraphs, neighborhoods_gdf):\n",
    "    \"\"\"Calculate walkability scores for each neighborhood.\"\"\"\n",
    "    print(\"Checkpoint 3: Calculating walkability scores...\")\n",
    "    walkability_scores = {}\n",
    "\n",
    "    for lie_name, subgraph_data in tqdm(subgraphs.items(), desc=\"Calculating walkability\"):\n",
    "        G_sub = cugraph.Graph(directed=False)\n",
    "        G_sub._nodes = subgraph_data['nodes']\n",
    "        if subgraph_data['edges'] is not None:\n",
    "            # Use the preserved edges_df with 'weight' column\n",
    "            G_sub.from_cudf_edgelist(subgraph_data['edges'], source='src', destination='dst', edge_attr='weight', symmetrize=False)\n",
    "        else:\n",
    "            G_sub._nodes = subgraph_data['nodes']\n",
    "\n",
    "        nodes_df = G_sub._nodes\n",
    "        for idx in range(len(nodes_df)):\n",
    "            node_data = nodes_df.iloc[idx].to_pandas()\n",
    "            if node_data['type'] == 'neighborhood':\n",
    "                residential = node_data['land_use_residential_percent']\n",
    "                commercial = node_data['land_use_commercial_percent']\n",
    "                education = node_data['land_use_education_percent']\n",
    "                ndvi = node_data['ndvi_mean']\n",
    "                tree_count = node_data['tree_count']\n",
    "                transit_count = node_data['transit_count']\n",
    "                open_area = node_data.get('land_use_city_open_area_m2', 0.0)\n",
    "\n",
    "                land_use_score = (residential * 0.4 + commercial * 0.3 + education * 0.2) / 100\n",
    "                ndvi_score = ndvi * 0.5 if pd.notna(ndvi) else 0.0\n",
    "                tree_score = min(1.0, tree_count / 100) * 0.2\n",
    "                transit_score = min(1.0, transit_count / 20) * 0.2\n",
    "                open_space_score = min(1.0, open_area / 10000) * 0.2\n",
    "                walkability = min(1.0, land_use_score + (ndvi_score * 0.4) + tree_score + transit_score + open_space_score)\n",
    "                walkability_scores[lie_name] = walkability\n",
    "\n",
    "                nodes_df['walkability'] = nodes_df['walkability'].astype(float) if 'walkability' in nodes_df.columns else 0.0\n",
    "                nodes_df.loc[nodes_df['vertex'] == node_data['vertex'], 'walkability'] = walkability\n",
    "\n",
    "                neighborhoods_gdf.loc[neighborhoods_gdf['LIE_NAME'] == lie_name, 'walkability'] = walkability\n",
    "\n",
    "        subgraph_data['nodes'] = nodes_df\n",
    "        subgraph_path = os.path.join(SUBGRAPH_DIR, f\"subgraph_{lie_name}_with_walkability.pkl\")\n",
    "        with open(subgraph_path, 'wb') as f:\n",
    "            pickle.dump(subgraph_data, f)\n",
    "\n",
    "    with open(os.path.join(CHECKPOINT_DIR, \"walkability_scores.pkl\"), 'wb') as f:\n",
    "        pickle.dump(walkability_scores, f)\n",
    "\n",
    "    print(\"Walkability scores calculated.\")\n",
    "    return subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_full_graph(subgraphs, road_network_data):\n",
    "    \"\"\"Reconstruct the full graph from subgraphs and road network.\"\"\"\n",
    "    print(\"Reconstructing full graph...\")\n",
    "    G = cugraph.Graph(directed=False)\n",
    "\n",
    "    # Reconstruct and add each subgraph\n",
    "    for lie_name, subgraph_data in subgraphs.items():\n",
    "        G_sub = cugraph.Graph(directed=False)\n",
    "        G_sub._nodes = subgraph_data['nodes']\n",
    "        if subgraph_data['edges'] is not None:\n",
    "            G_sub.from_cudf_edgelist(subgraph_data['edges'], source='src', destination='dst', edge_attr='weight', symmetrize=False)\n",
    "        G.add_nodes_from(G_sub._nodes)\n",
    "        if G_sub.number_of_edges() > 0:\n",
    "            G.add_edges_from(G_sub.edgelist.edgelist_df)\n",
    "\n",
    "    # Reconstruct the road network\n",
    "    road_network = cugraph.Graph(directed=False)\n",
    "    road_network._nodes = road_network_data['nodes']\n",
    "    if road_network_data['edges'] is not None:\n",
    "        road_network.from_cudf_edgelist(road_network_data['edges'], source='src', destination='dst', edge_attr='weight', symmetrize=False)\n",
    "\n",
    "    # Add road network to the full graph\n",
    "    G.add_nodes_from(road_network._nodes)\n",
    "    if road_network.number_of_edges() > 0:\n",
    "        G.add_edges_from(road_network.edgelist.edgelist_df)\n",
    "\n",
    "    print(f\"Full graph reconstructed: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_map(G, data):\n",
    "    \"\"\"Create an interactive Kepler.gl map.\"\"\"\n",
    "    print(\"Generating interactive Kepler.gl map...\")\n",
    "    neighborhoods_gdf = data['neighborhoods'].to_crs('EPSG:4326')\n",
    "    buildings_gdf = data['buildings'].to_crs('EPSG:4326')\n",
    "    roads_gdf = data['roads'].to_crs('EPSG:4326')\n",
    "    trees_gdf = data['trees'].to_crs('EPSG:4326')\n",
    "    transit_gdf = data['transit'].to_crs('EPSG:4326')\n",
    "    urban_masterplan_gdf = data['urban_masterplan'].to_crs('EPSG:4326')\n",
    "\n",
    "    # Add walkability scores to neighborhoods\n",
    "    nodes_df = G._nodes  # Access the nodes directly\n",
    "    for idx in range(len(nodes_df)):\n",
    "        node_data = nodes_df.iloc[idx].to_pandas()\n",
    "        if node_data['type'] == 'neighborhood':\n",
    "            lie_name = node_data['lie_name']\n",
    "            walkability = round(node_data.get('walkability', 0), 2)\n",
    "            transit_count = node_data['transit_count']\n",
    "            neighborhoods_gdf.loc[neighborhoods_gdf['LIE_NAME'].str.strip() == lie_name.strip(), 'walkability'] = walkability\n",
    "            neighborhoods_gdf.loc[neighborhoods_gdf['LIE_NAME'].str.strip() == lie_name.strip(), 'transit_count'] = transit_count\n",
    "\n",
    "    # Prepare data for Kepler.gl\n",
    "    neighborhoods_data = neighborhoods_gdf.drop(columns=['geometry']).copy()\n",
    "    for col in neighborhoods_data.columns:\n",
    "        if col not in ['geometry', 'LIE_NAME', 'SECT_NAME']:\n",
    "            neighborhoods_data[col] = pd.to_numeric(neighborhoods_data[col], errors='coerce').round(4)\n",
    "    geojson_data = {\n",
    "        'type': 'FeatureCollection',\n",
    "        'features': [\n",
    "            {\n",
    "                'type': 'Feature',\n",
    "                'properties': {k: v for k, v in row.drop('geometry').to_dict().items() if pd.notna(v) and v != 0},\n",
    "                'geometry': row['geometry'].__geo_interface__ if row['geometry'] is not None else None\n",
    "            }\n",
    "            for _, row in neighborhoods_gdf.iterrows()\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    buildings_data = buildings_gdf[['full_id', 'osm_id', 'building', '屋齡', '建物高度', '地上層數', '構造種類', '使用分區', 'geometry', 'area_m2']].copy()\n",
    "    buildings_data['建物高度'] = pd.to_numeric(buildings_data['建物高度'], errors='coerce').fillna(10).round(1)\n",
    "    buildings_data['地上層數'] = pd.to_numeric(buildings_data['地上層數'], errors='coerce').fillna(3).round(0)\n",
    "\n",
    "    roads_data = roads_gdf[['class', 'length_m', 'geometry']].copy()\n",
    "    roads_data['class'] = roads_data['class'].fillna('unknown')\n",
    "    roads_data['length_m'] = roads_data['length_m'].round(1)\n",
    "\n",
    "    trees_data = trees_gdf[['geometry']].copy()\n",
    "    trees_data['height_m'] = 10\n",
    "\n",
    "    transit_data = transit_gdf[['id', 'class', 'geometry']].copy()\n",
    "    transit_data['size'] = 10\n",
    "\n",
    "    # Include the original 'Area' column from urban_masterplan\n",
    "    urban_masterplan_data = urban_masterplan_gdf[['Category', 'Area', 'geometry']].copy()\n",
    "    urban_masterplan_data['area_m2'] = urban_masterplan_gdf.geometry.area\n",
    "\n",
    "    map_1 = KeplerGl(height=600, width=800, mapbox_api_access_token=MAPBOX_ACCESS_TOKEN)\n",
    "    map_1.add_data(data=geojson_data, name=\"Neighborhoods\")\n",
    "    map_1.add_data(data=buildings_data, name=\"Buildings\")\n",
    "    map_1.add_data(data=roads_data, name=\"Roads\")\n",
    "    map_1.add_data(data=trees_data, name=\"Trees\")\n",
    "    map_1.add_data(data=transit_data, name=\"Transit\")\n",
    "    map_1.add_data(data=urban_masterplan_data, name=\"Urban_Masterplan\")\n",
    "\n",
    "    if not os.path.exists(CONFIG_PATH):\n",
    "        print(\"No saved configuration found. Generating a temporary HTML for customization...\")\n",
    "        map_1.save_to_html(file_name=TEMP_HTML_PATH)\n",
    "        print(f\"Opening {TEMP_HTML_PATH} in your browser. Please customize the map, then export the configuration as kepler.gl.json.\")\n",
    "        webbrowser.open(f\"file://{os.path.abspath(TEMP_HTML_PATH)}\")\n",
    "        print(\"After customizing, click the 'Share' button in the Kepler.gl interface, then 'Export Configuration' to save as 'kepler.gl.json' in:\")\n",
    "        print(\"/home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data\")\n",
    "        input(\"Press Enter once you have saved the configuration file (kepler.gl.json), or Ctrl+C to cancel.\")\n",
    "        if not os.path.exists(CONFIG_PATH):\n",
    "            raise FileNotFoundError(f\"Configuration file {CONFIG_PATH} not found.\")\n",
    "\n",
    "    print(f\"Loading configuration from {CONFIG_PATH}...\")\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        custom_config = json.load(f)\n",
    "\n",
    "    for layer in custom_config['config']['visState']['layers']:\n",
    "        if layer['config']['dataId'] in [\"Neighborhoods\", \"Buildings\", \"Roads\", \"Trees\", \"Transit\", \"Urban_Masterplan\"]:\n",
    "            layer['config']['isVisible'] = True\n",
    "        if layer['config']['dataId'] == \"Neighborhoods\":\n",
    "            layer['config']['columns'] = {'geojson': 'geometry'}\n",
    "        elif layer['config']['dataId'] == \"Buildings\":\n",
    "            layer['config']['columns'] = {'geojson': 'geometry'}\n",
    "            layer['visualChannels']['colorField'] = {'name': 'building', 'type': 'string'}\n",
    "            layer['visualChannels']['heightField'] = {'name': '建物高度', 'type': 'real'}\n",
    "        elif layer['config']['dataId'] == \"Roads\":\n",
    "            layer['config']['columns'] = {'geojson': 'geometry'}\n",
    "        elif layer['config']['dataId'] == \"Trees\":\n",
    "            layer['config']['columns'] = {'geojson': 'geometry'}\n",
    "        elif layer['config']['dataId'] == \"Transit\":\n",
    "            layer['config']['columns'] = {'geojson': 'geometry'}\n",
    "        elif layer['config']['dataId'] == \"Urban_Masterplan\":\n",
    "            layer['config']['columns'] = {'geojson': 'geometry'}\n",
    "            layer['visualChannels']['colorField'] = {'name': 'Category', 'type': 'string'}\n",
    "            layer['visualChannels']['opacityField'] = {'name': 'Area', 'type': 'real'}\n",
    "\n",
    "    map_1 = KeplerGl(height=600, width=800, mapbox_api_access_token=MAPBOX_ACCESS_TOKEN, config=custom_config)\n",
    "    map_1.add_data(data=geojson_data, name=\"Neighborhoods\")\n",
    "    map_1.add_data(data=buildings_data, name=\"Buildings\")\n",
    "    map_1.add_data(data=roads_data, name=\"Roads\")\n",
    "    map_1.add_data(data=trees_data, name=\"Trees\")\n",
    "    map_1.add_data(data=transit_data, name=\"Transit\")\n",
    "    map_1.add_data(data=urban_masterplan_data, name=\"Urban_Masterplan\")\n",
    "\n",
    "    output_path = os.path.join(\"/home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data\", \"walkability_map_city_level.html\")\n",
    "    map_1.save_to_html(file_name=output_path)\n",
    "    print(f\"Interactive Kepler.gl map saved successfully at {output_path}\")\n",
    "\n",
    "    if os.path.exists(TEMP_HTML_PATH):\n",
    "        os.remove(TEMP_HTML_PATH)\n",
    "        print(f\"Temporary file {TEMP_HTML_PATH} removed.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the walkability analysis pipeline.\"\"\"\n",
    "    print(\"Starting walkability graph network analysis for the entire city...\")\n",
    "    try:\n",
    "        # Load and prepare data\n",
    "        data = load_and_prepare_data()\n",
    "\n",
    "        # Calculate neighborhood features\n",
    "        data['neighborhoods'], urban_plan_to_neighborhoods = calculate_neighborhood_features(\n",
    "            data['neighborhoods'], data['trees'], data['transit'], data['urban_masterplan']\n",
    "        )\n",
    "\n",
    "        # Build graph\n",
    "        subgraphs, road_network = build_graph(data, urban_plan_to_neighborhoods)\n",
    "\n",
    "        # Calculate walkability\n",
    "        subgraphs = calculate_walkability(subgraphs, data['neighborhoods'])\n",
    "\n",
    "        # Reconstruct full graph\n",
    "        G = reconstruct_full_graph(subgraphs, road_network)\n",
    "\n",
    "        # Create interactive map\n",
    "        create_interactive_map(G, data)\n",
    "\n",
    "        print(\"Analysis completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {e}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
