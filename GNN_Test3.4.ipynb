{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a986f754",
   "metadata": {},
   "source": [
    "Cell 0: CUDA Version check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc2c30cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.02.02 25.02.00\n"
     ]
    }
   ],
   "source": [
    "import cudf, cugraph\n",
    "print(cudf.__version__, cugraph.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac3ad7",
   "metadata": {},
   "source": [
    "Cell 1: Imports ,Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e388b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import logging\n",
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Third-party imports\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import cugraph\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv, BatchNorm\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from shapely import make_valid\n",
    "from shapely.errors import GEOSException\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from keplergl import KeplerGl\n",
    "\n",
    "# Plotting configuration\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Noto Sans CJK TC', 'Noto Serif CJK TC', 'Noto Sans Mono CJK TC', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Directory and file paths\n",
    "BASE_DIR = \"/home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data\"\n",
    "LANDUSE_NDVI_PATH = os.path.join(BASE_DIR, \"neighborhoods_with_ndvi_numerical_corrected.geojson\")\n",
    "OSM_BUILDINGS_PATH = os.path.join(BASE_DIR, \"Taipei_Buildings_fulldata.geojson\")\n",
    "OSM_ROADS_PATH = os.path.join(BASE_DIR, \"taipei_segments_cleaned_verified.geoparquet\")\n",
    "OSM_TREES_PATH = os.path.join(BASE_DIR, \"taipei_land.geoparquet\")\n",
    "OSM_TRANSIT_PATH = os.path.join(BASE_DIR, \"taipei_infrastructure.geoparquet\")\n",
    "URBAN_MASTERPLAN_PATH = os.path.join(BASE_DIR, \"Taipei_urban_masterplan.geojson\")\n",
    "ACCIDENTS_PATH = os.path.join(BASE_DIR, \"2023_accidents.geojson\")\n",
    "POPULATION_PATH = os.path.join(BASE_DIR, \"population_corrected.json\")\n",
    "SUBGRAPH_DIR = os.path.join(BASE_DIR, \"subgraphs\")\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"checkpoints\")\n",
    "INTERSECTION_CACHE_PATH = os.path.join(BASE_DIR, \"neighborhoods_with_intersections.geoparquet\")\n",
    "GRAPH_NODES_CACHE_PATH = os.path.join(BASE_DIR, \"graph_nodes.parquet\")\n",
    "GRAPH_EDGES_CACHE_PATH = os.path.join(BASE_DIR, \"graph_edges.parquet\")\n",
    "GRAPH_NODE_ID_CACHE_PATH = os.path.join(BASE_DIR, \"graph_node_id_to_index.json\")\n",
    "GRAPH_DATA_HASH_PATH = os.path.join(BASE_DIR, \"graph_data_hash.txt\")\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(SUBGRAPH_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Constants for spatial analysis\n",
    "BUFFER_DISTANCE = 10  # Meters, buffer distance for border sharing of accidents (tunable based on spatial resolution)\n",
    "MIN_ROAD_LENGTH = 10  # Meters, minimum road length to avoid inflated accident density (tunable based on dataset)\n",
    "\n",
    "# Land use category priorities for area assignment\n",
    "CATEGORY_PRIORITY = {\n",
    "    'City_Open_Area': 10,\n",
    "    'Pedestrian': 9,\n",
    "    'Public_Transportation': 8,\n",
    "    'Amenity': 7,\n",
    "    'Education': 6,\n",
    "    'Medical': 5,\n",
    "    'Commercial': 4,\n",
    "    'Residential': 3,\n",
    "    'Natural': 2,\n",
    "    'Road': 1,\n",
    "    'River': 1,\n",
    "    'Infrastructure': 1,\n",
    "    'Government': 1,\n",
    "    'Special_Zone': 1,\n",
    "    'Military': 1,\n",
    "    'Industrial': 1,\n",
    "    'Agriculture': 1\n",
    "}\n",
    "\n",
    "# Weights for land use diversity in walkability scoring\n",
    "land_use_weights = {\n",
    "    'city_open_area': 0.8,\n",
    "    'commercial': 0.7,\n",
    "    'infrastructure': 0.4,\n",
    "    'government': 0.5,\n",
    "    'public_transportation': 0.8,\n",
    "    'education': 0.7,\n",
    "    'medical': 0.6,\n",
    "    'amenity': 0.8,\n",
    "    'road': 0.3,\n",
    "    'pedestrian': 1.0,\n",
    "    'natural': 0.7,\n",
    "    'special_zone': 0.4,\n",
    "    'river': 0.7,\n",
    "    'military': 0.2,\n",
    "    'residential': 0.6,\n",
    "    'industrial': 0.3,\n",
    "    'agriculture': 0.4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4722f69a",
   "metadata": {},
   "source": [
    "Cell 2: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "713654c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_structure(data_dict):\n",
    "    \"\"\"Print a detailed summary of the data structure for each dataset.\"\"\"\n",
    "    print(\"\\n--- Data Structure Summary ---\")\n",
    "    for key, df in data_dict.items():\n",
    "        if isinstance(df, (gpd.GeoDataFrame, pd.DataFrame, cudf.DataFrame)):\n",
    "            df = df.to_pandas() if isinstance(df, cudf.DataFrame) else df\n",
    "            print(f\"\\nDataset: {key}\")\n",
    "            print(f\"Shape: {df.shape}\")\n",
    "            print(f\"Columns and Data Types:\\n{df.dtypes}\")\n",
    "            print(f\"Missing values (total): {df.isnull().sum().sum()}\")\n",
    "            print(f\"Missing values per column:\\n{df.isnull().sum()}\")\n",
    "            if 'LIE_NAME' in df.columns:\n",
    "                print(f\"Unique LIE_NAME: {df['LIE_NAME'].nunique()}\")\n",
    "            if 'class' in df.columns and key == 'roads':\n",
    "                print(f\"Road class counts:\\n{df['class'].value_counts()}\")\n",
    "            # Print sample data for better debugging\n",
    "            print(f\"Sample data (first 2 rows):\\n{df.head(2)}\")\n",
    "    print(\"--- End of Data Structure Summary ---\\n\")\n",
    "\n",
    "def fix_geometry(geom, buffer_size=1e-5):\n",
    "    \"\"\"Fix invalid geometries with logging for debugging.\"\"\"\n",
    "    if geom is None or geom.is_empty:\n",
    "        logging.debug(\"Geometry is None or empty, returning a default Point(0,0).\")\n",
    "        return Point(0, 0)  # Fallback to a default point\n",
    "    try:\n",
    "        geom = make_valid(geom)\n",
    "        if not geom.is_valid:\n",
    "            logging.debug(f\"Geometry invalid, applying buffer with size {buffer_size}: {geom.bounds}\")\n",
    "            geom = geom.buffer(buffer_size)\n",
    "            geom = make_valid(geom)\n",
    "        if not geom.is_valid:\n",
    "            logging.warning(f\"Geometry remains invalid after fixing: {geom.bounds}. Returning default Point(0,0).\")\n",
    "            return Point(0, 0)  # Fallback to a default point\n",
    "        return geom\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fixing geometry: {e}. Returning default Point(0,0).\")\n",
    "        return Point(0, 0)\n",
    "\n",
    "def print_percentage_calculation(neighborhoods_gdf, urban_masterplan_gdf, sample_size=3):\n",
    "    \"\"\"Print the land use percentage calculation process for a sample of neighborhoods.\"\"\"\n",
    "    print(\"\\n--- Percentage Calculation Process ---\")\n",
    "    sample_neighborhoods = neighborhoods_gdf.sample(min(sample_size, len(neighborhoods_gdf)), random_state=42)\n",
    "    \n",
    "    for idx, row in sample_neighborhoods.iterrows():\n",
    "        lie_name = row['LIE_NAME']\n",
    "        print(f\"\\nNeighborhood: {lie_name} (Index: {idx})\")\n",
    "        \n",
    "        neighborhood_geom = fix_geometry(row['geometry'])\n",
    "        if not neighborhood_geom.is_valid:\n",
    "            print(f\"Neighborhood geometry is invalid after fixing: {lie_name}\")\n",
    "            continue\n",
    "        \n",
    "        relevant_masterplan = urban_masterplan_gdf[urban_masterplan_gdf.intersects(neighborhood_geom)]\n",
    "        if relevant_masterplan.empty:\n",
    "            print(\"No master plan polygons intersect with this neighborhood.\")\n",
    "            continue\n",
    "        \n",
    "        temp_gdf = gpd.GeoDataFrame({'geometry': [neighborhood_geom]}, crs='EPSG:3826')\n",
    "        intersected = gpd.overlay(temp_gdf, relevant_masterplan, how='intersection', keep_geom_type=False)\n",
    "        if intersected.empty:\n",
    "            print(\"No valid intersections after overlay.\")\n",
    "            continue\n",
    "        \n",
    "        intersected['geometry'] = intersected['geometry'].apply(fix_geometry)\n",
    "        intersected = intersected[intersected.geometry.is_valid & ~intersected.geometry.is_empty]\n",
    "        if intersected.empty:\n",
    "            print(\"No valid geometries after fixing intersected polygons.\")\n",
    "            continue\n",
    "        \n",
    "        intersected['priority'] = intersected['Category'].map(CATEGORY_PRIORITY)\n",
    "        intersected = intersected.sort_values(by='priority', ascending=False)\n",
    "        \n",
    "        total_area_geom = intersected.geometry.union_all()\n",
    "        total_area = total_area_geom.area\n",
    "        print(f\"Total unique master plan area: {total_area:.2f} m²\")\n",
    "        \n",
    "        remaining_geom = total_area_geom\n",
    "        category_areas = {}\n",
    "        for category in intersected['Category'].unique():\n",
    "            category_rows = intersected[intersected['Category'] == category]\n",
    "            category_geom = category_rows.geometry.union_all()\n",
    "            try:\n",
    "                category_area_geom = category_geom.intersection(remaining_geom)\n",
    "                category_area = category_area_geom.area\n",
    "                category_areas[category] = category_area\n",
    "                print(f\"Area of {category} (priority {CATEGORY_PRIORITY.get(category, 0)}): {category_area:.2f} m²\")\n",
    "                remaining_geom = remaining_geom.difference(category_area_geom)\n",
    "            except GEOSException as e:\n",
    "                print(f\"Topology error for category {category}: {e}\")\n",
    "                category_areas[category] = 0.0\n",
    "        \n",
    "        print(\"\\nPercentages:\")\n",
    "        total_percentage = 0.0\n",
    "        for category, area in category_areas.items():\n",
    "            percentage = (area / total_area * 100) if total_area > 0 else 0.0\n",
    "            total_percentage += percentage\n",
    "            print(f\"{category}: {percentage:.2f}%\")\n",
    "        print(f\"Sum of percentages: {total_percentage:.2f}%\")\n",
    "    print(\"--- End of Percentage Calculation Process ---\\n\")\n",
    "\n",
    "def compute_data_hash(data_dict):\n",
    "    \"\"\"Compute a hash of the data for caching purposes.\"\"\"\n",
    "    hasher = hashlib.sha256()\n",
    "    for key, df in data_dict.items():\n",
    "        if isinstance(df, (gpd.GeoDataFrame, pd.DataFrame, cudf.DataFrame)):\n",
    "            df = df.to_pandas() if isinstance(df, cudf.DataFrame) else df\n",
    "            hasher.update(str(df.shape).encode('utf-8'))\n",
    "            hasher.update(str(sorted(df.columns)).encode('utf-8'))\n",
    "            \n",
    "            # Log column types for debugging\n",
    "            logging.info(f\"Dataset {key} column types:\\n{df.dtypes}\")\n",
    "            \n",
    "            # Create a copy of the sample and convert non-serializable types\n",
    "            sample_df = df.head(5).copy()\n",
    "            # Drop geometry column if present, as it's not JSON serializable\n",
    "            if 'geometry' in sample_df.columns:\n",
    "                sample_df = sample_df.drop(columns=['geometry'])\n",
    "            # Convert NumPy types to Python types\n",
    "            for col in sample_df.columns:\n",
    "                sample_df[col] = sample_df[col].apply(\n",
    "                    lambda x: x.tolist() if isinstance(x, np.ndarray) else\n",
    "                              float(x) if isinstance(x, (np.floating, np.integer)) else x\n",
    "                )\n",
    "            try:\n",
    "                sample = sample_df.to_json()\n",
    "                hasher.update(sample.encode('utf-8'))\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to serialize sample for dataset {key}: {e}\")\n",
    "                # Fallback: Convert to string representation\n",
    "                sample = str(sample_df.to_dict())\n",
    "                hasher.update(sample.encode('utf-8'))\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def check_spatial_overlap(gdf1, gdf2, label1=\"gdf1\", label2=\"gdf2\"):\n",
    "    \"\"\"Check for spatial overlap between two GeoDataFrames and log the results.\"\"\"\n",
    "    logging.info(f\"Checking spatial overlap between {label1} and {label2}...\")\n",
    "    gdf1 = gdf1.copy()\n",
    "    gdf2 = gdf2.copy()\n",
    "    \n",
    "    # Ensure CRS matches\n",
    "    if gdf1.crs != gdf2.crs:\n",
    "        logging.warning(f\"CRS mismatch between {label1} ({gdf1.crs}) and {label2} ({gdf2.crs}). Aligning to {gdf1.crs}...\")\n",
    "        gdf2 = gdf2.to_crs(gdf1.crs)\n",
    "    \n",
    "    # Validate geometries\n",
    "    gdf1['geometry'] = gdf1['geometry'].apply(fix_geometry)\n",
    "    gdf2['geometry'] = gdf2['geometry'].apply(fix_geometry)\n",
    "    \n",
    "    # Compute bounding boxes\n",
    "    gdf1_bounds = gdf1.total_bounds\n",
    "    gdf2_bounds = gdf2.total_bounds\n",
    "    logging.info(f\"{label1} bounds: {gdf1_bounds}\")\n",
    "    logging.info(f\"{label2} bounds: {gdf2_bounds}\")\n",
    "    \n",
    "    # Check if bounding boxes overlap\n",
    "    bounds_overlap = not (gdf1_bounds[2] < gdf2_bounds[0] or  # gdf1 max_x < gdf2 min_x\n",
    "                         gdf1_bounds[0] > gdf2_bounds[2] or  # gdf1 min_x > gdf2 max_x\n",
    "                         gdf1_bounds[3] < gdf2_bounds[1] or  # gdf1 max_y < gdf2 min_y\n",
    "                         gdf1_bounds[1] > gdf2_bounds[3])    # gdf1 min_y > gdf2 max_y\n",
    "    logging.info(f\"Bounding boxes overlap: {bounds_overlap}\")\n",
    "    \n",
    "    # Perform a sample intersection check\n",
    "    sample_size = min(10, len(gdf1), len(gdf2))\n",
    "    if sample_size > 0:\n",
    "        sample_gdf1 = gdf1.sample(sample_size, random_state=42)\n",
    "        intersects = gpd.sjoin(sample_gdf1, gdf2, how='inner', predicate='intersects')\n",
    "        logging.info(f\"Sample intersection check: {len(intersects)} intersections found out of {sample_size} samples.\")\n",
    "    \n",
    "    return bounds_overlap\n",
    "\n",
    "def compute_road_type_accident_correlation(roads_gdf, neighborhoods_gdf, accidents_gdf):\n",
    "    \"\"\"\n",
    "    Compute correlation between OSM road types and accident density (accidents per km of road length).\n",
    "    Uses road class as a proxy for width, with ordinal ranking based on OSM hierarchy.\n",
    "    Generates bar, box, and scatter plots for visualization.\n",
    "    \"\"\"\n",
    "    logging.info(\"Computing correlation between road types and accident density...\")\n",
    "    \n",
    "    # Validate inputs\n",
    "    if roads_gdf.empty or neighborhoods_gdf.empty or accidents_gdf.empty:\n",
    "        logging.error(\"One or more input GeoDataFrames are empty.\")\n",
    "        raise ValueError(\"Input GeoDataFrames cannot be empty.\")\n",
    "    \n",
    "    # Define ordinal width proxy based on OSM highway hierarchy\n",
    "    width_ranking = {\n",
    "        'motorway': 5, 'trunk': 5, 'primary': 4, 'secondary': 4, 'tertiary': 3,\n",
    "        'residential': 3, 'living_street': 3, 'service': 2, 'track': 2,\n",
    "        'path': 1, 'footway': 1, 'cycleway': 1, 'steps': 1, 'pedestrian': 1,\n",
    "        'unclassified': 0, 'bridleway': 0, 'unknown': 0\n",
    "    }\n",
    "    \n",
    "    # Assign width rank to roads\n",
    "    roads_gdf = roads_gdf.copy()\n",
    "    roads_gdf['width_rank'] = roads_gdf['class'].map(width_ranking).fillna(0).astype(int)\n",
    "    \n",
    "    # Buffer wider roads to increase their likelihood of capturing accidents\n",
    "    roads_gdf_buffered = roads_gdf.copy()\n",
    "    roads_gdf_buffered['geometry'] = roads_gdf_buffered.apply(\n",
    "        lambda row: row['geometry'].buffer(5) if row['width_rank'] >= 4 else row['geometry'], axis=1\n",
    "    )\n",
    "    \n",
    "    # Assign accidents to the nearest road using sjoin_nearest\n",
    "    logging.info(\"Assigning accidents to nearest road...\")\n",
    "    accidents_gdf = accidents_gdf.copy()\n",
    "    accidents_gdf['geometry'] = accidents_gdf['geometry'].apply(fix_geometry)\n",
    "    accidents_gdf = accidents_gdf[accidents_gdf['geometry'].is_valid & ~accidents_gdf['geometry'].is_empty]\n",
    "    \n",
    "    if accidents_gdf.empty:\n",
    "        logging.warning(\"No valid accidents after geometry fixing.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Use sjoin_nearest with weighted distance based on width_rank\n",
    "    nearest = gpd.sjoin_nearest(\n",
    "        accidents_gdf,\n",
    "        roads_gdf_buffered[['geometry', 'class', 'width_rank']],\n",
    "        how='left',\n",
    "        distance_col='distance'\n",
    "    )\n",
    "    # Adjust distance by width_rank to favor wider roads\n",
    "    nearest['weighted_distance'] = nearest['distance'] / (nearest['width_rank'].replace(0, 1) ** 2)\n",
    "    # Drop duplicates based on the accident index (DataFrame index)\n",
    "    nearest = nearest.sort_values('weighted_distance').drop_duplicates(keep='first')\n",
    "    \n",
    "    matched_accidents = nearest[['index_right']].copy().reset_index()\n",
    "    matched_accidents.columns = ['accident_idx', 'road_idx']\n",
    "    matched_accidents = matched_accidents.dropna(subset=['road_idx'])\n",
    "    matched_accidents['road_idx'] = matched_accidents['road_idx'].astype(int)\n",
    "    \n",
    "    logging.info(f\"Matched {len(matched_accidents)} accidents out of {len(accidents_gdf)}\")\n",
    "    \n",
    "    # Reassign accidents from footway/cycleway to nearby wider roads\n",
    "    footway_cycleway_accidents = matched_accidents[\n",
    "        matched_accidents['road_idx'].isin(\n",
    "            roads_gdf[roads_gdf['class'].isin(['footway', 'cycleway'])].index\n",
    "        )\n",
    "    ]\n",
    "    if not footway_cycleway_accidents.empty:\n",
    "        logging.info(f\"Reassigning {len(footway_cycleway_accidents)} accidents from footway/cycleway...\")\n",
    "        accidents_to_reassign = accidents_gdf.loc[footway_cycleway_accidents['accident_idx']].copy()\n",
    "        wider_roads = roads_gdf_buffered[roads_gdf_buffered['width_rank'] >= 4]\n",
    "        if not wider_roads.empty:\n",
    "            reassigned = gpd.sjoin_nearest(\n",
    "                accidents_to_reassign,\n",
    "                wider_roads[['geometry', 'class']],\n",
    "                how='left',\n",
    "                max_distance=10  # Only reassign if within 10 meters\n",
    "            )\n",
    "            reassigned_matches = pd.DataFrame({\n",
    "                'accident_idx': reassigned.index,\n",
    "                'road_idx': reassigned['index_right']\n",
    "            }).copy()\n",
    "            reassigned_matches = reassigned_matches.dropna(subset=['road_idx'])\n",
    "            reassigned_matches['road_idx'] = reassigned_matches['road_idx'].astype(int)\n",
    "            matched_accidents = matched_accidents[~matched_accidents['accident_idx'].isin(reassigned_matches['accident_idx'])]\n",
    "            matched_accidents = pd.concat([matched_accidents, reassigned_matches], ignore_index=True)\n",
    "            logging.info(f\"Reassigned {len(reassigned_matches)} accidents to wider roads\")\n",
    "    \n",
    "    # Count accidents per road segment\n",
    "    accident_counts = matched_accidents.groupby('road_idx').size().reindex(roads_gdf.index, fill_value=0)\n",
    "    roads_gdf['accident_count'] = accident_counts\n",
    "    \n",
    "    # Log the distribution of accidents by road type\n",
    "    accident_summary = roads_gdf.groupby('class')['accident_count'].sum()\n",
    "    logging.info(f\"Accidents by road type:\\n{accident_summary}\")\n",
    "    \n",
    "    # Filter short roads to avoid inflated accident density\n",
    "    roads_gdf = roads_gdf[roads_gdf['length_m'] >= MIN_ROAD_LENGTH]\n",
    "    \n",
    "    # Compute accident density (accidents per km)\n",
    "    roads_gdf['accident_density'] = roads_gdf['accident_count'] / (roads_gdf['length_m'] / 1000)\n",
    "    roads_gdf['accident_density'] = roads_gdf['accident_density'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # Normalize density by width_rank to reduce impact of narrow roads\n",
    "    roads_gdf['accident_density'] = roads_gdf['accident_density'] * (roads_gdf['width_rank'].replace(0, 1) / 5)\n",
    "    \n",
    "    # Log road type counts\n",
    "    logging.info(f\"Road type counts:\\n{roads_gdf['class'].value_counts()}\")\n",
    "    print(f\"Road type counts:\\n{roads_gdf['class'].value_counts()}\")\n",
    "    \n",
    "    # Aggregate by road class for summary\n",
    "    summary = roads_gdf.groupby('class').agg({\n",
    "        'length_m': 'sum',\n",
    "        'accident_count': 'sum',\n",
    "        'accident_density': 'mean',\n",
    "        'width_rank': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Filter out classes with negligible data\n",
    "    summary = summary[summary['length_m'] > 1000]  # At least 1km total length\n",
    "    summary = summary[summary['width_rank'] > 0]   # Exclude unknown, unclassified, bridleway\n",
    "    \n",
    "    # Log summary\n",
    "    print(\"\\n--- Road Type Accident Density Summary ---\")\n",
    "    print(summary[['class', 'length_m', 'accident_count', 'accident_density', 'width_rank']].round(2))\n",
    "    \n",
    "    # Compute Spearman's rank correlation\n",
    "    if len(summary) >= 2:\n",
    "        corr, p_value = spearmanr(summary['width_rank'], summary['accident_density'])\n",
    "        logging.info(f\"Spearman's correlation between road width rank and accident density: {corr:.3f} (p-value: {p_value:.3f})\")\n",
    "        print(f\"Spearman's correlation: {corr:.3f} (p-value: {p_value:.3f})\")\n",
    "    else:\n",
    "        logging.warning(\"Insufficient road types for correlation analysis.\")\n",
    "        print(\"Insufficient road types for correlation analysis.\")\n",
    "    \n",
    "    # Compute average accident density per neighborhood for walkability\n",
    "    logging.info(\"Computing average road accident density per neighborhood...\")\n",
    "    road_neighborhoods = gpd.sjoin(\n",
    "        roads_gdf[['geometry', 'class', 'length_m', 'width_rank', 'accident_density']], \n",
    "        neighborhoods_gdf[['geometry', 'LIE_NAME']], \n",
    "        how='left', predicate='intersects'\n",
    "    )\n",
    "    avg_accident_density = road_neighborhoods.groupby('LIE_NAME')['accident_density'].mean()\n",
    "    avg_accident_density = avg_accident_density.reindex(neighborhoods_gdf['LIE_NAME'], fill_value=0)\n",
    "    neighborhoods_gdf['avg_road_accident_density'] = avg_accident_density.fillna(0)\n",
    "    \n",
    "    # Visualize with multiple charts\n",
    "    # 1. Bar Chart: Mean accident density by road type\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    summary_sorted = summary.sort_values('width_rank', ascending=False)\n",
    "    sns.barplot(data=summary_sorted, x='class', y='accident_density', hue='width_rank', dodge=False)\n",
    "    plt.xlabel('Road Type')\n",
    "    plt.ylabel('Mean Accident Density (Accidents per km)')\n",
    "    plt.title('Mean Accident Density by Road Type')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Width Rank')\n",
    "    plt.tight_layout()\n",
    "    bar_path = os.path.join(BASE_DIR, 'road_type_accident_bar.png')\n",
    "    plt.savefig(bar_path)\n",
    "    plt.close()\n",
    "    logging.info(f\"Bar chart saved to {bar_path}\")\n",
    "    print(f\"Bar chart saved to {bar_path}\")\n",
    "    \n",
    "    # 2. Box Plot: Distribution of accident density by road type\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=roads_gdf[roads_gdf['class'].isin(summary['class'])], \n",
    "                x='class', y='accident_density', hue='width_rank', dodge=False)\n",
    "    plt.xlabel('Road Type')\n",
    "    plt.ylabel('Accident Density (Accidents per km)')\n",
    "    plt.title('Distribution of Accident Density by Road Type')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yscale('log')  # Log scale for skewed data\n",
    "    plt.legend(title='Width Rank')\n",
    "    plt.tight_layout()\n",
    "    box_path = os.path.join(BASE_DIR, 'road_type_accident_box.png')\n",
    "    plt.savefig(box_path)\n",
    "    plt.close()\n",
    "    logging.info(f\"Box chart saved to {box_path}\")\n",
    "    print(f\"Box chart saved to {box_path}\")\n",
    "    \n",
    "    # 3. Scatter Plot: Accident density vs. width rank with trend line\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=summary, x='width_rank', y='accident_density', \n",
    "                    size='length_m', sizes=(50, 500), hue='class', style='class', alpha=0.7)\n",
    "    z = np.polyfit(summary['width_rank'], summary['accident_density'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(summary['width_rank'], p(summary['width_rank']), \"r--\", alpha=0.5)\n",
    "    plt.xlabel('Road Width Rank (1=Path, 5=Motorway)')\n",
    "    plt.ylabel('Mean Accident Density (Accidents per km)')\n",
    "    plt.title('Road Type vs. Accident Density')\n",
    "    plt.yscale('log')  # Log scale for skewed data\n",
    "    plt.grid(True)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    scatter_path = os.path.join(BASE_DIR, 'road_type_accident_scatter.png')\n",
    "    plt.savefig(scatter_path)\n",
    "    plt.close()\n",
    "    logging.info(f\"Scatter plot saved to {scatter_path}\")\n",
    "    print(f\"Scatter plot saved to {scatter_path}\")\n",
    "    \n",
    "    # Log top accident-prone road types\n",
    "    top_types = summary.nlargest(3, 'accident_density')[['class', 'accident_density']]\n",
    "    logging.info(f\"Top 3 road types by accident density:\\n{top_types.round(2)}\")\n",
    "    print(f\"Top 3 road types by accident density:\\n{top_types.round(2)}\")\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ae081",
   "metadata": {},
   "source": [
    "Cell 3: Walkability Computation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b05a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_walkability_components(neighborhoods_gdf, sample_size=5):\n",
    "    \"\"\"\n",
    "    Compute walkability components for a sample of neighborhoods.\n",
    "    \n",
    "    Args:\n",
    "        neighborhoods_gdf (gpd.GeoDataFrame): GeoDataFrame of neighborhoods.\n",
    "        sample_size (int): Number of neighborhoods to sample.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with walkability components for sampled neighborhoods.\n",
    "    \"\"\"\n",
    "    if not all(col in neighborhoods_gdf.columns for col in ['ndvi_mean', 'tree_count', 'transit_count', 'intersection_density', 'accident_count', 'area_km2', 'avg_road_accident_density', 'elderly_percentage']):\n",
    "        logging.error(\"Required columns missing in neighborhoods_gdf for walkability computation.\")\n",
    "        raise KeyError(\"Missing required columns in neighborhoods_gdf.\")\n",
    "    \n",
    "    sample_gdf = neighborhoods_gdf.sample(min(sample_size, len(neighborhoods_gdf)), random_state=42)\n",
    "    \n",
    "    components = {\n",
    "        'LIE_NAME': [],\n",
    "        'land_use_diversity': [],\n",
    "        'green_space_score': [],\n",
    "        'transit_score': [],\n",
    "        'road_connectivity': [],\n",
    "        'safety_score': [],\n",
    "        'elderly_accessibility': [],\n",
    "        'pedestrian_infrastructure_score': [],\n",
    "        'walkability_score': [],\n",
    "        'walkability_category': []\n",
    "    }\n",
    "    \n",
    "    # Precompute normalization constants\n",
    "    ndvi_min, ndvi_max = neighborhoods_gdf['ndvi_mean'].min(), neighborhoods_gdf['ndvi_mean'].max()\n",
    "    tree_min, tree_max = neighborhoods_gdf['tree_count'].min(), neighborhoods_gdf['tree_count'].max()\n",
    "    transit_min, transit_max = neighborhoods_gdf['transit_count'].min(), neighborhoods_gdf['transit_count'].max()\n",
    "    intersection_density_min = neighborhoods_gdf['intersection_density'].min()\n",
    "    intersection_density_max = neighborhoods_gdf['intersection_density'].max()\n",
    "    accident_count_min, accident_count_max = neighborhoods_gdf['accident_count'].min(), neighborhoods_gdf['accident_count'].max()\n",
    "    accident_density_max = neighborhoods_gdf['avg_road_accident_density'].max()\n",
    "    pedestrian_road_max = neighborhoods_gdf.get('pedestrian_road_density', pd.Series(0)).max()\n",
    "    \n",
    "    # Compute accident_count_density_max for safety score\n",
    "    accident_count_density = neighborhoods_gdf['accident_count'] / neighborhoods_gdf['area_km2'].replace(0, 1e-6)\n",
    "    accident_count_density_max = accident_count_density.max() if accident_count_density.max() > 0 else 1.0  # Avoid division by zero\n",
    "    \n",
    "    for idx, row in sample_gdf.iterrows():\n",
    "        # Land Use Diversity (weighted by walkability desirability)\n",
    "        land_use_cols = [f\"land_use_{category.lower()}_percent\" for category in CATEGORY_PRIORITY.keys()]\n",
    "        land_use_values = [row.get(col, 0.0) / 100 for col in land_use_cols if col in row]\n",
    "        land_use_weights_list = [land_use_weights[category.lower()] for category in CATEGORY_PRIORITY.keys()]\n",
    "        weighted_values = [p * w for p, w in zip(land_use_values, land_use_weights_list) if p > 0]\n",
    "        if weighted_values:\n",
    "            total = sum(weighted_values)\n",
    "            if total > 0:\n",
    "                weighted_values = [v / total for v in weighted_values]\n",
    "                entropy = -np.sum([p * np.log2(p + 1e-10) for p in weighted_values])\n",
    "                max_entropy = np.log2(len(weighted_values))\n",
    "                land_use_diversity = entropy / max_entropy if max_entropy > 0 else 0\n",
    "            else:\n",
    "                land_use_diversity = 0\n",
    "        else:\n",
    "            land_use_diversity = 0\n",
    "        \n",
    "        # Green Space and Comfort (use linear scaling for better variation)\n",
    "        ndvi_normalized = ((row['ndvi_mean'] - ndvi_min) / (ndvi_max - ndvi_min + 1e-6)) if (ndvi_max - ndvi_min) > 0 else 0\n",
    "        tree_density = row['tree_count'] / row['area_km2'] if row['area_km2'] > 0 else 0\n",
    "        tree_density_max = (neighborhoods_gdf['tree_count'] / neighborhoods_gdf['area_km2']).replace(0, 1e-6).max()\n",
    "        tree_density_normalized = (tree_density / (tree_density_max + 1e-6)) if tree_density_max > 0 else 0\n",
    "        open_area = row.get('land_use_city_open_area_percent', 0.0) / 100\n",
    "        green_space_score = (0.4 * ndvi_normalized + 0.3 * tree_density_normalized + 0.3 * open_area)\n",
    "        \n",
    "        # Transit Accessibility (use linear scaling for better variation)\n",
    "        transit_raw = (row['transit_count'] - transit_min) / (transit_max - transit_min + 1e-6) if (transit_max - transit_min) > 0 else 0\n",
    "        transit_score = transit_raw\n",
    "        \n",
    "        # Road Connectivity (use linear scaling for better variation)\n",
    "        intersection_density = row['intersection_density']\n",
    "        intersection_density_normalized = (intersection_density - intersection_density_min) / (intersection_density_max - intersection_density_min + 1e-6) if (intersection_density_max - intersection_density_min) > 0 else 0\n",
    "        road_connectivity = intersection_density_normalized\n",
    "        \n",
    "        # Safety Score (adjust weights to balance contribution)\n",
    "        accident_count_density = row['accident_count'] / row['area_km2'] if row['area_km2'] > 0 else 0\n",
    "        accident_count_density = min(accident_count_density, accident_count_density_max * 0.5)\n",
    "        safety_score_count = 1 - (accident_count_density / (accident_count_density_max + 1e-6)) if accident_count_density_max > 0 else 1\n",
    "        accident_density = row['avg_road_accident_density']\n",
    "        safety_score_roads = 1 - (accident_density / (accident_density_max + 1e-6)) if accident_density_max > 0 else 1\n",
    "        pedestrian_roads = row.get('pedestrian_road_density', 0.0)\n",
    "        pedestrian_roads_safety = pedestrian_roads / (pedestrian_road_max + 1e-6) if pedestrian_road_max > 0 else 0\n",
    "        safety_score = (0.3 * safety_score_count + 0.3 * safety_score_roads + 0.4 * pedestrian_roads_safety) if row['accident_count'] != 0 else (0.5 * safety_score_roads + 0.5 * pedestrian_roads_safety)\n",
    "        \n",
    "        # Elderly Accessibility (adjust weights)\n",
    "        elderly_percentage = row['elderly_percentage'] / 100\n",
    "        medical_access = row.get('land_use_medical_percent', 0.0) / 100\n",
    "        elderly_accessibility = 0.3 * elderly_percentage + 0.4 * medical_access + 0.3 * pedestrian_roads_safety\n",
    "        \n",
    "        # Pedestrian Infrastructure (use linear scaling for better variation)\n",
    "        pedestrian_road_score = pedestrian_roads / (pedestrian_road_max + 1e-6) if pedestrian_road_max > 0 else 0\n",
    "        amenity_access = row.get('land_use_amenity_percent', 0.0) / 100\n",
    "        pedestrian_infrastructure_score = 0.5 * pedestrian_road_score + 0.5 * amenity_access\n",
    "        \n",
    "        # Walkability Score with adjusted weights to increase variation\n",
    "        base_score = (\n",
    "            0.25 * land_use_diversity +  # Increase weight\n",
    "            0.25 * green_space_score +   # Increase weight\n",
    "            0.15 * transit_score +\n",
    "            0.15 * road_connectivity +\n",
    "            0.20 * pedestrian_infrastructure_score  # Reduce weight to balance\n",
    "        )\n",
    "        safety_modifier = 0.7 + 0.3 * safety_score  # Reduce impact to increase variation\n",
    "        elderly_modifier = 1 + elderly_accessibility * 0.1  # Reduce impact to increase variation\n",
    "        walkability_score = base_score * safety_modifier * elderly_modifier\n",
    "        walkability_score = np.clip(walkability_score, 0, 1)\n",
    "        \n",
    "        # Categorize walkability score\n",
    "        if pd.isna(walkability_score):\n",
    "            category = 'low'\n",
    "        elif walkability_score < 0.33:\n",
    "            category = 'low'\n",
    "        elif walkability_score < 0.66:\n",
    "            category = 'medium'\n",
    "        else:\n",
    "            category = 'high'\n",
    "        \n",
    "        components['LIE_NAME'].append(row['LIE_NAME'])\n",
    "        components['land_use_diversity'].append(land_use_diversity)\n",
    "        components['green_space_score'].append(green_space_score)\n",
    "        components['transit_score'].append(transit_score)\n",
    "        components['road_connectivity'].append(road_connectivity)\n",
    "        components['safety_score'].append(safety_score)\n",
    "        components['elderly_accessibility'].append(elderly_accessibility)\n",
    "        components['pedestrian_infrastructure_score'].append(pedestrian_infrastructure_score)\n",
    "        components['walkability_score'].append(walkability_score)\n",
    "        components['walkability_category'].append(category)\n",
    "    \n",
    "    return pd.DataFrame(components)\n",
    "\n",
    "def compute_walkability_components_all(neighborhoods_df, data):\n",
    "    \"\"\"\n",
    "    Compute walkability components for all neighborhoods.\n",
    "    \n",
    "    Args:\n",
    "        neighborhoods_df (gpd.GeoDataFrame): GeoDataFrame of neighborhoods.\n",
    "        data (dict): Dictionary containing roads and other datasets.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with walkability components for all neighborhoods.\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if neighborhoods_df.empty:\n",
    "        logging.error(\"neighborhoods_df is empty.\")\n",
    "        raise ValueError(\"neighborhoods_df cannot be empty.\")\n",
    "    if 'roads' not in data or data['roads'].empty:\n",
    "        logging.error(\"Roads data is missing or empty.\")\n",
    "        raise ValueError(\"Roads data cannot be empty.\")\n",
    "    \n",
    "    required_cols = ['LIE_NAME', 'geometry', 'ndvi_mean', 'tree_count', 'transit_count', 'intersection_density', \n",
    "                     'accident_count', 'area_km2', 'avg_road_accident_density', 'elderly_percentage']\n",
    "    missing_cols = [col for col in required_cols if col not in neighborhoods_df.columns]\n",
    "    if missing_cols:\n",
    "        logging.error(f\"Missing columns in neighborhoods_df: {missing_cols}\")\n",
    "        raise KeyError(f\"Missing columns in neighborhoods_df: {missing_cols}\")\n",
    "    \n",
    "    # Check for non-null values in critical columns\n",
    "    for col in required_cols:\n",
    "        if col == 'geometry':\n",
    "            null_count = neighborhoods_df[col].isna().sum()\n",
    "            invalid_count = (~neighborhoods_df[col].is_valid).sum()\n",
    "            logging.info(f\"Geometry column: {null_count} nulls, {invalid_count} invalid geometries.\")\n",
    "        else:\n",
    "            null_count = neighborhoods_df[col].isna().sum()\n",
    "            zero_count = (neighborhoods_df[col] == 0).sum()\n",
    "            logging.info(f\"{col}: {null_count} nulls, {zero_count} zeros.\")\n",
    "    \n",
    "    # Ensure CRS matches\n",
    "    neighborhoods_df = neighborhoods_df.copy()\n",
    "    neighborhoods_df = neighborhoods_df.to_crs('EPSG:3826')\n",
    "    roads_df = data['roads'].copy()\n",
    "    roads_df = roads_df.to_crs('EPSG:3826')\n",
    "    \n",
    "    # Validate geometries\n",
    "    neighborhoods_df['geometry'] = neighborhoods_df['geometry'].apply(fix_geometry)\n",
    "    invalid_geoms = neighborhoods_df[~neighborhoods_df.geometry.is_valid]\n",
    "    if not invalid_geoms.empty:\n",
    "        logging.warning(f\"Found {len(invalid_geoms)} invalid geometries in neighborhoods_df after fixing.\")\n",
    "        neighborhoods_df = neighborhoods_df[neighborhoods_df.geometry.is_valid]\n",
    "        if neighborhoods_df.empty:\n",
    "            logging.error(\"All geometries in neighborhoods_df are invalid after fixing.\")\n",
    "            raise ValueError(\"No valid geometries in neighborhoods_df.\")\n",
    "    \n",
    "    roads_df['geometry'] = roads_df['geometry'].apply(fix_geometry)\n",
    "    invalid_roads = roads_df[~roads_df.geometry.is_valid]\n",
    "    if not invalid_roads.empty:\n",
    "        logging.warning(f\"Found {len(invalid_roads)} invalid geometries in roads_df after fixing.\")\n",
    "        roads_df = roads_df[roads_df.geometry.is_valid]\n",
    "        if roads_df.empty:\n",
    "            logging.error(\"All geometries in roads_df are invalid after fixing.\")\n",
    "            raise ValueError(\"No valid geometries in roads_df.\")\n",
    "    \n",
    "    # Compute pedestrian road density\n",
    "    pedestrian_road_types = ['footway', 'pedestrian', 'cycleway']\n",
    "    pedestrian_roads_df = roads_df[roads_df['class'].isin(pedestrian_road_types)]\n",
    "    logging.info(f\"Filtered {len(pedestrian_roads_df)} roads of types {pedestrian_road_types} out of {len(roads_df)} total roads.\")\n",
    "    \n",
    "    # Check spatial overlap\n",
    "    overlap = check_spatial_overlap(neighborhoods_df, pedestrian_roads_df, \"neighborhoods\", \"pedestrian_roads\")\n",
    "    if not overlap:\n",
    "        logging.warning(\"No spatial overlap between neighborhoods and pedestrian roads. Buffering geometries to find matches...\")\n",
    "        neighborhoods_buffered = neighborhoods_df.copy()\n",
    "        neighborhoods_buffered['geometry'] = neighborhoods_buffered['geometry'].buffer(50)  # Buffer by 50 meters\n",
    "        pedestrian_roads = gpd.sjoin(\n",
    "            pedestrian_roads_df,\n",
    "            neighborhoods_buffered[['geometry', 'LIE_NAME']],\n",
    "            how='left',\n",
    "            predicate='intersects'\n",
    "        )\n",
    "    else:\n",
    "        pedestrian_roads = gpd.sjoin(\n",
    "            pedestrian_roads_df,\n",
    "            neighborhoods_df[['geometry', 'LIE_NAME']],\n",
    "            how='left',\n",
    "            predicate='intersects'\n",
    "        )\n",
    "    \n",
    "    logging.info(f\"Pedestrian roads join resulted in {len(pedestrian_roads)} matches.\")\n",
    "    pedestrian_lengths = pedestrian_roads.groupby('index_right')['length_m'].sum().reindex(neighborhoods_df.index, fill_value=0)\n",
    "    neighborhoods_df['pedestrian_road_density'] = pedestrian_lengths / (neighborhoods_df['area_km2'] * 1000)\n",
    "    logging.info(f\"Pedestrian road density stats:\\n{neighborhoods_df['pedestrian_road_density'].describe()}\")\n",
    "    \n",
    "    # Precompute normalization constants for raw features\n",
    "    ndvi_min, ndvi_max = neighborhoods_df['ndvi_mean'].min(), neighborhoods_df['ndvi_mean'].max()\n",
    "    tree_min, tree_max = neighborhoods_df['tree_count'].min(), neighborhoods_df['tree_count'].max()\n",
    "    transit_min, transit_max = neighborhoods_df['transit_count'].min(), neighborhoods_df['transit_count'].max()\n",
    "    intersection_density_min = neighborhoods_df['intersection_density'].min()\n",
    "    intersection_density_max = neighborhoods_df['intersection_density'].max()\n",
    "    accident_count_min, accident_count_max = neighborhoods_df['accident_count'].min(), neighborhoods_df['accident_count'].max()\n",
    "    accident_density_max = neighborhoods_df['avg_road_accident_density'].max()\n",
    "    pedestrian_road_max = neighborhoods_df['pedestrian_road_density'].max()\n",
    "    \n",
    "    # Compute accident_count_density_max for safety score\n",
    "    accident_count_density = neighborhoods_df['accident_count'] / neighborhoods_df['area_km2'].replace(0, 1e-6)\n",
    "    accident_count_density_max = accident_count_density.max() if accident_count_density.max() > 0 else 1.0  # Avoid division by zero\n",
    "    \n",
    "    components = {\n",
    "        'LIE_NAME': [],\n",
    "        'land_use_diversity': [],\n",
    "        'green_space_score': [],\n",
    "        'transit_score': [],\n",
    "        'road_connectivity': [],\n",
    "        'safety_score': [],\n",
    "        'elderly_accessibility': [],\n",
    "        'pedestrian_infrastructure_score': [],\n",
    "        'walkability_score': [],\n",
    "        'walkability_category': []\n",
    "    }\n",
    "    \n",
    "    for idx, row in tqdm(neighborhoods_df.iterrows(), total=len(neighborhoods_df), desc=\"Computing walkability scores\"):\n",
    "        # Land Use Diversity (weighted by walkability desirability)\n",
    "        land_use_cols = [f\"land_use_{category.lower()}_percent\" for category in CATEGORY_PRIORITY.keys()]\n",
    "        land_use_values = [row.get(col, 0.0) / 100 for col in land_use_cols if col in row]\n",
    "        land_use_weights_list = [land_use_weights[category.lower()] for category in CATEGORY_PRIORITY.keys()]\n",
    "        weighted_values = [p * w for p, w in zip(land_use_values, land_use_weights_list) if p > 0]\n",
    "        if weighted_values:\n",
    "            total = sum(weighted_values)\n",
    "            if total > 0:\n",
    "                weighted_values = [v / total for v in weighted_values]\n",
    "                entropy = -np.sum([p * np.log2(p + 1e-10) for p in weighted_values])\n",
    "                max_entropy = np.log2(len(weighted_values))\n",
    "                land_use_diversity = entropy / max_entropy if max_entropy > 0 else 0\n",
    "            else:\n",
    "                land_use_diversity = 0\n",
    "        else:\n",
    "            land_use_diversity = 0\n",
    "        \n",
    "        # Green Space and Comfort (use linear scaling for better variation)\n",
    "        ndvi_normalized = ((row['ndvi_mean'] - ndvi_min) / (ndvi_max - ndvi_min + 1e-6)) if (ndvi_max - ndvi_min) > 0 else 0\n",
    "        tree_density = row['tree_count'] / row['area_km2'] if row['area_km2'] > 0 else 0\n",
    "        tree_density_max = (neighborhoods_df['tree_count'] / neighborhoods_df['area_km2']).replace(0, 1e-6).max()\n",
    "        tree_density_normalized = (tree_density / (tree_density_max + 1e-6)) if tree_density_max > 0 else 0\n",
    "        open_area = row.get('land_use_city_open_area_percent', 0.0) / 100\n",
    "        green_space_score = (0.4 * ndvi_normalized + 0.3 * tree_density_normalized + 0.3 * open_area)\n",
    "        \n",
    "        # Transit Accessibility (use linear scaling for better variation)\n",
    "        transit_raw = (row['transit_count'] - transit_min) / (transit_max - transit_min + 1e-6) if (transit_max - transit_min) > 0 else 0\n",
    "        transit_score = transit_raw\n",
    "        \n",
    "        # Road Connectivity (use linear scaling for better variation)\n",
    "        intersection_density = row['intersection_density']\n",
    "        intersection_density_normalized = (intersection_density - intersection_density_min) / (intersection_density_max - intersection_density_min + 1e-6) if (intersection_density_max - intersection_density_min) > 0 else 0\n",
    "        road_connectivity = intersection_density_normalized\n",
    "        \n",
    "        # Safety Score (adjust weights to balance contribution)\n",
    "        accident_count_density = row['accident_count'] / row['area_km2'] if row['area_km2'] > 0 else 0\n",
    "        accident_count_density = min(accident_count_density, accident_count_density_max * 0.5)\n",
    "        safety_score_count = 1 - (accident_count_density / (accident_count_density_max + 1e-6)) if accident_count_density_max > 0 else 1\n",
    "        accident_density = row['avg_road_accident_density']\n",
    "        safety_score_roads = 1 - (accident_density / (accident_density_max + 1e-6)) if accident_density_max > 0 else 1\n",
    "        pedestrian_roads = row.get('pedestrian_road_density', 0.0)\n",
    "        pedestrian_roads_safety = pedestrian_roads / (pedestrian_road_max + 1e-6) if pedestrian_road_max > 0 else 0\n",
    "        safety_score = (0.3 * safety_score_count + 0.3 * safety_score_roads + 0.4 * pedestrian_roads_safety) if row['accident_count'] != 0 else (0.5 * safety_score_roads + 0.5 * pedestrian_roads_safety)\n",
    "        \n",
    "        # Elderly Accessibility (adjust weights)\n",
    "        elderly_percentage = row['elderly_percentage'] / 100\n",
    "        medical_access = row.get('land_use_medical_percent', 0.0) / 100\n",
    "        elderly_accessibility = 0.3 * elderly_percentage + 0.4 * medical_access + 0.3 * pedestrian_roads_safety\n",
    "        \n",
    "        # Pedestrian Infrastructure (use linear scaling for better variation)\n",
    "        pedestrian_road_score = pedestrian_roads / (pedestrian_road_max + 1e-6) if pedestrian_road_max > 0 else 0\n",
    "        amenity_access = row.get('land_use_amenity_percent', 0.0) / 100\n",
    "        pedestrian_infrastructure_score = 0.5 * pedestrian_road_score + 0.5 * amenity_access\n",
    "        \n",
    "        # Walkability Score with adjusted weights to increase variation\n",
    "        base_score = (\n",
    "            0.25 * land_use_diversity +  # Increase weight\n",
    "            0.25 * green_space_score +   # Increase weight\n",
    "            0.15 * transit_score +\n",
    "            0.15 * road_connectivity +\n",
    "            0.20 * pedestrian_infrastructure_score  # Reduce weight to balance\n",
    "        )\n",
    "        safety_modifier = 0.7 + 0.3 * safety_score  # Reduce impact to increase variation\n",
    "        elderly_modifier = 1 + elderly_accessibility * 0.1  # Reduce impact to increase variation\n",
    "        walkability_score = base_score * safety_modifier * elderly_modifier\n",
    "        walkability_score = np.clip(walkability_score, 0, 1)\n",
    "        \n",
    "        # Categorize walkability score\n",
    "        if pd.isna(walkability_score):\n",
    "            category = 'low'\n",
    "        elif walkability_score < 0.33:\n",
    "            category = 'low'\n",
    "        elif walkability_score < 0.66:\n",
    "            category = 'medium'\n",
    "        else:\n",
    "            category = 'high'\n",
    "        \n",
    "        components['LIE_NAME'].append(row['LIE_NAME'])\n",
    "        components['land_use_diversity'].append(land_use_diversity)\n",
    "        components['green_space_score'].append(green_space_score)\n",
    "        components['transit_score'].append(transit_score)\n",
    "        components['road_connectivity'].append(road_connectivity)\n",
    "        components['safety_score'].append(safety_score)\n",
    "        components['elderly_accessibility'].append(elderly_accessibility)\n",
    "        components['pedestrian_infrastructure_score'].append(pedestrian_infrastructure_score)\n",
    "        components['walkability_score'].append(walkability_score)\n",
    "        components['walkability_category'].append(category)\n",
    "    \n",
    "    result_df = pd.DataFrame(components)\n",
    "    logging.info(f\"Walkability score distribution:\\n{result_df['walkability_score'].describe()}\")\n",
    "    logging.info(f\"Walkability category distribution:\\n{result_df['walkability_category'].value_counts()}\")\n",
    "    return result_df\n",
    "\n",
    "def compute_walkability_scores(G, data):\n",
    "    \"\"\"Compute walkability scores for all neighborhoods in the graph.\"\"\"\n",
    "    logging.info(\"Computing walkability scores for all neighborhoods...\")\n",
    "    \n",
    "    neighborhoods_gdf = data['neighborhoods']\n",
    "    walkability_df = compute_walkability_components_all(neighborhoods_gdf, data)\n",
    "    \n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    nodes_df = nodes_df.merge(\n",
    "        walkability_df[['LIE_NAME', 'land_use_diversity', 'green_space_score', 'transit_score', \n",
    "                       'road_connectivity', 'safety_score', 'elderly_accessibility', \n",
    "                       'pedestrian_infrastructure_score', 'walkability_score', 'walkability_category']],\n",
    "        on='LIE_NAME',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill NaN values for non-neighborhood nodes\n",
    "    for col in ['land_use_diversity', 'green_space_score', 'transit_score', 'road_connectivity', \n",
    "                'safety_score', 'elderly_accessibility', 'pedestrian_infrastructure_score', \n",
    "                'walkability_score', 'walkability_category']:\n",
    "        nodes_df[col] = nodes_df[col].fillna(0 if col != 'walkability_category' else 'low')\n",
    "    \n",
    "    G._nodes = cudf.from_pandas(nodes_df)\n",
    "    logging.info(\"Finished computing walkability scores.\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d0683",
   "metadata": {},
   "source": [
    "Cell 4 Main Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3d3ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "from shapely import make_valid\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    logging.info(\"Stage 1: Loading and preparing data...\")\n",
    "    \n",
    "    # Define file paths and their corresponding keys\n",
    "    data_files = {\n",
    "        'neighborhoods': LANDUSE_NDVI_PATH,\n",
    "        'buildings': OSM_BUILDINGS_PATH,\n",
    "        'roads': OSM_ROADS_PATH,\n",
    "        'trees': OSM_TREES_PATH,\n",
    "        'transit': OSM_TRANSIT_PATH,\n",
    "        'urban_masterplan': URBAN_MASTERPLAN_PATH,\n",
    "        'accidents': ACCIDENTS_PATH,\n",
    "        'population': POPULATION_PATH\n",
    "    }\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # Load data with progress bar\n",
    "    for key, path in tqdm(data_files.items(), desc=\"Loading files\"):\n",
    "        try:\n",
    "            if key == 'population':\n",
    "                with open(path, 'r') as f:\n",
    "                    data[key] = pd.DataFrame(json.load(f))\n",
    "                # Log columns of population_df to debug missing columns\n",
    "                logging.info(f\"Columns in population_df after loading: {list(data[key].columns)}\")\n",
    "            elif path.endswith('.geoparquet'):\n",
    "                data[key] = gpd.read_parquet(path)\n",
    "            else:\n",
    "                data[key] = gpd.read_file(path)\n",
    "            logging.info(f\"Loaded {key} with shape {data[key].shape}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load {key} from {path}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Log columns of neighborhoods_gdf to debug missing 'area_km2'\n",
    "    neighborhoods_gdf = data['neighborhoods']\n",
    "    logging.info(f\"Columns in neighborhoods_gdf after loading: {list(neighborhoods_gdf.columns)}\")\n",
    "    \n",
    "    # Ensure all GeoDataFrames are in the same CRS\n",
    "    target_crs = 'EPSG:3826'\n",
    "    for key in ['neighborhoods', 'buildings', 'roads', 'trees', 'transit', 'urban_masterplan', 'accidents']:\n",
    "        if key in data and isinstance(data[key], gpd.GeoDataFrame):\n",
    "            if data[key].crs != target_crs:\n",
    "                data[key] = data[key].to_crs(target_crs)\n",
    "                logging.info(f\"Converted {key} to CRS {target_crs}\")\n",
    "    \n",
    "    # Fix geometries in all GeoDataFrames\n",
    "    for key in ['neighborhoods', 'buildings', 'roads', 'trees', 'transit', 'urban_masterplan', 'accidents']:\n",
    "        if key in data and isinstance(data[key], gpd.GeoDataFrame):\n",
    "            data[key]['geometry'] = data[key]['geometry'].apply(fix_geometry)\n",
    "            invalid_geoms = data[key][~data[key].geometry.is_valid]\n",
    "            if not invalid_geoms.empty:\n",
    "                logging.warning(f\"Found {len(invalid_geoms)} invalid geometries in {key} after fixing.\")\n",
    "                data[key] = data[key][data[key].geometry.is_valid]\n",
    "    \n",
    "    # Compute intersections for neighborhoods\n",
    "    logging.info(\"Computing intersections for neighborhoods...\")\n",
    "    roads_gdf = data['roads']\n",
    "    \n",
    "    # Log columns of roads_gdf to debug missing 'length_m'\n",
    "    logging.info(f\"Columns in roads_gdf after loading: {list(roads_gdf.columns)}\")\n",
    "    \n",
    "    # Extract endpoints from road segments\n",
    "    logging.info(\"Extracting endpoints from road segments...\")\n",
    "    endpoints = []\n",
    "    road_indices = []\n",
    "    for idx, row in tqdm(roads_gdf.iterrows(), total=len(roads_gdf), desc=\"Extracting endpoints\"):\n",
    "        geom = row['geometry']\n",
    "        if geom.geom_type == 'LineString':\n",
    "            coords = list(geom.coords)\n",
    "            start_point = Point(coords[0])\n",
    "            end_point = Point(coords[-1])\n",
    "            if start_point.is_valid and end_point.is_valid:\n",
    "                endpoints.extend([start_point, end_point])\n",
    "                road_indices.extend([idx, idx])\n",
    "        elif geom.geom_type == 'MultiLineString':\n",
    "            for line in geom.geoms:\n",
    "                coords = list(line.coords)\n",
    "                start_point = Point(coords[0])\n",
    "                end_point = Point(coords[-1])\n",
    "                if start_point.is_valid and end_point.is_valid:\n",
    "                    endpoints.extend([start_point, end_point])\n",
    "                    road_indices.extend([idx, idx])\n",
    "    \n",
    "    if not endpoints:\n",
    "        logging.warning(\"No valid endpoints extracted from road segments. Using fallback method for intersections.\")\n",
    "        neighborhoods_gdf = data['neighborhoods']\n",
    "        road_neighborhoods = gpd.sjoin(roads_gdf[['geometry']], neighborhoods_gdf[['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "        intersection_counts = road_neighborhoods.groupby('index_right').size()\n",
    "        neighborhoods_gdf['intersection_count'] = intersection_counts.reindex(neighborhoods_gdf.index, fill_value=0)\n",
    "    else:\n",
    "        endpoints_gdf = gpd.GeoDataFrame({'geometry': endpoints, 'road_idx': road_indices}, crs=target_crs)\n",
    "        \n",
    "        # Create a spatial index for endpoints\n",
    "        endpoints_sindex = endpoints_gdf.sindex\n",
    "        \n",
    "        # Cluster endpoints to identify intersections (points shared by 3 or more roads)\n",
    "        logging.info(\"Building endpoint-to-road mapping...\")\n",
    "        endpoint_to_roads = {}\n",
    "        for idx, point in tqdm(endpoints_gdf.iterrows(), total=len(endpoints_gdf), desc=\"Building endpoint-to-road mapping\"):\n",
    "            point_geom = point['geometry']\n",
    "            road_idx = point['road_idx']\n",
    "            point_key = (round(point_geom.x, 6), round(point_geom.y, 6))  # Round to avoid floating-point precision issues\n",
    "            if point_key not in endpoint_to_roads:\n",
    "                endpoint_to_roads[point_key] = set()\n",
    "            endpoint_to_roads[point_key].add(road_idx)\n",
    "        \n",
    "        logging.info(\"Identifying intersections...\")\n",
    "        intersections = []\n",
    "        for point_key, road_ids in tqdm(endpoint_to_roads.items(), desc=\"Identifying intersections\"):\n",
    "            if len(road_ids) >= 3:  # Intersection if shared by 3 or more roads\n",
    "                intersections.append(Point(point_key))\n",
    "        \n",
    "        if not intersections:\n",
    "            logging.warning(\"No intersections found using endpoint clustering. Using fallback method.\")\n",
    "            neighborhoods_gdf = data['neighborhoods']\n",
    "            road_neighborhoods = gpd.sjoin(roads_gdf[['geometry']], neighborhoods_gdf[['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "            intersection_counts = road_neighborhoods.groupby('index_right').size()\n",
    "            neighborhoods_gdf['intersection_count'] = intersection_counts.reindex(neighborhoods_gdf.index, fill_value=0)\n",
    "        else:\n",
    "            intersections_gdf = gpd.GeoDataFrame({'geometry': intersections}, crs=target_crs)\n",
    "            \n",
    "            # Count intersections per neighborhood\n",
    "            logging.info(\"Counting intersections per neighborhood...\")\n",
    "            neighborhoods_gdf = data['neighborhoods']\n",
    "            intersections_joined = gpd.sjoin(intersections_gdf, neighborhoods_gdf[['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "            intersection_counts = intersections_joined.groupby('index_right').size()\n",
    "            neighborhoods_gdf['intersection_count'] = intersection_counts.reindex(neighborhoods_gdf.index, fill_value=0)\n",
    "    \n",
    "    # Compute or verify area_km2\n",
    "    if 'area_km2' not in neighborhoods_gdf.columns:\n",
    "        logging.warning(\"'area_km2' column missing in neighborhoods_gdf. Computing from geometry...\")\n",
    "        # Compute area in square meters, then convert to square kilometers\n",
    "        neighborhoods_gdf['area_m2'] = neighborhoods_gdf['geometry'].area\n",
    "        neighborhoods_gdf['area_km2'] = neighborhoods_gdf['area_m2'] / 1_000_000  # Convert m² to km²\n",
    "        logging.info(f\"Computed area_km2 stats:\\n{neighborhoods_gdf['area_km2'].describe()}\")\n",
    "    else:\n",
    "        logging.info(f\"area_km2 already present. Stats:\\n{neighborhoods_gdf['area_km2'].describe()}\")\n",
    "    \n",
    "    # Compute intersection density\n",
    "    neighborhoods_gdf['intersection_density'] = neighborhoods_gdf['intersection_count'] / neighborhoods_gdf['area_km2'].replace(0, 1e-6)\n",
    "    logging.info(f\"Intersection count stats:\\n{neighborhoods_gdf['intersection_count'].describe()}\")\n",
    "    logging.info(f\"Intersection density stats:\\n{neighborhoods_gdf['intersection_density'].describe()}\")\n",
    "    \n",
    "    # Cache the result\n",
    "    try:\n",
    "        neighborhoods_gdf.to_parquet(INTERSECTION_CACHE_PATH)\n",
    "        logging.info(f\"Saved neighborhoods with intersections to {INTERSECTION_CACHE_PATH}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save neighborhoods with intersections: {e}\")\n",
    "    \n",
    "    data['neighborhoods'] = neighborhoods_gdf\n",
    "    \n",
    "    # Compute tree count per neighborhood\n",
    "    logging.info(\"Computing tree count per neighborhood...\")\n",
    "    trees_gdf = data['trees']\n",
    "    trees_joined = gpd.sjoin(trees_gdf[['geometry']], data['neighborhoods'][['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "    tree_counts = trees_joined.groupby('index_right').size()\n",
    "    data['neighborhoods']['tree_count'] = tree_counts.reindex(data['neighborhoods'].index, fill_value=0)\n",
    "    \n",
    "    # Compute transit count per neighborhood\n",
    "    logging.info(\"Computing transit count per neighborhood...\")\n",
    "    transit_gdf = data['transit']\n",
    "    transit_joined = gpd.sjoin(transit_gdf[['geometry']], data['neighborhoods'][['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "    transit_counts = transit_joined.groupby('index_right').size()\n",
    "    data['neighborhoods']['transit_count'] = transit_counts.reindex(data['neighborhoods'].index, fill_value=0)\n",
    "    \n",
    "    # Compute accident count per neighborhood\n",
    "    logging.info(\"Computing accident count per neighborhood...\")\n",
    "    accidents_gdf = data['accidents']\n",
    "    accidents_buffered = accidents_gdf.copy()\n",
    "    accidents_buffered['geometry'] = accidents_buffered['geometry'].buffer(BUFFER_DISTANCE)\n",
    "    accidents_joined = gpd.sjoin(accidents_buffered[['geometry']], data['neighborhoods'][['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "    accident_counts = accidents_joined.groupby('index_right').size()\n",
    "    data['neighborhoods']['accident_count'] = accident_counts.reindex(data['neighborhoods'].index, fill_value=0)\n",
    "    \n",
    "    # Compute road density per neighborhood\n",
    "    logging.info(\"Computing road density per neighborhood...\")\n",
    "    roads_gdf = data['roads']\n",
    "    \n",
    "    # Log columns of roads_gdf to debug missing 'length_m'\n",
    "    logging.info(f\"Columns in roads_gdf before computing road density: {list(roads_gdf.columns)}\")\n",
    "    \n",
    "    # Compute length_m if missing\n",
    "    if 'length_m' not in roads_gdf.columns:\n",
    "        logging.warning(\"'length_m' column missing in roads_gdf. Computing from geometry...\")\n",
    "        roads_gdf['length_m'] = roads_gdf['geometry'].length  # Length in meters (since CRS is EPSG:3826)\n",
    "        logging.info(f\"Computed length_m stats:\\n{roads_gdf['length_m'].describe()}\")\n",
    "    \n",
    "    roads_joined = gpd.sjoin(roads_gdf[['geometry', 'length_m']], data['neighborhoods'][['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "    road_lengths = roads_joined.groupby('index_right')['length_m'].sum()\n",
    "    data['neighborhoods']['road_density'] = road_lengths.reindex(data['neighborhoods'].index, fill_value=0) / (data['neighborhoods']['area_km2'] * 1000)\n",
    "    logging.info(f\"Road density stats:\\n{data['neighborhoods']['road_density'].describe()}\")\n",
    "    \n",
    "    # Merge population data\n",
    "    logging.info(\"Merging population data...\")\n",
    "    population_df = data['population']\n",
    "    population_df['LIE_NAME'] = population_df['LIE_NAME'].astype(str).str.strip()\n",
    "    data['neighborhoods']['LIE_NAME'] = data['neighborhoods']['LIE_NAME'].astype(str).str.strip()\n",
    "    \n",
    "    # Check for possible column names for total_population and elderly_percentage\n",
    "    expected_cols = ['total_population', 'elderly_percentage']\n",
    "    population_cols = list(population_df.columns)\n",
    "    missing_cols = [col for col in expected_cols if col not in population_cols]\n",
    "    \n",
    "    if missing_cols:\n",
    "        logging.warning(f\"Expected columns {missing_cols} not found in population_df. Attempting to find alternatives...\")\n",
    "        # Possible alternative names\n",
    "        total_pop_alt = None\n",
    "        elderly_alt = None\n",
    "        for col in population_cols:\n",
    "            col_lower = col.lower()\n",
    "            if 'population' in col_lower and total_pop_alt is None:\n",
    "                total_pop_alt = col\n",
    "                logging.info(f\"Found alternative for total_population: {col}\")\n",
    "            if 'elderly' in col_lower and elderly_alt is None:\n",
    "                elderly_alt = col\n",
    "                logging.info(f\"Found alternative for elderly_percentage: {col}\")\n",
    "        \n",
    "        # Rename columns if alternatives are found\n",
    "        if total_pop_alt:\n",
    "            population_df = population_df.rename(columns={total_pop_alt: 'total_population'})\n",
    "        else:\n",
    "            logging.warning(\"No alternative found for total_population. Setting to 0.\")\n",
    "            population_df['total_population'] = 0\n",
    "        if elderly_alt:\n",
    "            population_df = population_df.rename(columns={elderly_alt: 'elderly_percentage'})\n",
    "        else:\n",
    "            logging.warning(\"No alternative found for elderly_percentage. Setting to 0.\")\n",
    "            population_df['elderly_percentage'] = 0\n",
    "    \n",
    "    # Perform the merge\n",
    "    data['neighborhoods'] = data['neighborhoods'].merge(\n",
    "        population_df[['LIE_NAME', 'total_population', 'elderly_percentage']],\n",
    "        on='LIE_NAME',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Compute land use percentages\n",
    "    logging.info(\"Computing land use percentages for neighborhoods...\")\n",
    "    urban_masterplan_gdf = data['urban_masterplan']\n",
    "    print_percentage_calculation(data['neighborhoods'], urban_masterplan_gdf, sample_size=3)\n",
    "    \n",
    "    for idx, row in data['neighborhoods'].iterrows():\n",
    "        neighborhood_geom = row['geometry']\n",
    "        if not neighborhood_geom.is_valid:\n",
    "            continue\n",
    "        relevant_masterplan = urban_masterplan_gdf[urban_masterplan_gdf.intersects(neighborhood_geom)]\n",
    "        if relevant_masterplan.empty:\n",
    "            continue\n",
    "        \n",
    "        temp_gdf = gpd.GeoDataFrame({'geometry': [neighborhood_geom]}, crs='EPSG:3826')\n",
    "        intersected = gpd.overlay(temp_gdf, relevant_masterplan, how='intersection', keep_geom_type=False)\n",
    "        if intersected.empty:\n",
    "            continue\n",
    "        \n",
    "        intersected['geometry'] = intersected['geometry'].apply(fix_geometry)\n",
    "        intersected = intersected[intersected.geometry.is_valid & ~intersected.geometry.is_empty]\n",
    "        if intersected.empty:\n",
    "            continue\n",
    "        \n",
    "        intersected['priority'] = intersected['Category'].map(CATEGORY_PRIORITY)\n",
    "        intersected = intersected.sort_values(by='priority', ascending=False)\n",
    "        \n",
    "        total_area = intersected.geometry.union_all().area\n",
    "        remaining_geom = intersected.geometry.union_all()\n",
    "        category_areas = {}\n",
    "        for category in intersected['Category'].unique():\n",
    "            category_rows = intersected[intersected['Category'] == category]\n",
    "            category_geom = category_rows.geometry.union_all()\n",
    "            try:\n",
    "                category_area_geom = category_geom.intersection(remaining_geom)\n",
    "                category_area = category_area_geom.area\n",
    "                category_areas[category] = category_area\n",
    "                remaining_geom = remaining_geom.difference(category_area_geom)\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Topology error for category {category} in neighborhood {row['LIE_NAME']}: {e}\")\n",
    "                category_areas[category] = 0.0\n",
    "        \n",
    "        for category in CATEGORY_PRIORITY.keys():\n",
    "            area = category_areas.get(category, 0.0)\n",
    "            percentage = (area / total_area * 100) if total_area > 0 else 0.0\n",
    "            data['neighborhoods'].at[idx, f'land_use_{category.lower()}_percent'] = percentage\n",
    "    \n",
    "    # Fill NaN values in land use percentages\n",
    "    for category in CATEGORY_PRIORITY.keys():\n",
    "        col = f'land_use_{category.lower()}_percent'\n",
    "        data['neighborhoods'][col] = data['neighborhoods'][col].fillna(0.0)\n",
    "    \n",
    "    # Fill NaN values in other columns\n",
    "    for col in ['intersection_count', 'intersection_density', 'tree_count', 'transit_count', 'accident_count', 'road_density', 'total_population', 'elderly_percentage']:\n",
    "        data['neighborhoods'][col] = data['neighborhoods'][col].fillna(0)\n",
    "    \n",
    "    # Print data structure summary\n",
    "    print_data_structure(data)\n",
    "    \n",
    "    logging.info(\"Finished loading and preparing data.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48993e82",
   "metadata": {},
   "source": [
    "Cell 5 compute_intersection_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2d1971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intersection_counts(neighborhoods_gdf, roads_gdf):\n",
    "    logging.info(\"Computing intersection counts for neighborhoods...\")\n",
    "    \n",
    "    # Extract endpoints from road segments\n",
    "    def get_endpoints(line):\n",
    "        if line is None or line.is_empty:\n",
    "            return []\n",
    "        coords = list(line.coords)\n",
    "        return [Point(coords[0]), Point(coords[-1])]\n",
    "    \n",
    "    endpoints = []\n",
    "    for idx, row in tqdm(roads_gdf.iterrows(), total=len(roads_gdf), desc=\"Extracting endpoints\"):\n",
    "        points = get_endpoints(row['geometry'])\n",
    "        for point in points:\n",
    "            endpoints.append({'geometry': point, 'road_idx': idx})\n",
    "    \n",
    "    endpoints_gdf = gpd.GeoDataFrame(endpoints, crs='EPSG:3826')\n",
    "    \n",
    "    # Build a mapping of endpoints to road indices\n",
    "    endpoint_to_roads = {}\n",
    "    for idx, row in tqdm(endpoints_gdf.iterrows(), total=len(endpoints_gdf), desc=\"Building endpoint-to-road mapping\"):\n",
    "        point = row['geometry']\n",
    "        road_idx = row['road_idx']\n",
    "        point_tuple = (point.x, point.y)\n",
    "        if point_tuple not in endpoint_to_roads:\n",
    "            endpoint_to_roads[point_tuple] = set()\n",
    "        endpoint_to_roads[point_tuple].add(road_idx)\n",
    "    \n",
    "    # Identify intersections (endpoints shared by 3 or more roads)\n",
    "    intersections = []\n",
    "    for point_tuple, road_indices in tqdm(endpoint_to_roads.items(), desc=\"Identifying intersections\"):\n",
    "        if len(road_indices) >= 3:  # Intersection if 3 or more roads share the endpoint\n",
    "            intersections.append({'geometry': Point(point_tuple)})\n",
    "    \n",
    "    if not intersections:\n",
    "        logging.warning(\"No intersections found. Setting intersection counts to 0.\")\n",
    "        neighborhoods_gdf['intersection_count'] = 0\n",
    "        neighborhoods_gdf['intersection_density'] = 0.0\n",
    "        return neighborhoods_gdf\n",
    "    \n",
    "    intersections_gdf = gpd.GeoDataFrame(intersections, crs='EPSG:3826')\n",
    "    \n",
    "    # Spatial join to count intersections per neighborhood\n",
    "    intersection_counts = gpd.sjoin(\n",
    "        neighborhoods_gdf[['geometry', 'LIE_NAME']],\n",
    "        intersections_gdf,\n",
    "        how='left',\n",
    "        predicate='contains'\n",
    "    )\n",
    "    intersection_counts = intersection_counts.groupby('LIE_NAME').size().reindex(neighborhoods_gdf['LIE_NAME'], fill_value=0)\n",
    "    neighborhoods_gdf['intersection_count'] = intersection_counts\n",
    "    \n",
    "    # Compute intersection density (intersections per km²)\n",
    "    neighborhoods_gdf['intersection_density'] = neighborhoods_gdf['intersection_count'] / neighborhoods_gdf['area_km2']\n",
    "    neighborhoods_gdf['intersection_density'] = neighborhoods_gdf['intersection_density'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    logging.info(f\"Intersection count stats: {neighborhoods_gdf['intersection_count'].describe()}\")\n",
    "    logging.info(f\"Intersection density stats: {neighborhoods_gdf['intersection_density'].describe()}\")\n",
    "    \n",
    "    return neighborhoods_gdf\n",
    "\n",
    "def build_graph(data, force_recompute=False):\n",
    "    logging.info(\"Stage 2: Building the graph...\")\n",
    "    \n",
    "    # Compute data hash to check if graph needs recomputing\n",
    "    data_hash = compute_data_hash(data)\n",
    "    cached_hash = None\n",
    "    if os.path.exists(GRAPH_DATA_HASH_PATH):\n",
    "        with open(GRAPH_DATA_HASH_PATH, 'r') as f:\n",
    "            cached_hash = f.read().strip()\n",
    "    \n",
    "    if not force_recompute and cached_hash == data_hash and all(\n",
    "        os.path.exists(path) for path in [GRAPH_NODES_CACHE_PATH, GRAPH_EDGES_CACHE_PATH, GRAPH_NODE_ID_CACHE_PATH]\n",
    "    ):\n",
    "        logging.info(\"Data unchanged. Loading graph from cache...\")\n",
    "        nodes_df = cudf.read_parquet(GRAPH_NODES_CACHE_PATH)\n",
    "        edges_df = cudf.read_parquet(GRAPH_EDGES_CACHE_PATH)\n",
    "        with open(GRAPH_NODE_ID_CACHE_PATH, 'r') as f:\n",
    "            node_id_to_index = json.load(f)\n",
    "        G = cugraph.Graph()\n",
    "        G.from_cudf_edgelist(\n",
    "            edges_df,\n",
    "            source='src',\n",
    "            destination='dst',\n",
    "            edge_attr='weight'\n",
    "        )\n",
    "        G._nodes = nodes_df\n",
    "        logging.info(\"Graph loaded from cache.\")\n",
    "        return G\n",
    "    \n",
    "    neighborhoods_gdf = data['neighborhoods'].copy()\n",
    "    buildings_gdf = data['buildings'].copy()\n",
    "    roads_gdf = data['roads'].copy()\n",
    "    trees_gdf = data['trees'].copy()\n",
    "    transit_gdf = data['transit'].copy()\n",
    "    \n",
    "    # Create nodes for neighborhoods, buildings, roads, trees, and transit\n",
    "    nodes = []\n",
    "    node_id_to_index = {}\n",
    "    current_idx = 0\n",
    "    \n",
    "    # Neighborhood nodes\n",
    "    for idx, row in neighborhoods_gdf.iterrows():\n",
    "        node_id = f\"neighborhood_{row['LIE_NAME']}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'neighborhood',\n",
    "            'LIE_NAME': row['LIE_NAME'],\n",
    "            'geometry': row['geometry'].centroid,\n",
    "            'ndvi_mean': row['ndvi_mean'],\n",
    "            'total_population': row['total_population'],\n",
    "            'elderly_percentage': row['elderly_percentage'],\n",
    "            'area_km2': row['area_km2']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    # Building nodes\n",
    "    for idx, row in buildings_gdf.iterrows():\n",
    "        node_id = f\"building_{idx}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'building',\n",
    "            'LIE_NAME': None,\n",
    "            'geometry': row['geometry'].centroid,\n",
    "            'building_type': row['building'],\n",
    "            'area_m2': row['area_m2']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    # Road nodes\n",
    "    for idx, row in roads_gdf.iterrows():\n",
    "        node_id = f\"road_{idx}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'road',\n",
    "            'LIE_NAME': None,\n",
    "            'geometry': row['geometry'].centroid,\n",
    "            'class': row['class'],\n",
    "            'length_m': row['length_m']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    # Tree nodes\n",
    "    for idx, row in trees_gdf.iterrows():\n",
    "        node_id = f\"tree_{idx}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'tree',\n",
    "            'LIE_NAME': None,\n",
    "            'geometry': row['geometry']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    # Transit nodes\n",
    "    for idx, row in transit_gdf.iterrows():\n",
    "        node_id = f\"transit_{idx}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'transit',\n",
    "            'LIE_NAME': None,\n",
    "            'geometry': row['geometry'],\n",
    "            'class': row['class']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    nodes_df = pd.DataFrame(nodes)\n",
    "    nodes_gdf = gpd.GeoDataFrame(nodes_df, geometry='geometry', crs='EPSG:3826')\n",
    "    nodes_df = cudf.from_pandas(nodes_df.drop(columns=['geometry']))\n",
    "    \n",
    "    # Create edges based on spatial proximity\n",
    "    edges = []\n",
    "    nodes_gdf_sindex = nodes_gdf.sindex\n",
    "    \n",
    "    # Neighborhood-to-neighborhood edges (shared borders)\n",
    "    logging.info(\"Creating neighborhood-to-neighborhood edges...\")\n",
    "    for idx1, row1 in neighborhoods_gdf.iterrows():\n",
    "        geom1 = row1['geometry']\n",
    "        node_idx1 = node_id_to_index[f\"neighborhood_{row1['LIE_NAME']}\"]\n",
    "        possible_matches = list(nodes_gdf_sindex.query(geom1, predicate='intersects'))\n",
    "        for idx2 in possible_matches:\n",
    "            row2 = nodes_gdf.iloc[idx2]\n",
    "            if row2['type'] != 'neighborhood':\n",
    "                continue\n",
    "            if row1['LIE_NAME'] == row2['LIE_NAME']:\n",
    "                continue\n",
    "            geom2 = neighborhoods_gdf[neighborhoods_gdf['LIE_NAME'] == row2['LIE_NAME']]['geometry'].iloc[0]\n",
    "            if geom1.intersects(geom2):\n",
    "                node_idx2 = node_id_to_index[f\"neighborhood_{row2['LIE_NAME']}\"]\n",
    "                edges.append({\n",
    "                    'src': node_idx1,\n",
    "                    'dst': node_idx2,\n",
    "                    'weight': 1.0\n",
    "                })\n",
    "    \n",
    "    # Other edges (neighborhood to building, road, tree, transit)\n",
    "    logging.info(\"Creating edges between neighborhoods and other entities...\")\n",
    "    for idx, row in tqdm(nodes_gdf.iterrows(), total=len(nodes_gdf), desc=\"Creating edges\"):\n",
    "        if row['type'] == 'neighborhood':\n",
    "            continue\n",
    "        geom = row['geometry']\n",
    "        possible_matches = list(neighborhoods_gdf.sindex.query(geom, predicate='contains'))\n",
    "        for match_idx in possible_matches:\n",
    "            neighborhood = neighborhoods_gdf.iloc[match_idx]\n",
    "            if neighborhood['geometry'].contains(geom):\n",
    "                node_idx1 = node_id_to_index[row['node_id']]\n",
    "                node_idx2 = node_id_to_index[f\"neighborhood_{neighborhood['LIE_NAME']}\"]\n",
    "                weight = 1.0\n",
    "                if row['type'] == 'transit':\n",
    "                    weight = 2.0  # Higher weight for transit nodes\n",
    "                edges.append({\n",
    "                    'src': node_idx1,\n",
    "                    'dst': node_idx2,\n",
    "                    'weight': weight\n",
    "                })\n",
    "                edges.append({\n",
    "                    'src': node_idx2,\n",
    "                    'dst': node_idx1,\n",
    "                    'weight': weight\n",
    "                })\n",
    "    \n",
    "    edges_df = cudf.DataFrame(edges)\n",
    "    \n",
    "    # Build the graph\n",
    "    G = cugraph.Graph()\n",
    "    G.from_cudf_edgelist(\n",
    "        edges_df,\n",
    "        source='src',\n",
    "        destination='dst',\n",
    "        edge_attr='weight'\n",
    "    )\n",
    "    G._nodes = nodes_df\n",
    "    \n",
    "    # Cache the graph\n",
    "    nodes_df.to_parquet(GRAPH_NODES_CACHE_PATH)\n",
    "    edges_df.to_parquet(GRAPH_EDGES_CACHE_PATH)\n",
    "    with open(GRAPH_NODE_ID_CACHE_PATH, 'w') as f:\n",
    "        json.dump(node_id_to_index, f)\n",
    "    with open(GRAPH_DATA_HASH_PATH, 'w') as f:\n",
    "        f.write(data_hash)\n",
    "    \n",
    "    logging.info(\"Graph construction completed.\")\n",
    "    return G\n",
    "\n",
    "def prepare_gnn_data(G):\n",
    "    logging.info(\"Stage 3: Preparing data for GNN...\")\n",
    "    \n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    edges_df = G.edgelist.edgelist_df.to_pandas()\n",
    "    \n",
    "    # Create node features\n",
    "    feature_columns = [\n",
    "        'ndvi_mean', 'total_population', 'elderly_percentage', 'area_km2',\n",
    "        'area_m2', 'length_m'\n",
    "    ]\n",
    "    features = []\n",
    "    for idx, row in nodes_df.iterrows():\n",
    "        node_features = []\n",
    "        for col in feature_columns:\n",
    "            value = row.get(col, 0.0)\n",
    "            if pd.isna(value):\n",
    "                value = 0.0\n",
    "            node_features.append(value)\n",
    "        \n",
    "        # One-hot encode node type\n",
    "        node_type = row['type']\n",
    "        type_encoding = [0] * 5  # 5 types: neighborhood, building, road, tree, transit\n",
    "        type_mapping = {\n",
    "            'neighborhood': 0,\n",
    "            'building': 1,\n",
    "            'road': 2,\n",
    "            'tree': 3,\n",
    "            'transit': 4\n",
    "        }\n",
    "        type_idx = type_mapping.get(node_type, 0)\n",
    "        type_encoding[type_idx] = 1\n",
    "        node_features.extend(type_encoding)\n",
    "        \n",
    "        features.append(node_features)\n",
    "    \n",
    "    feature_matrix = np.array(features, dtype=np.float32)\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    numerical_features = feature_matrix[:, :len(feature_columns)]\n",
    "    means = numerical_features.mean(axis=0)\n",
    "    stds = numerical_features.std(axis=0)\n",
    "    stds[stds == 0] = 1  # Avoid division by zero\n",
    "    numerical_features = (numerical_features - means) / stds\n",
    "    feature_matrix[:, :len(feature_columns)] = numerical_features\n",
    "    \n",
    "    # Create edge indices for PyG\n",
    "    edge_index = torch.tensor(\n",
    "        np.array([edges_df['src'].values, edges_df['dst'].values]),\n",
    "        dtype=torch.long\n",
    "    )\n",
    "    edge_attr = torch.tensor(edges_df['weight'].values, dtype=torch.float)\n",
    "    \n",
    "    # Create target (walkability score) for neighborhood nodes\n",
    "    y = np.zeros(len(nodes_df), dtype=np.float32)\n",
    "    if 'walkability_score' in nodes_df.columns:\n",
    "        walkability_scores = nodes_df['walkability_score'].fillna(0).values\n",
    "        mask = nodes_df['type'] == 'neighborhood'\n",
    "        y[mask] = walkability_scores[mask]\n",
    "    else:\n",
    "        logging.warning(\"Walkability scores not found in nodes_df. Setting targets to 0.\")\n",
    "    \n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    \n",
    "    node_type_mapping = {\n",
    "        'neighborhood': 0,\n",
    "        'building': 1,\n",
    "        'road': 2,\n",
    "        'tree': 3,\n",
    "        'transit': 4\n",
    "    }\n",
    "    node_type = nodes_df['type'].map(node_type_mapping).fillna(-1).astype(int).values\n",
    "    node_type = torch.tensor(node_type, dtype=torch.long)\n",
    "    \n",
    "    data = Data(\n",
    "        x=torch.tensor(feature_matrix, dtype=torch.float),\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=y,\n",
    "        node_type=node_type\n",
    "    )\n",
    "    \n",
    "    logging.info(\"GNN data prepared.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b04d9d",
   "metadata": {},
   "source": [
    "Cell 6: Graph Construction (build_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4158e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def compute_neighborhood_neighborhood_edges(args):\n",
    "    idx, row, neighborhoods_gdf, neighborhood_sindex = args\n",
    "    edges = []\n",
    "    geom = row['geometry']\n",
    "    possible_matches_index = list(neighborhood_sindex.intersection(geom.bounds))\n",
    "    for other_idx in possible_matches_index:\n",
    "        if other_idx != idx:\n",
    "            other_row = neighborhoods_gdf.iloc[other_idx]\n",
    "            other_geom = other_row['geometry']\n",
    "            try:\n",
    "                if geom.buffer(1e-3).intersects(other_geom.buffer(1e-3)) or geom.buffer(1e-3).touches(other_geom.buffer(1e-3)):\n",
    "                    src = f\"neighborhood_{idx}\"\n",
    "                    dst = f\"neighborhood_{other_idx}\"\n",
    "                    edges.append({'src': src, 'dst': dst})\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error checking intersection between neighborhood {idx} and {other_idx}: {e}\")\n",
    "    return edges\n",
    "\n",
    "def compute_neighborhood_building_edges(args):\n",
    "    idx, row, buildings_gdf, building_sindex = args\n",
    "    edges = []\n",
    "    geom = row['geometry']\n",
    "    possible_matches_index = list(building_sindex.intersection(geom.bounds))\n",
    "    for building_idx in possible_matches_index:\n",
    "        building_row = buildings_gdf.iloc[building_idx]\n",
    "        building_geom = building_row['geometry']\n",
    "        try:\n",
    "            if geom.buffer(1e-3).intersects(building_geom.buffer(1e-3)):\n",
    "                src = f\"neighborhood_{idx}\"\n",
    "                dst = f\"building_{building_idx}\"\n",
    "                edges.append({'src': src, 'dst': dst})\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error checking intersection between neighborhood {idx} and building {building_idx}: {e}\")\n",
    "    return edges\n",
    "\n",
    "def compute_neighborhood_road_edges(args):\n",
    "    idx, row, roads_gdf, road_sindex = args\n",
    "    edges = []\n",
    "    geom = row['geometry']\n",
    "    possible_matches_index = list(road_sindex.intersection(geom.bounds))\n",
    "    for road_idx in possible_matches_index:\n",
    "        road_row = roads_gdf.iloc[road_idx]\n",
    "        road_geom = road_row['geometry']\n",
    "        try:\n",
    "            if geom.buffer(1e-3).intersects(road_geom.buffer(1e-3)):\n",
    "                src = f\"neighborhood_{idx}\"\n",
    "                dst = f\"road_{road_idx}\"\n",
    "                edges.append({'src': src, 'dst': dst})\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error checking intersection between neighborhood {idx} and road {road_idx}: {e}\")\n",
    "    return edges\n",
    "\n",
    "def build_graph(data, force_recompute=False):\n",
    "    logging.info(\"Stage 2: Building city graph...\")\n",
    "    current_hash = compute_data_hash(data)\n",
    "    nodes_df = None\n",
    "    edges_df = None\n",
    "    node_id_to_index = {}\n",
    "\n",
    "    if not force_recompute and os.path.exists(GRAPH_NODES_CACHE_PATH) and os.path.exists(GRAPH_EDGES_CACHE_PATH):\n",
    "        try:\n",
    "            with open(GRAPH_DATA_HASH_PATH, 'r') as f:\n",
    "                cached_hash = f.read()\n",
    "            if cached_hash == current_hash:\n",
    "                logging.info(\"Data hash matches cached hash. Loading graph from cache...\")\n",
    "                nodes_df = cudf.read_parquet(GRAPH_NODES_CACHE_PATH)\n",
    "                edges_df = cudf.read_parquet(GRAPH_EDGES_CACHE_PATH)\n",
    "                with open(GRAPH_NODE_ID_CACHE_PATH, 'r') as f:\n",
    "                    node_id_to_index = json.load(f)\n",
    "                G = cugraph.Graph()\n",
    "                G._nodes = nodes_df\n",
    "                if not edges_df.empty:\n",
    "                    G.from_cudf_edgelist(edges_df, source='src', destination='dst')\n",
    "                logging.info(f\"Loaded graph from cache: {len(nodes_df)} nodes, {len(edges_df)} edges\")\n",
    "                return G\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load cached graph: {e}. Recomputing graph...\")\n",
    "\n",
    "    logging.info(\"Constructing graph nodes...\")\n",
    "    neighborhoods_gdf = data['neighborhoods']\n",
    "    buildings_gdf = data['buildings']\n",
    "    roads_gdf = data['roads']\n",
    "\n",
    "    # Compute area_m2 for buildings if not present\n",
    "    if 'area_m2' not in buildings_gdf.columns:\n",
    "        logging.warning(\"'area_m2' column missing in buildings_gdf. Computing from geometry...\")\n",
    "        buildings_gdf['area_m2'] = buildings_gdf['geometry'].area\n",
    "        logging.info(f\"Computed area_m2 stats:\\n{buildings_gdf['area_m2'].describe()}\")\n",
    "\n",
    "    # Create nodes for neighborhoods\n",
    "    logging.info(\"Adding neighborhood nodes...\")\n",
    "    neighborhood_nodes = []\n",
    "    for idx, row in tqdm(neighborhoods_gdf.iterrows(), total=len(neighborhoods_gdf), desc=\"Neighborhood nodes\"):\n",
    "        node_id = f\"neighborhood_{idx}\"\n",
    "        node_id_to_index[node_id] = idx\n",
    "        neighborhood_nodes.append({\n",
    "            'vertex': node_id,\n",
    "            'type': 'neighborhood',\n",
    "            'LIE_NAME': row['LIE_NAME'],\n",
    "            'ndvi_mean': row['ndvi_mean'],\n",
    "            'tree_count': row['tree_count'],\n",
    "            'transit_count': row['transit_count'],\n",
    "            'accident_count': row['accident_count'],\n",
    "            'road_density': row['road_density'],\n",
    "            'intersection_density': row['intersection_density'],\n",
    "            'total_population': row['total_population'],\n",
    "            'elderly_percentage': row['elderly_percentage'],\n",
    "            # Add land use features for later use\n",
    "            **{f'land_use_{category.lower()}_percent': row.get(f'land_use_{category.lower()}_percent', 0.0) for category in CATEGORY_PRIORITY.keys()}\n",
    "        })\n",
    "\n",
    "    # Create nodes for buildings\n",
    "    logging.info(\"Adding building nodes...\")\n",
    "    building_nodes = []\n",
    "    for idx, row in tqdm(buildings_gdf.iterrows(), total=len(buildings_gdf), desc=\"Building nodes\"):\n",
    "        node_id = f\"building_{idx}\"\n",
    "        node_id_to_index[node_id] = idx + len(neighborhoods_gdf)\n",
    "        building_nodes.append({\n",
    "            'vertex': node_id,\n",
    "            'type': 'building',\n",
    "            'building': row['building'],\n",
    "            'area_m2': row['area_m2']\n",
    "        })\n",
    "\n",
    "    logging.info(\"Adding road nodes...\")\n",
    "    road_nodes = []\n",
    "    for idx, row in tqdm(roads_gdf.iterrows(), total=len(roads_gdf), desc=\"Road nodes\"):\n",
    "        node_id = f\"road_{idx}\"\n",
    "        node_id_to_index[node_id] = idx + len(neighborhoods_gdf) + len(buildings_gdf)\n",
    "        road_nodes.append({\n",
    "            'vertex': node_id,\n",
    "            'type': 'road',\n",
    "            'class': row['class'],\n",
    "            'length_m': row['length_m']\n",
    "        })\n",
    "\n",
    "    # Combine all nodes\n",
    "    nodes = neighborhood_nodes + building_nodes + road_nodes\n",
    "    nodes_df = cudf.DataFrame(nodes)\n",
    "\n",
    "    # Create spatial indices\n",
    "    logging.info(\"Creating spatial indices...\")\n",
    "    neighborhoods_gdf = neighborhoods_gdf.copy()\n",
    "    buildings_gdf = buildings_gdf.copy()\n",
    "    roads_gdf = roads_gdf.copy()\n",
    "\n",
    "    neighborhoods_gdf['geometry'] = neighborhoods_gdf['geometry'].apply(fix_geometry)\n",
    "    buildings_gdf['geometry'] = buildings_gdf['geometry'].apply(fix_geometry)\n",
    "    roads_gdf['geometry'] = roads_gdf['geometry'].apply(fix_geometry)\n",
    "\n",
    "    # Drop rows with invalid geometries\n",
    "    neighborhoods_gdf = neighborhoods_gdf[neighborhoods_gdf.geometry.is_valid & ~neighborhoods_gdf.geometry.is_empty]\n",
    "    buildings_gdf = buildings_gdf[buildings_gdf.geometry.is_valid & ~buildings_gdf.geometry.is_empty]\n",
    "    roads_gdf = roads_gdf[roads_gdf.geometry.is_valid & ~roads_gdf.geometry.is_empty]\n",
    "\n",
    "    if neighborhoods_gdf.empty or buildings_gdf.empty or roads_gdf.empty:\n",
    "        logging.error(\"One or more GeoDataFrames are empty after geometry validation.\")\n",
    "        raise ValueError(\"GeoDataFrames cannot be empty after geometry validation.\")\n",
    "\n",
    "    neighborhood_sindex = neighborhoods_gdf.sindex\n",
    "    building_sindex = buildings_gdf.sindex\n",
    "    road_sindex = roads_gdf.sindex\n",
    "\n",
    "    # Create edges\n",
    "    logging.info(\"Creating edges based on spatial proximity...\")\n",
    "    edges = []\n",
    "    \n",
    "    # Parallelize neighborhood-neighborhood edges\n",
    "    logging.info(\"Computing neighborhood-neighborhood edges...\")\n",
    "    with Pool() as pool:\n",
    "        tasks = [(i, row, neighborhoods_gdf, neighborhood_sindex) for i, row in neighborhoods_gdf.iterrows()]\n",
    "        results = list(tqdm(pool.imap(compute_neighborhood_neighborhood_edges, tasks), total=len(tasks), desc=\"Neighborhood-Neighborhood edges\"))\n",
    "    for batch in results:\n",
    "        edges.extend(batch)\n",
    "    logging.info(f\"Created {len(edges)} neighborhood-neighborhood edges\")\n",
    "    \n",
    "    # Parallelize neighborhood-building edges\n",
    "    logging.info(\"Computing neighborhood-building edges...\")\n",
    "    with Pool() as pool:\n",
    "        tasks = [(i, row, buildings_gdf, building_sindex) for i, row in neighborhoods_gdf.iterrows()]\n",
    "        results = list(tqdm(pool.imap(compute_neighborhood_building_edges, tasks), total=len(tasks), desc=\"Neighborhood-Building edges\"))\n",
    "    for batch in results:\n",
    "        edges.extend(batch)\n",
    "    logging.info(f\"Created {len(edges)} total edges after neighborhood-building\")\n",
    "    \n",
    "    # Parallelize neighborhood-road edges\n",
    "    logging.info(\"Computing neighborhood-road edges...\")\n",
    "    with Pool() as pool:\n",
    "        tasks = [(i, row, roads_gdf, road_sindex) for i, row in neighborhoods_gdf.iterrows()]\n",
    "        results = list(tqdm(pool.imap(compute_neighborhood_road_edges, tasks), total=len(tasks), desc=\"Neighborhood-Road edges\"))\n",
    "    for batch in results:\n",
    "        edges.extend(batch)\n",
    "    logging.info(f\"Created {len(edges)} total edges after neighborhood-road\")\n",
    "    \n",
    "    edges_df = cudf.DataFrame(edges)\n",
    "    \n",
    "    # Validate edges (relaxed validation)\n",
    "    valid_nodes = set(nodes_df['vertex'].to_pandas())\n",
    "    if edges_df.empty:\n",
    "        logging.warning(\"No edges created. Graph will have nodes but no edges.\")\n",
    "    else:\n",
    "        edges_df = edges_df[edges_df['src'].isin(valid_nodes) & edges_df['dst'].isin(valid_nodes)]\n",
    "        logging.info(f\"After validation, {len(edges_df)} edges remain\")\n",
    "        if not edges_df.empty:\n",
    "            logging.info(f\"Sample edges after validation:\\n{edges_df.head().to_pandas()}\")\n",
    "    \n",
    "    # Create the graph\n",
    "    G = cugraph.Graph()\n",
    "    G._nodes = nodes_df\n",
    "    if not edges_df.empty:\n",
    "        G.from_cudf_edgelist(edges_df, source='src', destination='dst')\n",
    "    else:\n",
    "        logging.warning(\"No valid edges created. Graph will have nodes but no edges.\")\n",
    "    \n",
    "    # Save graph data to cache\n",
    "    logging.info(\"Saving graph data to cache...\")\n",
    "    try:\n",
    "        nodes_df.to_parquet(GRAPH_NODES_CACHE_PATH)\n",
    "        edges_df.to_parquet(GRAPH_EDGES_CACHE_PATH)\n",
    "        with open(GRAPH_DATA_HASH_PATH, 'w') as f:\n",
    "            f.write(current_hash)\n",
    "        with open(GRAPH_NODE_ID_CACHE_PATH, 'w') as f:\n",
    "            json.dump(node_id_to_index, f)\n",
    "        logging.info(\"Successfully saved graph data to cache.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save graph data to cache: {e}\")\n",
    "    \n",
    "    logging.info(f\"City graph constructed: {len(nodes_df)} nodes, {len(edges_df)} edges\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14e080",
   "metadata": {},
   "source": [
    "Cell 7: Rule-Based Walkability Scores (compute_walkability_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ef4aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_walkability_scores(G, data):\n",
    "    \"\"\"Compute walkability scores for neighborhoods and update the graph.\"\"\"\n",
    "    logging.info(\"Computing walkability scores for neighborhoods...\")\n",
    "    \n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    \n",
    "    # Use the original neighborhoods GeoDataFrame\n",
    "    walkability_components = compute_walkability_components_all(data['neighborhoods'], data)\n",
    "    \n",
    "    # Standardize LIE_NAME for merging\n",
    "    nodes_df['LIE_NAME'] = nodes_df['LIE_NAME'].astype(str).str.strip()\n",
    "    walkability_components['LIE_NAME'] = walkability_components['LIE_NAME'].astype(str).str.strip()\n",
    "    \n",
    "    # Log for debugging\n",
    "    logging.info(f\"Number of neighborhood nodes in nodes_df: {len(nodes_df[nodes_df['type'] == 'neighborhood'])}\")\n",
    "    logging.info(f\"Number of entries in walkability_components: {len(walkability_components)}\")\n",
    "    logging.info(f\"Sample LIE_NAME in nodes_df: {nodes_df['LIE_NAME'].head().tolist()}\")\n",
    "    logging.info(f\"Sample LIE_NAME in walkability_components: {walkability_components['LIE_NAME'].head().tolist()}\")\n",
    "    \n",
    "    # Map LIE_NAME to walkability_score and walkability_category\n",
    "    nodes_df['walkability_score'] = np.nan\n",
    "    nodes_df['walkability_category'] = np.nan\n",
    "    walkability_dict = dict(zip(walkability_components['LIE_NAME'], walkability_components['walkability_score']))\n",
    "    category_dict = dict(zip(walkability_components['LIE_NAME'], walkability_components['walkability_category']))\n",
    "    nodes_df['walkability_score'] = nodes_df['LIE_NAME'].map(walkability_dict)\n",
    "    nodes_df['walkability_category'] = nodes_df['LIE_NAME'].map(category_dict)\n",
    "    \n",
    "    # Check for unmatched nodes\n",
    "    unmatched = nodes_df[nodes_df['type'] == 'neighborhood']['walkability_score'].isna().sum()\n",
    "    if unmatched > 0:\n",
    "        logging.error(f\"Found {unmatched} neighborhood nodes without walkability scores. This should not happen after fixing computation.\")\n",
    "        raise ValueError(\"Failed to assign walkability scores to all neighborhood nodes.\")\n",
    "    \n",
    "    G._nodes = cudf.from_pandas(nodes_df)\n",
    "    \n",
    "    logging.info(\"Finished computing walkability scores.\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989dbf84",
   "metadata": {},
   "source": [
    "Cell 8 prepare_gnn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a057108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_gnn_data(G):\n",
    "    \"\"\"Prepare data for GNN training.\"\"\"\n",
    "    logging.info(\"Preparing data for GNN training...\")\n",
    "    \n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    \n",
    "    # Define features for different node types\n",
    "    feature_sets = {\n",
    "        'neighborhood': ['ndvi_mean', 'tree_count', 'transit_count', 'accident_count', \n",
    "                         'road_density', 'intersection_density', 'total_population', 'elderly_percentage',\n",
    "                         'land_use_residential_percent', 'land_use_commercial_percent', \n",
    "                         'land_use_education_percent', 'land_use_city_open_area_percent',\n",
    "                         'land_use_public_transportation_percent', 'land_use_pedestrian_percent'],\n",
    "        'building': ['area_m2'],\n",
    "        'road': ['length_m']\n",
    "    }\n",
    "    \n",
    "    # Determine the maximum number of features (based on neighborhood nodes)\n",
    "    max_features = max(len(features) for features in feature_sets.values())\n",
    "    \n",
    "    # Normalize features for each node type and pad to max_features\n",
    "    feature_data = []\n",
    "    for node_type, features in tqdm(feature_sets.items(), desc=\"Normalizing features by node type\"):\n",
    "        type_nodes = nodes_df[nodes_df['type'] == node_type]\n",
    "        if type_nodes.empty:\n",
    "            continue\n",
    "        \n",
    "        node_features = []\n",
    "        for feature in features:\n",
    "            if feature not in type_nodes.columns:\n",
    "                logging.warning(f\"Feature column {feature} not found in nodes_df for type {node_type}. Setting to 0.\")\n",
    "                type_nodes[feature] = 0\n",
    "            feature_values = type_nodes[feature].astype(float).fillna(0)\n",
    "            feature_max = feature_values.max()\n",
    "            feature_min = feature_values.min()\n",
    "            if feature_max == feature_min:\n",
    "                logging.warning(f\"Feature {feature} for type {node_type} has no variation (max=min). Setting to 0.\")\n",
    "                normalized = np.zeros_like(feature_values)\n",
    "            else:\n",
    "                normalized = (feature_values - feature_min) / (feature_max - feature_min)\n",
    "            node_features.append(normalized)\n",
    "        \n",
    "        # Stack features for this node type\n",
    "        feature_matrix = np.stack(node_features, axis=1)\n",
    "        \n",
    "        # Pad with zeros to match max_features\n",
    "        if feature_matrix.shape[1] < max_features:\n",
    "            padding = np.zeros((feature_matrix.shape[0], max_features - feature_matrix.shape[1]))\n",
    "            feature_matrix = np.hstack((feature_matrix, padding))\n",
    "        \n",
    "        feature_data.append((type_nodes.index, feature_matrix))\n",
    "    \n",
    "    # Combine features for all nodes\n",
    "    all_indices = []\n",
    "    all_features = []\n",
    "    for indices, features in feature_data:\n",
    "        all_indices.extend(indices)\n",
    "        all_features.append(features)\n",
    "    \n",
    "    all_features = np.vstack(all_features)\n",
    "    nodes_df.loc[all_indices, 'feature_index'] = np.arange(len(all_indices))\n",
    "    \n",
    "    # Map node types to numerical values\n",
    "    type_to_int = {'neighborhood': 0, 'building': 1, 'road': 2}\n",
    "    node_type = nodes_df['type'].map(type_to_int).astype(int).values\n",
    "    node_type_tensor = torch.tensor(node_type, dtype=torch.long)\n",
    "    \n",
    "    # Prepare edge index\n",
    "    if G.edgelist is not None and not G.edgelist.edgelist_df.empty:\n",
    "        edges_df = G.edgelist.edgelist_df.to_pandas()\n",
    "        \n",
    "        # Create a dictionary for fast lookups of feature_index\n",
    "        vertex_to_feature_index = dict(zip(nodes_df['vertex'], nodes_df['feature_index']))\n",
    "        \n",
    "        edge_index = []\n",
    "        for _, edge in tqdm(edges_df.iterrows(), total=len(edges_df), desc=\"Preparing edge index\"):\n",
    "            src_idx = vertex_to_feature_index.get(edge['src'])\n",
    "            dst_idx = vertex_to_feature_index.get(edge['dst'])\n",
    "            if src_idx is not None and dst_idx is not None:\n",
    "                edge_index.append([int(src_idx), int(dst_idx)])\n",
    "        \n",
    "        if edge_index:\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        else:\n",
    "            logging.warning(\"No valid edges after mapping node IDs to indices. GNN will treat nodes independently.\")\n",
    "            edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "    else:\n",
    "        logging.warning(\"No edges in the graph. GNN will treat nodes independently.\")\n",
    "        edge_index = torch.zeros((2, 0), dtype=torch.long)\n",
    "    \n",
    "    # Prepare node features and labels\n",
    "    x = torch.tensor(all_features, dtype=torch.float)\n",
    "    y = torch.tensor(nodes_df['walkability_score'].fillna(0).values, dtype=torch.float)\n",
    "    \n",
    "    # Create masks for neighborhood nodes (for training)\n",
    "    neighborhood_mask = nodes_df['type'] == 'neighborhood'\n",
    "    train_mask = torch.zeros(len(nodes_df), dtype=torch.bool)\n",
    "    train_mask[neighborhood_mask] = True\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, y=y, train_mask=train_mask, node_type=node_type_tensor)\n",
    "    \n",
    "    logging.info(f\"Prepared GNN data: {len(nodes_df)} nodes, {edge_index.shape[1]} edges\")\n",
    "    logging.info(f\"Feature matrix shape: {x.shape}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a9a6b5",
   "metadata": {},
   "source": [
    "Cell 9: WalkabilityGNN, train_gnn_model, predict_walkability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "635ecc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WalkabilityGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super(WalkabilityGNN, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)\n",
    "        # Add a linear layer for cases with no edges\n",
    "        self.linear = torch.nn.Linear(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        if edge_index.shape[1] == 0:  # No edges\n",
    "            logging.warning(\"No edges in the graph. Using linear layer for node features only.\")\n",
    "            return self.linear(x).squeeze()\n",
    "        x = self.conv1(x, edge_index, edge_attr).relu()\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return x\n",
    "\n",
    "def train_gnn_model(data):\n",
    "    \"\"\"Train the GNN model to predict walkability scores.\"\"\"\n",
    "    logging.info(\"Stage 4: Training GNN model...\")\n",
    "    \n",
    "    # Validate input data\n",
    "    if data.x.shape[0] == 0 or data.y.shape[0] == 0:\n",
    "        logging.error(\"Input data for GNN training is empty.\")\n",
    "        raise ValueError(\"Input data for GNN training cannot be empty.\")\n",
    "    \n",
    "    # Log target distribution\n",
    "    y_stats = pd.Series(data.y.cpu().numpy()).describe()\n",
    "    logging.info(f\"Target (walkability_score) distribution:\\n{y_stats}\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = WalkabilityGNN(\n",
    "        in_channels=data.x.shape[1],\n",
    "        hidden_channels=128,\n",
    "        out_channels=1,\n",
    "        heads=4\n",
    "    ).to(device)\n",
    "    data = data.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    # Use the existing train_mask for splitting\n",
    "    train_mask = data.train_mask\n",
    "    num_train_nodes = train_mask.sum().item()\n",
    "    if num_train_nodes == 0:\n",
    "        logging.error(\"No nodes available for training. Check train_mask.\")\n",
    "        raise ValueError(\"No nodes available for training.\")\n",
    "    \n",
    "    # Split training nodes into train and validation sets (80-20 split)\n",
    "    indices = torch.arange(num_train_nodes, device=device)\n",
    "    train_size = int(0.8 * num_train_nodes)\n",
    "    train_indices, val_indices = indices[:train_size], indices[train_size:]\n",
    "    \n",
    "    full_train_mask = data.train_mask.clone()\n",
    "    full_val_mask = data.train_mask.clone()\n",
    "    \n",
    "    # Map indices back to the original node indices\n",
    "    train_node_indices = torch.where(train_mask)[0][train_indices]\n",
    "    val_node_indices = torch.where(train_mask)[0][val_indices]\n",
    "    \n",
    "    train_mask = torch.zeros_like(full_train_mask, dtype=torch.bool)\n",
    "    val_mask = torch.zeros_like(full_val_mask, dtype=torch.bool)\n",
    "    train_mask[train_node_indices] = True\n",
    "    val_mask[val_node_indices] = True\n",
    "    \n",
    "    # Early stopping parameters\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 15\n",
    "    patience_counter = 0\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in tqdm(range(100), desc=\"Training epochs\"):  # Reduced max epochs for efficiency\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr).squeeze()\n",
    "        train_loss = F.mse_loss(out[train_mask], data.y[train_mask])\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = F.mse_loss(out[val_mask], data.y[val_mask])\n",
    "        model.train()\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            logging.info(f\"Epoch {epoch}, Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "        \n",
    "        # Early stopping based on validation loss\n",
    "        if val_loss.item() < best_val_loss:\n",
    "            best_val_loss = val_loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            logging.info(f\"Early stopping at epoch {epoch}. Best validation loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "\n",
    "    model.eval()\n",
    "    logging.info(\"Finished training GNN model.\")\n",
    "    return model\n",
    "\n",
    "def predict_walkability(G, model):\n",
    "    \"\"\"Predict walkability scores using the trained GNN model.\"\"\"\n",
    "    logging.info(\"Predicting walkability with GNN...\")\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    data = prepare_gnn_data(G)\n",
    "    data = data.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(data.x, data.edge_index, data.edge_attr).squeeze()\n",
    "\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    # Apply sigmoid and clip to ensure predictions are in [0, 1]\n",
    "    predictions = 1 / (1 + np.exp(-predictions))\n",
    "    predictions = np.clip(predictions, 0, 1)\n",
    "\n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    nodes_df['walkability_gnn'] = predictions\n",
    "    \n",
    "    # Ensure no NaN values in predictions\n",
    "    if nodes_df['walkability_gnn'].isna().any():\n",
    "        logging.warning(f\"Found {nodes_df['walkability_gnn'].isna().sum()} NaN values in GNN predictions. Filling with 0.\")\n",
    "        nodes_df['walkability_gnn'] = nodes_df['walkability_gnn'].fillna(0)\n",
    "    \n",
    "    # Log prediction distribution\n",
    "    prediction_stats = pd.Series(predictions).describe()\n",
    "    logging.info(f\"GNN prediction (walkability_gnn) distribution:\\n{prediction_stats}\")\n",
    "    \n",
    "    G._nodes = cudf.from_pandas(nodes_df)\n",
    "\n",
    "    logging.info(\"Finished predicting walkability with GNN.\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5406821f",
   "metadata": {},
   "source": [
    "Cell 10: Interactive Map Generation (create_interactive_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d059c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_map(G, data):\n",
    "    \"\"\"Generate an interactive Kepler.gl map to visualize walkability scores.\"\"\"\n",
    "    logging.info(\"Generating interactive Kepler.gl map...\")\n",
    "    \n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    neighborhoods_gdf = data['neighborhoods'].copy()\n",
    "\n",
    "    # Standardize LIE_NAME for merging\n",
    "    nodes_df['LIE_NAME'] = nodes_df['LIE_NAME'].astype(str).str.strip()\n",
    "    neighborhoods_gdf['LIE_NAME'] = neighborhoods_gdf['LIE_NAME'].astype(str).str.strip()\n",
    "\n",
    "    # Filter for neighborhood nodes and select necessary columns\n",
    "    neighborhood_nodes = nodes_df[nodes_df['type'] == 'neighborhood'][['LIE_NAME', 'walkability_score', 'walkability_gnn', 'walkability_category']]\n",
    "\n",
    "    # Log for debugging\n",
    "    nodes_lie_names = set(neighborhood_nodes['LIE_NAME'])\n",
    "    gdf_lie_names = set(neighborhoods_gdf['LIE_NAME'])\n",
    "    logging.info(f\"Neighborhood nodes count: {len(neighborhood_nodes)}\")\n",
    "    logging.info(f\"Neighborhoods_gdf count: {len(neighborhoods_gdf)}\")\n",
    "    logging.info(f\"Sample LIE_NAME in nodes_df: {list(nodes_lie_names)[:5]}\")\n",
    "    logging.info(f\"Sample LIE_NAME in neighborhoods_gdf: {list(gdf_lie_names)[:5]}\")\n",
    "    logging.info(f\"Common LIE_NAMEs: {len(nodes_lie_names & gdf_lie_names)}\")\n",
    "    logging.info(f\"Nodes LIE_NAMEs not in GDF: {list(nodes_lie_names - gdf_lie_names)}\")\n",
    "    logging.info(f\"GDF LIE_NAMEs not in nodes: {list(gdf_lie_names - nodes_lie_names)}\")\n",
    "    logging.info(f\"Nodes nulls: {neighborhood_nodes.isna().sum().to_dict()}\")\n",
    "    logging.info(f\"GDF geometry nulls: {neighborhoods_gdf['geometry'].isna().sum()}\")\n",
    "\n",
    "    # Merge data\n",
    "    map_data = neighborhoods_gdf[['LIE_NAME', 'geometry']].merge(\n",
    "        neighborhood_nodes,\n",
    "        on='LIE_NAME',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Drop duplicates in-place\n",
    "    map_data.drop_duplicates(subset=['LIE_NAME'], keep='first', inplace=True)\n",
    "\n",
    "    # Log merge results and score distributions\n",
    "    logging.info(f\"Merged map_data rows: {len(map_data)}\")\n",
    "    logging.info(f\"Walkability score nulls: {map_data['walkability_score'].isna().sum()}\")\n",
    "    logging.info(f\"Walkability GNN nulls: {map_data['walkability_gnn'].isna().sum()}\")\n",
    "    logging.info(f\"Walkability score distribution in map_data:\\n{map_data['walkability_score'].describe()}\")\n",
    "    logging.info(f\"Walkability GNN distribution in map_data:\\n{map_data['walkability_gnn'].describe()}\")\n",
    "    logging.info(f\"Walkability category distribution in map_data:\\n{map_data['walkability_category'].value_counts()}\")\n",
    "\n",
    "    # Fill NaN values\n",
    "    map_data['walkability_score'] = map_data['walkability_score'].fillna(0)\n",
    "    map_data['walkability_gnn'] = map_data['walkability_gnn'].fillna(0)\n",
    "    map_data['walkability_category'] = map_data['walkability_category'].fillna('low')\n",
    "\n",
    "    # Convert to GeoDataFrame and transform CRS\n",
    "    map_data = gpd.GeoDataFrame(map_data, geometry='geometry', crs='EPSG:3826')\n",
    "    map_data['geometry'] = map_data['geometry'].to_crs('EPSG:4326')\n",
    "    \n",
    "    # Prepare data for Kepler.gl\n",
    "    kepler_data = {\n",
    "        'neighborhoods': map_data[['LIE_NAME', 'walkability_score', 'walkability_gnn', 'walkability_category', 'geometry']].to_json()\n",
    "    }\n",
    "\n",
    "    # Kepler.gl configuration\n",
    "    config = {\n",
    "        \"version\": \"v1\",\n",
    "        \"config\": {\n",
    "            \"visState\": {\n",
    "                \"layers\": [\n",
    "                    {\n",
    "                        \"id\": \"neighborhoods\",\n",
    "                        \"type\": \"geojson\",\n",
    "                        \"config\": {\n",
    "                            \"dataId\": \"neighborhoods\",\n",
    "                            \"label\": \"Neighborhoods\",\n",
    "                            \"color\": [18, 147, 154],\n",
    "                            \"columns\": {\n",
    "                                \"geojson\": \"geometry\"\n",
    "                            },\n",
    "                            \"isVisible\": True,\n",
    "                            \"visConfig\": {\n",
    "                                \"opacity\": 0.7,\n",
    "                                \"strokeOpacity\": 0.9,\n",
    "                                \"thickness\": 1,\n",
    "                                \"strokeColor\": [255, 255, 255],\n",
    "                                \"colorRange\": {\n",
    "                                    \"name\": \"Global Warming\",\n",
    "                                    \"type\": \"sequential\",\n",
    "                                    \"colors\": [\n",
    "                                        \"#5A1846\", \"#900C3F\", \"#C70039\",\n",
    "                                        \"#E3611C\", \"#F1920E\", \"#FFC107\"\n",
    "                                    ]\n",
    "                                },\n",
    "                                \"strokeColorRange\": {\n",
    "                                    \"name\": \"Global Warming\",\n",
    "                                    \"type\": \"sequential\",\n",
    "                                    \"colors\": [\n",
    "                                        \"#5A1846\", \"#900C3F\", \"#C70039\",\n",
    "                                        \"#E3611C\", \"#F1920E\", \"#FFC107\"\n",
    "                                    ]\n",
    "                                },\n",
    "                                \"colorField\": {\n",
    "                                    \"name\": \"walkability_gnn\",\n",
    "                                    \"type\": \"real\"\n",
    "                                },\n",
    "                                \"colorScale\": \"quantile\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"visualChannels\": {\n",
    "                            \"colorField\": {\n",
    "                                \"name\": \"walkability_gnn\",\n",
    "                                \"type\": \"real\"\n",
    "                            },\n",
    "                            \"colorScale\": \"quantile\"\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"interactionConfig\": {\n",
    "                    \"tooltip\": {\n",
    "                        \"fieldsToShow\": {\n",
    "                            \"neighborhoods\": [\n",
    "                                {\"name\": \"LIE_NAME\", \"format\": None},\n",
    "                                {\"name\": \"walkability_score\", \"format\": \"{:.3f}\"},\n",
    "                                {\"name\": \"walkability_gnn\", \"format\": \"{:.3f}\"},\n",
    "                                {\"name\": \"walkability_category\", \"format\": None}\n",
    "                            ]\n",
    "                        },\n",
    "                        \"enabled\": True\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"mapState\": {\n",
    "                \"latitude\": 25.0330,\n",
    "                \"longitude\": 121.5654,\n",
    "                \"zoom\": 11\n",
    "            },\n",
    "            \"mapStyle\": {\n",
    "                \"styleType\": \"dark\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    map_1 = KeplerGl(height=800, data=kepler_data, config=config)\n",
    "    map_path = os.path.join(BASE_DIR, 'taipei_walkability_map.html')\n",
    "    map_1.save_to_html(file_name=map_path)\n",
    "    logging.info(f\"Interactive map generated and saved as {map_path}\")\n",
    "    print(f\"Map saved to {map_path}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0683d",
   "metadata": {},
   "source": [
    "Cell 11: Main Execution (main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f13b11d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 21:47:46,856 - INFO - Ensured subgraph directory exists: /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/subgraphs\n",
      "2025-04-22 21:47:46,857 - INFO - Stage 1: Loading and preparing data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting load_and_prepare_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:   0%|          | 0/8 [00:00<?, ?it/s]2025-04-22 21:47:46,961 - INFO - Loaded neighborhoods with shape (456, 57)\n",
      "Loading files:  12%|█▎        | 1/8 [00:00<00:00,  9.69it/s]2025-04-22 21:47:48,157 - INFO - Loaded buildings with shape (74306, 9)\n",
      "Loading files:  25%|██▌       | 2/8 [00:01<00:04,  1.34it/s]2025-04-22 21:47:48,209 - INFO - Loaded roads with shape (81444, 2)\n",
      "2025-04-22 21:47:48,244 - INFO - Loaded trees with shape (5019, 12)\n",
      "2025-04-22 21:47:48,319 - INFO - Loaded transit with shape (29892, 11)\n",
      "Loading files:  62%|██████▎   | 5/8 [00:01<00:00,  3.99it/s]2025-04-22 21:47:48,973 - INFO - Loaded urban_masterplan with shape (15521, 15)\n",
      "2025-04-22 21:47:50,082 - INFO - Loaded accidents with shape (56133, 8)\n",
      "Loading files:  88%|████████▊ | 7/8 [00:03<00:00,  1.97it/s]2025-04-22 21:47:50,091 - INFO - Columns in population_df after loading: ['LIE_NAME', 'Total_Population', 'Elderly_Percentage']\n",
      "2025-04-22 21:47:50,092 - INFO - Loaded population with shape (456, 3)\n",
      "Loading files: 100%|██████████| 8/8 [00:03<00:00,  2.47it/s]\n",
      "2025-04-22 21:47:50,093 - INFO - Columns in neighborhoods_gdf after loading: ['LIE_NAME', 'SECT_NAME', '2024population', 'land_use_city_open_area_count', 'land_use_city_open_area_area_m2', 'land_use_city_open_area_percent', 'land_use_commercial_count', 'land_use_commercial_area_m2', 'land_use_commercial_percent', 'land_use_infrastructure_count', 'land_use_infrastructure_area_m2', 'land_use_infrastructure_percent', 'land_use_government_count', 'land_use_government_area_m2', 'land_use_government_percent', 'land_use_public_transportation_count', 'land_use_public_transportation_area_m2', 'land_use_public_transportation_percent', 'land_use_education_count', 'land_use_education_area_m2', 'land_use_education_percent', 'land_use_medical_count', 'land_use_medical_area_m2', 'land_use_medical_percent', 'land_use_amenity_count', 'land_use_amenity_area_m2', 'land_use_amenity_percent', 'land_use_road_count', 'land_use_road_area_m2', 'land_use_road_percent', 'land_use_pedestrian_count', 'land_use_pedestrian_area_m2', 'land_use_pedestrian_percent', 'land_use_natural_count', 'land_use_natural_area_m2', 'land_use_natural_percent', 'land_use_special_zone_count', 'land_use_special_zone_area_m2', 'land_use_special_zone_percent', 'land_use_river_count', 'land_use_river_area_m2', 'land_use_river_percent', 'land_use_military_count', 'land_use_military_area_m2', 'land_use_military_percent', 'land_use_residential_count', 'land_use_residential_area_m2', 'land_use_residential_percent', 'land_use_industrial_count', 'land_use_industrial_area_m2', 'land_use_industrial_percent', 'land_use_agriculture_count', 'land_use_agriculture_area_m2', 'land_use_agriculture_percent', 'ndvi_mean', 'ndvi_median', 'geometry']\n",
      "2025-04-22 21:47:50,223 - INFO - Converted buildings to CRS EPSG:3826\n",
      "2025-04-22 21:47:50,253 - INFO - Converted trees to CRS EPSG:3826\n",
      "2025-04-22 21:47:50,276 - INFO - Converted transit to CRS EPSG:3826\n",
      "2025-04-22 21:47:50,341 - INFO - Converted urban_masterplan to CRS EPSG:3826\n",
      "2025-04-22 21:47:50,367 - INFO - Converted accidents to CRS EPSG:3826\n",
      "2025-04-22 21:47:54,802 - INFO - Computing intersections for neighborhoods...\n",
      "2025-04-22 21:47:54,803 - INFO - Columns in roads_gdf after loading: ['class', 'geometry']\n",
      "2025-04-22 21:47:54,803 - INFO - Extracting endpoints from road segments...\n",
      "Extracting endpoints: 100%|██████████| 81444/81444 [00:04<00:00, 18500.74it/s]\n",
      "2025-04-22 21:47:59,350 - INFO - Building endpoint-to-road mapping...\n",
      "Building endpoint-to-road mapping: 100%|██████████| 162888/162888 [00:04<00:00, 36172.96it/s]\n",
      "2025-04-22 21:48:03,856 - INFO - Identifying intersections...\n",
      "Identifying intersections: 100%|██████████| 101237/101237 [00:00<00:00, 1409480.73it/s]\n",
      "2025-04-22 21:48:03,943 - INFO - Counting intersections per neighborhood...\n",
      "2025-04-22 21:48:03,988 - WARNING - 'area_km2' column missing in neighborhoods_gdf. Computing from geometry...\n",
      "2025-04-22 21:48:03,990 - INFO - Computed area_km2 stats:\n",
      "count    456.000000\n",
      "mean       0.588925\n",
      "std        1.351428\n",
      "min        0.031744\n",
      "25%        0.134566\n",
      "50%        0.209650\n",
      "75%        0.425264\n",
      "max       16.324434\n",
      "Name: area_km2, dtype: float64\n",
      "2025-04-22 21:48:03,992 - INFO - Intersection count stats:\n",
      "count    456.000000\n",
      "mean      33.800439\n",
      "std       24.763978\n",
      "min        1.000000\n",
      "25%       16.000000\n",
      "50%       26.000000\n",
      "75%       46.000000\n",
      "max      185.000000\n",
      "Name: intersection_count, dtype: float64\n",
      "2025-04-22 21:48:03,994 - INFO - Intersection density stats:\n",
      "count    456.000000\n",
      "mean     126.190854\n",
      "std       83.149602\n",
      "min        2.960154\n",
      "25%       66.874411\n",
      "50%      111.292996\n",
      "75%      165.079134\n",
      "max      490.471026\n",
      "Name: intersection_density, dtype: float64\n",
      "2025-04-22 21:48:04,013 - INFO - Saved neighborhoods with intersections to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/neighborhoods_with_intersections.geoparquet\n",
      "2025-04-22 21:48:04,014 - INFO - Computing tree count per neighborhood...\n",
      "2025-04-22 21:48:04,052 - INFO - Computing transit count per neighborhood...\n",
      "2025-04-22 21:48:04,154 - INFO - Computing accident count per neighborhood...\n",
      "2025-04-22 21:48:04,989 - INFO - Computing road density per neighborhood...\n",
      "2025-04-22 21:48:04,989 - INFO - Columns in roads_gdf before computing road density: ['class', 'geometry']\n",
      "2025-04-22 21:48:04,990 - WARNING - 'length_m' column missing in roads_gdf. Computing from geometry...\n",
      "2025-04-22 21:48:04,999 - INFO - Computed length_m stats:\n",
      "count     81444.000000\n",
      "mean        145.622456\n",
      "std        2304.902398\n",
      "min           0.030284\n",
      "25%          28.160770\n",
      "50%          61.698697\n",
      "75%         130.534001\n",
      "max      426414.891763\n",
      "Name: length_m, dtype: float64\n",
      "2025-04-22 21:48:05,341 - INFO - Road density stats:\n",
      "count     456.000000\n",
      "mean      158.024830\n",
      "std       171.789340\n",
      "min         7.561174\n",
      "25%        55.500294\n",
      "50%        96.985664\n",
      "75%       214.942848\n",
      "max      1657.324822\n",
      "Name: road_density, dtype: float64\n",
      "2025-04-22 21:48:05,342 - INFO - Merging population data...\n",
      "2025-04-22 21:48:05,343 - WARNING - Expected columns ['total_population', 'elderly_percentage'] not found in population_df. Attempting to find alternatives...\n",
      "2025-04-22 21:48:05,344 - INFO - Found alternative for total_population: Total_Population\n",
      "2025-04-22 21:48:05,345 - INFO - Found alternative for elderly_percentage: Elderly_Percentage\n",
      "2025-04-22 21:48:05,348 - INFO - Computing land use percentages for neighborhoods...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Percentage Calculation Process ---\n",
      "\n",
      "Neighborhood: 板溪里 (Index: 373)\n",
      "Total unique master plan area: 63777.59 m²\n",
      "Area of City_Open_Area (priority 10): 478.13 m²\n",
      "Area of Education (priority 6): 8173.36 m²\n",
      "Area of Commercial (priority 4): 23017.42 m²\n",
      "Area of Residential (priority 3): 32108.69 m²\n",
      "\n",
      "Percentages:\n",
      "City_Open_Area: 0.75%\n",
      "Education: 12.82%\n",
      "Commercial: 36.09%\n",
      "Residential: 50.34%\n",
      "Sum of percentages: 100.00%\n",
      "\n",
      "Neighborhood: 芝山里 (Index: 39)\n",
      "Total unique master plan area: 1061285.95 m²\n",
      "Area of Education (priority 6): 80442.15 m²\n",
      "Area of Residential (priority 3): 196204.41 m²\n",
      "Area of Natural (priority 2): 775753.11 m²\n",
      "Area of River (priority 1): 2754.93 m²\n",
      "Area of Government (priority 1): 6131.35 m²\n",
      "\n",
      "Percentages:\n",
      "Education: 7.58%\n",
      "Residential: 18.49%\n",
      "Natural: 73.10%\n",
      "River: 0.26%\n",
      "Government: 0.58%\n",
      "Sum of percentages: 100.00%\n",
      "\n",
      "Neighborhood: 和平里 (Index: 340)\n",
      "Total unique master plan area: 98073.11 m²\n",
      "Area of City_Open_Area (priority 10): 4608.53 m²\n",
      "Area of Public_Transportation (priority 8): 5304.93 m²\n",
      "Area of Commercial (priority 4): 15463.83 m²\n",
      "Area of Residential (priority 3): 50273.32 m²\n",
      "Area of Special_Zone (priority 1): 22422.50 m²\n",
      "\n",
      "Percentages:\n",
      "City_Open_Area: 4.70%\n",
      "Public_Transportation: 5.41%\n",
      "Commercial: 15.77%\n",
      "Residential: 51.26%\n",
      "Special_Zone: 22.86%\n",
      "Sum of percentages: 100.00%\n",
      "--- End of Percentage Calculation Process ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 21:48:05,976 - WARNING - Topology error for category Residential in neighborhood 陽明里: TopologyException: found non-noded intersection between LINESTRING (305235 2.7814e+06, 305232 2.7814e+06) and LINESTRING (305233 2.7814e+06, 305238 2.7814e+06) at 305235.20948094811 2781398.7662400398\n",
      "2025-04-22 21:48:06,217 - WARNING - Topology error for category Natural in neighborhood 陽明里: TopologyException: found non-noded intersection between LINESTRING (305351 2.78185e+06, 305350 2.78185e+06) and LINESTRING (305350 2.78185e+06, 305350 2.78185e+06) at 305349.93489947024 2781846.2793679931\n",
      "2025-04-22 21:48:07,009 - WARNING - Topology error for category Agriculture in neighborhood 豐年里: TopologyException: found non-noded intersection between LINESTRING (299444 2.78096e+06, 299444 2.78096e+06) and LINESTRING (299444 2.78096e+06, 299444 2.78096e+06) at 299443.97226843517 2780959.3288864125\n",
      "2025-04-22 21:48:07,688 - WARNING - Topology error for category Pedestrian in neighborhood 關渡里: TopologyException: found non-noded intersection between LINESTRING (296871 2.77919e+06, 296870 2.77919e+06) and LINESTRING (296871 2.77919e+06, 296869 2.77919e+06) at 296869.99175471725 2779190.4644867191\n",
      "2025-04-22 21:48:07,819 - WARNING - Topology error for category Infrastructure in neighborhood 關渡里: TopologyException: found non-noded intersection between LINESTRING (296911 2.77884e+06, 296911 2.77884e+06) and LINESTRING (296911 2.77884e+06, 296911 2.77884e+06) at 296911.44015440479 2778839.5138171767\n",
      "2025-04-22 21:48:10,564 - WARNING - Topology error for category Government in neighborhood 福順里: TopologyException: found non-noded intersection between LINESTRING (301533 2.77502e+06, 301533 2.77502e+06) and LINESTRING (301533 2.77502e+06, 301533 2.77502e+06) at 301532.96899694926 2775020.80209503\n",
      "2025-04-22 21:48:11,856 - WARNING - Topology error for category River in neighborhood 五分里: TopologyException: found non-noded intersection between LINESTRING (312333 2.77328e+06, 312333 2.77328e+06) and LINESTRING (312332 2.77328e+06, 312333 2.77328e+06) at 312332.69765419257 2773277.367184672\n",
      "2025-04-22 21:48:12,714 - WARNING - Topology error for category Road in neighborhood 南港里: TopologyException: found non-noded intersection between LINESTRING (311644 2.77189e+06, 311657 2.7719e+06) and LINESTRING (311630 2.77189e+06, 311657 2.7719e+06) at 311657.42832653591 2771895.1006993004\n",
      "2025-04-22 21:48:13,433 - WARNING - Topology error for category Special_Zone in neighborhood 中南里: TopologyException: found non-noded intersection between LINESTRING (311955 2.77135e+06, 311955 2.77135e+06) and LINESTRING (311955 2.77135e+06, 311955 2.77135e+06) at 311955.22635489638 2771352.7457217239\n",
      "2025-04-22 21:48:13,450 - WARNING - Topology error for category Road in neighborhood 中南里: TopologyException: found non-noded intersection between LINESTRING (311345 2.77161e+06, 311344 2.77161e+06) and LINESTRING (311344 2.77161e+06, 311344 2.77161e+06) at 311344.20670072001 2771612.8331892812\n",
      "2025-04-22 21:48:13,460 - WARNING - Topology error for category Infrastructure in neighborhood 中南里: TopologyException: found non-noded intersection between LINESTRING (311955 2.77135e+06, 311955 2.77135e+06) and LINESTRING (311955 2.77135e+06, 311955 2.77135e+06) at 311955.22635489638 2771352.7457217239\n",
      "2025-04-22 21:48:13,484 - WARNING - Topology error for category Government in neighborhood 中南里: TopologyException: found non-noded intersection between LINESTRING (311038 2.77019e+06, 311035 2.77019e+06) and LINESTRING (311038 2.77019e+06, 311035 2.77019e+06) at 311037.75737428927 2770193.7686056904\n",
      "2025-04-22 21:48:13,495 - WARNING - Topology error for category River in neighborhood 中南里: TopologyException: found non-noded intersection between LINESTRING (311955 2.77135e+06, 311955 2.77135e+06) and LINESTRING (311955 2.77135e+06, 311955 2.77135e+06) at 311955.22635489638 2771352.7457217239\n",
      "2025-04-22 21:48:15,399 - WARNING - Topology error for category Infrastructure in neighborhood 舊莊里: TopologyException: found non-noded intersection between LINESTRING (312826 2.76904e+06, 312824 2.76904e+06) and LINESTRING (312825 2.76904e+06, 312824 2.76904e+06) at 312824.50008723215 2769040.8990552658\n",
      "2025-04-22 21:48:15,964 - WARNING - Topology error for category Commercial in neighborhood 安康里: TopologyException: found non-noded intersection between LINESTRING (307950 2.76963e+06, 307949 2.76962e+06) and LINESTRING (307949 2.76962e+06, 307949 2.76962e+06) at 307948.62715117587 2769624.3576414455\n",
      "2025-04-22 21:48:16,098 - WARNING - Topology error for category River in neighborhood 九如里: TopologyException: found non-noded intersection between LINESTRING (312138 2.77015e+06, 312138 2.77014e+06) and LINESTRING (312138 2.77015e+06, 312138 2.77014e+06) at 312138.34700008662 2770150.6440004231\n",
      "2025-04-22 21:48:18,451 - WARNING - Topology error for category Road in neighborhood 博嘉里: TopologyException: found non-noded intersection between LINESTRING (308001 2.76569e+06, 308001 2.76569e+06) and LINESTRING (308001 2.76569e+06, 308000 2.76568e+06) at 308000.95038661739 2765685.1649155417\n",
      "2025-04-22 21:48:18,802 - WARNING - Topology error for category Residential in neighborhood 萬盛里: TopologyException: found non-noded intersection between LINESTRING (304449 2.76649e+06, 304450 2.7665e+06) and LINESTRING (304449 2.76648e+06, 304450 2.7665e+06) at 304449.97013593506 2766495.7391462782\n",
      "2025-04-22 21:48:19,123 - WARNING - Topology error for category Commercial in neighborhood 萬興里: TopologyException: found non-noded intersection between LINESTRING (307982 2.76418e+06, 307986 2.76418e+06) and LINESTRING (307982 2.76418e+06, 307984 2.76418e+06) at 307982.32430909772 2764175.6883213609\n",
      "2025-04-22 21:48:19,339 - WARNING - Topology error for category Residential in neighborhood 萬興里: TopologyException: found non-noded intersection between LINESTRING (307982 2.76418e+06, 307986 2.76418e+06) and LINESTRING (307982 2.76418e+06, 307984 2.76418e+06) at 307982.32430909772 2764175.6883213609\n",
      "2025-04-22 21:48:19,398 - WARNING - Topology error for category Natural in neighborhood 萬興里: TopologyException: found non-noded intersection between LINESTRING (310276 2.76631e+06, 310277 2.76631e+06) and LINESTRING (310277 2.76631e+06, 310277 2.76631e+06) at 310277.46693728981 2766309.5977024199\n",
      "2025-04-22 21:48:19,511 - WARNING - Topology error for category River in neighborhood 萬興里: TopologyException: found non-noded intersection between LINESTRING (307907 2.76419e+06, 307910 2.76418e+06) and LINESTRING (307907 2.76419e+06, 307907 2.76419e+06) at 307906.68941059953 2764186.3632669998\n",
      "2025-04-22 21:48:19,553 - WARNING - Topology error for category Infrastructure in neighborhood 萬興里: TopologyException: found non-noded intersection between LINESTRING (307982 2.76418e+06, 307986 2.76418e+06) and LINESTRING (307982 2.76418e+06, 307984 2.76418e+06) at 307982.32430909772 2764175.6883213609\n",
      "2025-04-22 21:48:19,617 - WARNING - Topology error for category Road in neighborhood 萬興里: TopologyException: found non-noded intersection between LINESTRING (309012 2.76609e+06, 309011 2.76609e+06) and LINESTRING (309011 2.76609e+06, 309016 2.76609e+06) at 309011.33430958074 2766091.7154930364\n",
      "2025-04-22 21:48:20,901 - INFO - Finished loading and preparing data.\n",
      "2025-04-22 21:48:20,948 - INFO - Computing correlation between road types and accident density...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Structure Summary ---\n",
      "\n",
      "Dataset: neighborhoods\n",
      "Shape: (456, 67)\n",
      "Columns and Data Types:\n",
      "LIE_NAME                            object\n",
      "SECT_NAME                           object\n",
      "2024population                       int32\n",
      "land_use_city_open_area_count        int32\n",
      "land_use_city_open_area_area_m2    float64\n",
      "                                    ...   \n",
      "transit_count                        int64\n",
      "accident_count                       int64\n",
      "road_density                       float64\n",
      "total_population                     int64\n",
      "elderly_percentage                 float64\n",
      "Length: 67, dtype: object\n",
      "Missing values (total): 0\n",
      "Missing values per column:\n",
      "LIE_NAME                           0\n",
      "SECT_NAME                          0\n",
      "2024population                     0\n",
      "land_use_city_open_area_count      0\n",
      "land_use_city_open_area_area_m2    0\n",
      "                                  ..\n",
      "transit_count                      0\n",
      "accident_count                     0\n",
      "road_density                       0\n",
      "total_population                   0\n",
      "elderly_percentage                 0\n",
      "Length: 67, dtype: int64\n",
      "Unique LIE_NAME: 456\n",
      "Sample data (first 2 rows):\n",
      "  LIE_NAME SECT_NAME  2024population  land_use_city_open_area_count  \\\n",
      "0      湖田里       北投區             856                              0   \n",
      "1      菁山里       士林區            1509                              0   \n",
      "\n",
      "   land_use_city_open_area_area_m2  land_use_city_open_area_percent  \\\n",
      "0                              0.0                              0.0   \n",
      "1                              0.0                              0.0   \n",
      "\n",
      "   land_use_commercial_count  land_use_commercial_area_m2  \\\n",
      "0                          0                          0.0   \n",
      "1                          0                          0.0   \n",
      "\n",
      "   land_use_commercial_percent  land_use_infrastructure_count  ...  \\\n",
      "0                          0.0                              0  ...   \n",
      "1                          0.0                              4  ...   \n",
      "\n",
      "   intersection_count       area_m2   area_km2  intersection_density  \\\n",
      "0                 110  1.632443e+07  16.324434              6.738365   \n",
      "1                  94  1.171922e+07  11.719218              8.021013   \n",
      "\n",
      "   tree_count  transit_count  accident_count  road_density  total_population  \\\n",
      "0         106            400              70      7.561174               857   \n",
      "1          54            313              33      9.913151              1485   \n",
      "\n",
      "   elderly_percentage  \n",
      "0               28.24  \n",
      "1               25.72  \n",
      "\n",
      "[2 rows x 67 columns]\n",
      "\n",
      "Dataset: buildings\n",
      "Shape: (74306, 9)\n",
      "Columns and Data Types:\n",
      "full_id       object\n",
      "osm_id        object\n",
      "building      object\n",
      "屋齡            object\n",
      "建物高度          object\n",
      "地上層數          object\n",
      "構造種類          object\n",
      "使用分區          object\n",
      "geometry    geometry\n",
      "dtype: object\n",
      "Missing values (total): 49\n",
      "Missing values per column:\n",
      "full_id      0\n",
      "osm_id       0\n",
      "building    31\n",
      "屋齡           0\n",
      "建物高度         0\n",
      "地上層數        18\n",
      "構造種類         0\n",
      "使用分區         0\n",
      "geometry     0\n",
      "dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "    full_id   osm_id    building    屋齡  建物高度  地上層數     構造種類     使用分區  \\\n",
      "0  r2633015  2633015   dormitory  <NA>  <NA>  <NA>  Unknown  Unknown   \n",
      "1  r2633016  2633016  university  <NA>  <NA>  <NA>  Unknown  Unknown   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((304401.483 2780904.12, 304430.995 27...  \n",
      "1  POLYGON ((304357.549 2780790.65, 304352.56 278...  \n",
      "\n",
      "Dataset: roads\n",
      "Shape: (81444, 3)\n",
      "Columns and Data Types:\n",
      "class         object\n",
      "geometry    geometry\n",
      "length_m     float64\n",
      "dtype: object\n",
      "Missing values (total): 604\n",
      "Missing values per column:\n",
      "class       604\n",
      "geometry      0\n",
      "length_m      0\n",
      "dtype: int64\n",
      "Road class counts:\n",
      "class\n",
      "service          21575\n",
      "footway          19776\n",
      "residential      15429\n",
      "tertiary          5402\n",
      "steps             4301\n",
      "secondary         4059\n",
      "path              3857\n",
      "unclassified      1957\n",
      "primary           1292\n",
      "cycleway           878\n",
      "track              741\n",
      "trunk              609\n",
      "pedestrian         323\n",
      "motorway           316\n",
      "living_street      267\n",
      "unknown             56\n",
      "bridleway            2\n",
      "Name: count, dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "     class                                           geometry       length_m\n",
      "0     None  LINESTRING (324778.511 2780945.263, 324826.86 ...  426414.891763\n",
      "1  service  LINESTRING (296169.224 2759463.114, 296160.343...    1055.960489\n",
      "\n",
      "Dataset: trees\n",
      "Shape: (5019, 12)\n",
      "Columns and Data Types:\n",
      "id               object\n",
      "geometry       geometry\n",
      "version           int32\n",
      "sources          object\n",
      "subtype          object\n",
      "class            object\n",
      "surface          object\n",
      "names            object\n",
      "level           float64\n",
      "source_tags      object\n",
      "wikidata         object\n",
      "elevation       float64\n",
      "dtype: object\n",
      "Missing values (total): 24487\n",
      "Missing values per column:\n",
      "id                0\n",
      "geometry          0\n",
      "version           0\n",
      "sources           0\n",
      "subtype           0\n",
      "class             0\n",
      "surface        5013\n",
      "names          4553\n",
      "level          5015\n",
      "source_tags       0\n",
      "wikidata       4943\n",
      "elevation      4963\n",
      "dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "                                 id  \\\n",
      "0  08b4ba2399d31fff0003c5f62a41c335   \n",
      "1  08b4ba0ae3a22fff0003ca54f368c81d   \n",
      "\n",
      "                                            geometry  version  \\\n",
      "0  POLYGON ((161777.639 2544255.739, 161813.286 2...        0   \n",
      "1  POLYGON ((297620.639 2760911.484, 297592.092 2...        0   \n",
      "\n",
      "                                             sources subtype   class surface  \\\n",
      "0  [{'property': '', 'dataset': 'OpenStreetMap', ...    land  island    None   \n",
      "1  [{'property': '', 'dataset': 'OpenStreetMap', ...  forest    wood    None   \n",
      "\n",
      "                                               names  level  \\\n",
      "0  {'primary': '臺灣', 'common': [('af', 'Taiwan'),...    NaN   \n",
      "1                                               None    NaN   \n",
      "\n",
      "                               source_tags wikidata  elevation  \n",
      "0  [(place, island), (type, multipolygon)]   Q22502        NaN  \n",
      "1  [(natural, wood), (type, multipolygon)]     None        NaN  \n",
      "\n",
      "Dataset: transit\n",
      "Shape: (29892, 11)\n",
      "Columns and Data Types:\n",
      "id               object\n",
      "geometry       geometry\n",
      "version           int32\n",
      "sources          object\n",
      "subtype          object\n",
      "class            object\n",
      "surface          object\n",
      "names            object\n",
      "level           float64\n",
      "source_tags      object\n",
      "wikidata         object\n",
      "dtype: object\n",
      "Missing values (total): 101060\n",
      "Missing values per column:\n",
      "id                 0\n",
      "geometry           0\n",
      "version            0\n",
      "sources            0\n",
      "subtype            0\n",
      "class              0\n",
      "surface        28287\n",
      "names          17731\n",
      "level          25803\n",
      "source_tags        0\n",
      "wikidata       29239\n",
      "dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "                                 id  \\\n",
      "0  08b4ba0ac6758fff0001be0bbe7a4ada   \n",
      "1  08b4ba0ae332afff0001a76b5977464d   \n",
      "\n",
      "                                            geometry  version  \\\n",
      "0  LINESTRING (320296.896 2765488.056, 320042.372...        0   \n",
      "1                     POINT (296843.534 2759002.204)        0   \n",
      "\n",
      "                                             sources  subtype       class  \\\n",
      "0  [{'property': '', 'dataset': 'OpenStreetMap', ...    power  power_line   \n",
      "1  [{'property': '', 'dataset': 'OpenStreetMap', ...  barrier        gate   \n",
      "\n",
      "  surface names  level                         source_tags wikidata  \n",
      "0    None  None    NaN  [(power, line), (voltage, 345000)]     None  \n",
      "1    None  None    NaN                   [(barrier, gate)]     None  \n",
      "\n",
      "Dataset: urban_masterplan\n",
      "Shape: (15521, 15)\n",
      "Columns and Data Types:\n",
      "編號            object\n",
      "圖層            object\n",
      "顏色            object\n",
      "街廓編號          object\n",
      "分區代碼          object\n",
      "分區簡稱          object\n",
      "使用分區          object\n",
      "分區說明          object\n",
      "原屬分區          object\n",
      "變更前代碼         object\n",
      "變更前簡稱         object\n",
      "變更前分區         object\n",
      "Category      object\n",
      "Area         float64\n",
      "geometry    geometry\n",
      "dtype: object\n",
      "Missing values (total): 92705\n",
      "Missing values per column:\n",
      "編號              0\n",
      "圖層              5\n",
      "顏色              5\n",
      "街廓編號        15508\n",
      "分區代碼            5\n",
      "分區簡稱            5\n",
      "使用分區            0\n",
      "分區說明        15409\n",
      "原屬分區        15205\n",
      "變更前代碼       15521\n",
      "變更前簡稱       15521\n",
      "變更前分區       15521\n",
      "Category        0\n",
      "Area            0\n",
      "geometry        0\n",
      "dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "  編號  圖層  顏色  街廓編號 分區代碼 分區簡稱  使用分區  分區說明  原屬分區 變更前代碼 變更前簡稱 變更前分區  \\\n",
      "0  1  32  19  None  PEA    公  公園用地  None  None  None  None  None   \n",
      "1  2  32  19  None  PEA    公  公園用地  None  None  None  None  None   \n",
      "\n",
      "         Category      Area                                           geometry  \n",
      "0  City_Open_Area   823.190  MULTIPOLYGON (((303340.099 2771175.776, 303329...  \n",
      "1  City_Open_Area  4721.047  MULTIPOLYGON (((303230.925 2771108.301, 303088...  \n",
      "\n",
      "Dataset: accidents\n",
      "Shape: (56133, 8)\n",
      "Columns and Data Types:\n",
      "Month            object\n",
      "Day               int32\n",
      "Hours             int32\n",
      "Minute            int32\n",
      "Location         object\n",
      "Speed_limit       int32\n",
      "Roadtype          int32\n",
      "geometry       geometry\n",
      "dtype: object\n",
      "Missing values (total): 0\n",
      "Missing values per column:\n",
      "Month          0\n",
      "Day            0\n",
      "Hours          0\n",
      "Minute         0\n",
      "Location       0\n",
      "Speed_limit    0\n",
      "Roadtype       0\n",
      "geometry       0\n",
      "dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "     Month  Day  Hours  Minute              Location  Speed_limit  Roadtype  \\\n",
      "0  January    1      0      46  大安區仁愛路4段與忠孝東路4段216巷口           50         4   \n",
      "1  January    1      0      46  大安區仁愛路4段與忠孝東路4段216巷口           50         4   \n",
      "\n",
      "                        geometry  \n",
      "0  POINT (305807.42 2770096.759)  \n",
      "1  POINT (305807.42 2770096.759)  \n",
      "\n",
      "Dataset: population\n",
      "Shape: (456, 3)\n",
      "Columns and Data Types:\n",
      "LIE_NAME               object\n",
      "Total_Population        int64\n",
      "Elderly_Percentage    float64\n",
      "dtype: object\n",
      "Missing values (total): 0\n",
      "Missing values per column:\n",
      "LIE_NAME              0\n",
      "Total_Population      0\n",
      "Elderly_Percentage    0\n",
      "dtype: int64\n",
      "Unique LIE_NAME: 456\n",
      "Sample data (first 2 rows):\n",
      "  LIE_NAME  Total_Population  Elderly_Percentage\n",
      "0      南福里             12021               16.10\n",
      "1      奇岩里             11200               22.68\n",
      "--- End of Data Structure Summary ---\n",
      "\n",
      "Starting compute_road_type_accident_correlation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 21:48:21,412 - INFO - Assigning accidents to nearest road...\n",
      "2025-04-22 21:48:26,095 - INFO - Matched 27933 accidents out of 56133\n",
      "2025-04-22 21:48:26,099 - INFO - Reassigning 2200 accidents from footway/cycleway...\n",
      "2025-04-22 21:48:26,151 - INFO - Reassigned 612 accidents to wider roads\n",
      "2025-04-22 21:48:26,158 - INFO - Accidents by road type:\n",
      "class\n",
      "bridleway           0\n",
      "cycleway          110\n",
      "footway          1478\n",
      "living_street      36\n",
      "motorway           62\n",
      "path               40\n",
      "pedestrian         36\n",
      "primary          3503\n",
      "residential      4474\n",
      "secondary        9515\n",
      "service          2231\n",
      "steps              22\n",
      "tertiary         3985\n",
      "track               4\n",
      "trunk            1276\n",
      "unclassified      719\n",
      "unknown            23\n",
      "Name: accident_count, dtype: int64\n",
      "2025-04-22 21:48:26,167 - INFO - Road type counts:\n",
      "class\n",
      "service          21204\n",
      "footway          16755\n",
      "residential      14861\n",
      "tertiary          5113\n",
      "secondary         3869\n",
      "path              3610\n",
      "steps             2968\n",
      "unclassified      1894\n",
      "primary           1209\n",
      "cycleway           825\n",
      "track              716\n",
      "trunk              593\n",
      "motorway           313\n",
      "pedestrian         297\n",
      "living_street      264\n",
      "unknown             54\n",
      "bridleway            2\n",
      "Name: count, dtype: int64\n",
      "2025-04-22 21:48:26,178 - INFO - Spearman's correlation between road width rank and accident density: 0.798 (p-value: 0.001)\n",
      "2025-04-22 21:48:26,178 - INFO - Computing average road accident density per neighborhood...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Road type counts:\n",
      "class\n",
      "service          21204\n",
      "footway          16755\n",
      "residential      14861\n",
      "tertiary          5113\n",
      "secondary         3869\n",
      "path              3610\n",
      "steps             2968\n",
      "unclassified      1894\n",
      "primary           1209\n",
      "cycleway           825\n",
      "track              716\n",
      "trunk              593\n",
      "motorway           313\n",
      "pedestrian         297\n",
      "living_street      264\n",
      "unknown             54\n",
      "bridleway            2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Road Type Accident Density Summary ---\n",
      "            class    length_m  accident_count  accident_density  width_rank\n",
      "1        cycleway   263682.07             109              0.12           1\n",
      "2         footway  1767503.72            1443              0.33           1\n",
      "3   living_street    23929.78              35              0.76           3\n",
      "4        motorway   215317.82              62              1.55           5\n",
      "5            path   720602.60              39              0.04           1\n",
      "6      pedestrian    31830.26              35              0.19           1\n",
      "7         primary   212330.19            3415             30.15           4\n",
      "8     residential  1455020.72            4439              2.16           3\n",
      "9       secondary   534770.12            9257             26.77           4\n",
      "10        service  2502054.44            2226              0.30           2\n",
      "11          steps   161013.29              17              0.05           1\n",
      "12       tertiary   755660.86            3952              4.92           3\n",
      "13          track   159053.47               4              0.01           2\n",
      "14          trunk   265713.76            1256              8.54           5\n",
      "Spearman's correlation: 0.798 (p-value: 0.001)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 21:48:26,712 - INFO - Bar chart saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_bar.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bar chart saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_bar.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 21:48:27,135 - INFO - Box chart saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_box.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box chart saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_box.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 21:48:27,386 - INFO - Scatter plot saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_scatter.png\n",
      "2025-04-22 21:48:27,389 - INFO - Top 3 road types by accident density:\n",
      "        class  accident_density\n",
      "7     primary             30.15\n",
      "9   secondary             26.77\n",
      "14      trunk              8.54\n",
      "2025-04-22 21:48:27,390 - INFO - Stage 2: Building city graph...\n",
      "2025-04-22 21:48:27,391 - INFO - Dataset neighborhoods column types:\n",
      "LIE_NAME                            object\n",
      "SECT_NAME                           object\n",
      "2024population                       int32\n",
      "land_use_city_open_area_count        int32\n",
      "land_use_city_open_area_area_m2    float64\n",
      "                                    ...   \n",
      "accident_count                       int64\n",
      "road_density                       float64\n",
      "total_population                     int64\n",
      "elderly_percentage                 float64\n",
      "avg_road_accident_density          float64\n",
      "Length: 68, dtype: object\n",
      "2025-04-22 21:48:27,400 - INFO - Dataset buildings column types:\n",
      "full_id       object\n",
      "osm_id        object\n",
      "building      object\n",
      "屋齡            object\n",
      "建物高度          object\n",
      "地上層數          object\n",
      "構造種類          object\n",
      "使用分區          object\n",
      "geometry    geometry\n",
      "dtype: object\n",
      "2025-04-22 21:48:27,404 - INFO - Dataset roads column types:\n",
      "class         object\n",
      "geometry    geometry\n",
      "length_m     float64\n",
      "dtype: object\n",
      "2025-04-22 21:48:27,407 - INFO - Dataset trees column types:\n",
      "id               object\n",
      "geometry       geometry\n",
      "version           int32\n",
      "sources          object\n",
      "subtype          object\n",
      "class            object\n",
      "surface          object\n",
      "names            object\n",
      "level           float64\n",
      "source_tags      object\n",
      "wikidata         object\n",
      "elevation       float64\n",
      "dtype: object\n",
      "2025-04-22 21:48:27,410 - INFO - Dataset transit column types:\n",
      "id               object\n",
      "geometry       geometry\n",
      "version           int32\n",
      "sources          object\n",
      "subtype          object\n",
      "class            object\n",
      "surface          object\n",
      "names            object\n",
      "level           float64\n",
      "source_tags      object\n",
      "wikidata         object\n",
      "dtype: object\n",
      "2025-04-22 21:48:27,413 - INFO - Dataset urban_masterplan column types:\n",
      "編號            object\n",
      "圖層            object\n",
      "顏色            object\n",
      "街廓編號          object\n",
      "分區代碼          object\n",
      "分區簡稱          object\n",
      "使用分區          object\n",
      "分區說明          object\n",
      "原屬分區          object\n",
      "變更前代碼         object\n",
      "變更前簡稱         object\n",
      "變更前分區         object\n",
      "Category      object\n",
      "Area         float64\n",
      "geometry    geometry\n",
      "dtype: object\n",
      "2025-04-22 21:48:27,417 - INFO - Dataset accidents column types:\n",
      "Month            object\n",
      "Day               int32\n",
      "Hours             int32\n",
      "Minute            int32\n",
      "Location         object\n",
      "Speed_limit       int32\n",
      "Roadtype          int32\n",
      "geometry       geometry\n",
      "dtype: object\n",
      "2025-04-22 21:48:27,420 - INFO - Dataset population column types:\n",
      "LIE_NAME               object\n",
      "Total_Population        int64\n",
      "Elderly_Percentage    float64\n",
      "dtype: object\n",
      "2025-04-22 21:48:27,421 - INFO - Constructing graph nodes...\n",
      "2025-04-22 21:48:27,421 - WARNING - 'area_m2' column missing in buildings_gdf. Computing from geometry...\n",
      "2025-04-22 21:48:27,428 - INFO - Computed area_m2 stats:\n",
      "count    74306.000000\n",
      "mean       326.713464\n",
      "std        783.686503\n",
      "min          0.603543\n",
      "25%         97.315016\n",
      "50%        182.929099\n",
      "75%        304.603966\n",
      "max      62939.914744\n",
      "Name: area_m2, dtype: float64\n",
      "2025-04-22 21:48:27,429 - INFO - Adding neighborhood nodes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scatter plot saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_scatter.png\n",
      "Top 3 road types by accident density:\n",
      "        class  accident_density\n",
      "7     primary             30.15\n",
      "9   secondary             26.77\n",
      "14      trunk              8.54\n",
      "Starting build_graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neighborhood nodes: 100%|██████████| 456/456 [00:00<00:00, 19563.67it/s]\n",
      "2025-04-22 21:48:27,454 - INFO - Adding building nodes...\n",
      "Building nodes: 100%|██████████| 74306/74306 [00:01<00:00, 56245.83it/s]\n",
      "2025-04-22 21:48:28,777 - INFO - Adding road nodes...\n",
      "Road nodes: 100%|██████████| 81444/81444 [00:01<00:00, 52667.21it/s]\n",
      "2025-04-22 21:48:30,717 - INFO - Creating spatial indices...\n",
      "2025-04-22 21:48:33,218 - INFO - Creating edges based on spatial proximity...\n",
      "2025-04-22 21:48:33,219 - INFO - Computing neighborhood-neighborhood edges...\n",
      "Neighborhood-Neighborhood edges: 100%|██████████| 456/456 [00:03<00:00, 120.49it/s]\n",
      "2025-04-22 21:48:37,363 - INFO - Created 2796 neighborhood-neighborhood edges\n",
      "2025-04-22 21:48:37,363 - INFO - Computing neighborhood-building edges...\n",
      "Neighborhood-Building edges: 100%|██████████| 456/456 [05:20<00:00,  1.42it/s]\n",
      "2025-04-22 21:53:58,231 - INFO - Created 77576 total edges after neighborhood-building\n",
      "2025-04-22 21:53:58,231 - INFO - Computing neighborhood-road edges...\n",
      "Neighborhood-Road edges: 100%|██████████| 456/456 [04:19<00:00,  1.76it/s]\n",
      "2025-04-22 21:58:17,845 - INFO - Created 143871 total edges after neighborhood-road\n",
      "2025-04-22 21:58:17,994 - INFO - After validation, 143871 edges remain\n",
      "2025-04-22 21:58:17,999 - INFO - Sample edges after validation:\n",
      "              src             dst\n",
      "0  neighborhood_0  neighborhood_1\n",
      "1  neighborhood_0  neighborhood_5\n",
      "2  neighborhood_0  neighborhood_4\n",
      "3  neighborhood_0  neighborhood_2\n",
      "4  neighborhood_1  neighborhood_8\n",
      "2025-04-22 21:58:18,137 - INFO - Saving graph data to cache...\n",
      "2025-04-22 21:58:18,470 - INFO - Successfully saved graph data to cache.\n",
      "2025-04-22 21:58:18,471 - INFO - City graph constructed: 156206 nodes, 143871 edges\n",
      "2025-04-22 21:58:18,614 - INFO - Graph edge count: 143871\n",
      "2025-04-22 21:58:18,615 - INFO - Computing walkability scores for neighborhoods...\n",
      "2025-04-22 21:58:18,727 - INFO - LIE_NAME: 0 nulls, 0 zeros.\n",
      "2025-04-22 21:58:18,731 - INFO - Geometry column: 0 nulls, 0 invalid geometries.\n",
      "2025-04-22 21:58:18,732 - INFO - ndvi_mean: 0 nulls, 0 zeros.\n",
      "2025-04-22 21:58:18,733 - INFO - tree_count: 0 nulls, 0 zeros.\n",
      "2025-04-22 21:58:18,734 - INFO - transit_count: 0 nulls, 1 zeros.\n",
      "2025-04-22 21:58:18,735 - INFO - intersection_density: 0 nulls, 0 zeros.\n",
      "2025-04-22 21:58:18,736 - INFO - accident_count: 0 nulls, 1 zeros.\n",
      "2025-04-22 21:58:18,737 - INFO - area_km2: 0 nulls, 0 zeros.\n",
      "2025-04-22 21:58:18,737 - INFO - avg_road_accident_density: 456 nulls, 0 zeros.\n",
      "2025-04-22 21:58:18,738 - INFO - elderly_percentage: 0 nulls, 0 zeros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting compute_walkability_scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 21:58:20,308 - INFO - Filtered 20977 roads of types ['footway', 'pedestrian', 'cycleway'] out of 81444 total roads.\n",
      "2025-04-22 21:58:20,309 - INFO - Checking spatial overlap between neighborhoods and pedestrian_roads...\n",
      "2025-04-22 21:58:20,630 - INFO - neighborhoods bounds: [ 296266.05303084 2761514.89561711  317197.26073793 2789176.16901603]\n",
      "2025-04-22 21:58:20,631 - INFO - pedestrian_roads bounds: [ 295216.88848867 2758756.21684514  314125.37860428 2787925.43931427]\n",
      "2025-04-22 21:58:20,631 - INFO - Bounding boxes overlap: True\n",
      "2025-04-22 21:58:20,640 - INFO - Sample intersection check: 337 intersections found out of 10 samples.\n",
      "2025-04-22 21:58:20,749 - INFO - Pedestrian roads join resulted in 24636 matches.\n",
      "2025-04-22 21:58:20,754 - INFO - Pedestrian road density stats:\n",
      "count    456.000000\n",
      "mean      15.841759\n",
      "std       10.276229\n",
      "min        0.000000\n",
      "25%        8.202007\n",
      "50%       15.006599\n",
      "75%       21.558429\n",
      "max       57.267153\n",
      "Name: pedestrian_road_density, dtype: float64\n",
      "Computing walkability scores: 100%|██████████| 456/456 [00:00<00:00, 2807.82it/s]\n",
      "2025-04-22 21:58:20,925 - INFO - Walkability score distribution:\n",
      "count    456.000000\n",
      "mean       0.250785\n",
      "std        0.065527\n",
      "min        0.046299\n",
      "25%        0.209132\n",
      "50%        0.254688\n",
      "75%        0.289512\n",
      "max        0.487566\n",
      "Name: walkability_score, dtype: float64\n",
      "2025-04-22 21:58:20,926 - INFO - Walkability category distribution:\n",
      "walkability_category\n",
      "low       413\n",
      "medium     43\n",
      "Name: count, dtype: int64\n",
      "2025-04-22 21:58:20,984 - INFO - Number of neighborhood nodes in nodes_df: 456\n",
      "2025-04-22 21:58:20,984 - INFO - Number of entries in walkability_components: 456\n",
      "2025-04-22 21:58:20,985 - INFO - Sample LIE_NAME in nodes_df: ['湖田里', '菁山里', '大屯里', '平等里', '泉源里']\n",
      "2025-04-22 21:58:20,986 - INFO - Sample LIE_NAME in walkability_components: ['湖田里', '菁山里', '大屯里', '平等里', '泉源里']\n",
      "2025-04-22 21:58:21,108 - INFO - Finished computing walkability scores.\n",
      "2025-04-22 21:58:21,112 - INFO - Preparing data for GNN training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prepare_gnn_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing features by node type: 100%|██████████| 3/3 [00:00<00:00, 69.27it/s]\n",
      "Preparing edge index: 100%|██████████| 143871/143871 [00:01<00:00, 97204.32it/s]\n",
      "2025-04-22 21:58:22,777 - WARNING - No valid edges after mapping node IDs to indices. GNN will treat nodes independently.\n",
      "2025-04-22 21:58:22,944 - INFO - Prepared GNN data: 156206 nodes, 0 edges\n",
      "2025-04-22 21:58:22,945 - INFO - Feature matrix shape: torch.Size([156206, 14])\n",
      "2025-04-22 21:58:22,954 - INFO - Stage 4: Training GNN model...\n",
      "2025-04-22 21:58:22,961 - INFO - Target (walkability_score) distribution:\n",
      "count    156206.000000\n",
      "mean          0.000732\n",
      "std           0.013985\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           0.487566\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting train_gnn_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   0%|          | 0/100 [00:00<?, ?it/s]2025-04-22 21:58:23,129 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,463 - INFO - Epoch 0, Train Loss: 0.1660, Val Loss: 0.1804\n",
      "Training epochs:   1%|          | 1/100 [00:00<00:33,  2.92it/s]2025-04-22 21:58:23,465 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,470 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,473 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,480 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,486 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,490 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,496 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,502 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,505 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,512 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,516 - INFO - Epoch 10, Train Loss: 0.1376, Val Loss: 0.1511\n",
      "2025-04-22 21:58:23,517 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,520 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,525 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,529 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,532 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,537 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,541 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,546 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,548 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,551 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,554 - INFO - Epoch 20, Train Loss: 0.1125, Val Loss: 0.1250\n",
      "2025-04-22 21:58:23,555 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,558 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,563 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "Training epochs:  24%|██▍       | 24/100 [00:00<00:01, 69.45it/s]2025-04-22 21:58:23,566 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,569 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,572 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,579 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,583 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,586 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,589 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,592 - INFO - Epoch 30, Train Loss: 0.0908, Val Loss: 0.1023\n",
      "2025-04-22 21:58:23,592 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,597 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,600 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,603 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,606 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,613 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,616 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,620 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,623 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,628 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,632 - INFO - Epoch 40, Train Loss: 0.0726, Val Loss: 0.0831\n",
      "2025-04-22 21:58:23,632 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,635 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,638 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,643 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,646 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,649 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,652 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,655 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,664 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "Training epochs:  50%|█████     | 50/100 [00:00<00:00, 125.40it/s]2025-04-22 21:58:23,667 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,670 - INFO - Epoch 50, Train Loss: 0.0576, Val Loss: 0.0671\n",
      "2025-04-22 21:58:23,671 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,677 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,680 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,683 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,686 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,986 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,994 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:23,998 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,001 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,005 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,010 - INFO - Epoch 60, Train Loss: 0.0454, Val Loss: 0.0540\n",
      "2025-04-22 21:58:24,011 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,017 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,021 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,029 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,033 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,037 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,041 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,045 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,049 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "Training epochs:  70%|███████   | 70/100 [00:00<00:00, 79.98it/s] 2025-04-22 21:58:24,052 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,055 - INFO - Epoch 70, Train Loss: 0.0358, Val Loss: 0.0435\n",
      "2025-04-22 21:58:24,056 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,061 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,064 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,068 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,073 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,078 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,081 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,085 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,088 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,093 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,098 - INFO - Epoch 80, Train Loss: 0.0283, Val Loss: 0.0351\n",
      "2025-04-22 21:58:24,099 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,103 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,108 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,112 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,115 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,119 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,122 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,125 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,128 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,131 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,134 - INFO - Epoch 90, Train Loss: 0.0226, Val Loss: 0.0286\n",
      "2025-04-22 21:58:24,134 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,137 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,140 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,144 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,147 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,149 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "Training epochs:  97%|█████████▋| 97/100 [00:01<00:00, 115.33it/s]2025-04-22 21:58:24,153 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,156 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:24,161 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "Training epochs: 100%|██████████| 100/100 [00:01<00:00, 95.87it/s]\n",
      "2025-04-22 21:58:24,165 - INFO - Finished training GNN model.\n",
      "2025-04-22 21:58:24,166 - INFO - Predicting walkability with GNN...\n",
      "2025-04-22 21:58:24,167 - INFO - Preparing data for GNN training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting predict_walkability...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing features by node type: 100%|██████████| 3/3 [00:00<00:00, 73.09it/s]\n",
      "Preparing edge index: 100%|██████████| 143871/143871 [00:01<00:00, 90284.28it/s]\n",
      "2025-04-22 21:58:25,953 - WARNING - No valid edges after mapping node IDs to indices. GNN will treat nodes independently.\n",
      "2025-04-22 21:58:26,111 - INFO - Prepared GNN data: 156206 nodes, 0 edges\n",
      "2025-04-22 21:58:26,112 - INFO - Feature matrix shape: torch.Size([156206, 14])\n",
      "2025-04-22 21:58:26,123 - WARNING - No edges in the graph. Using linear layer for node features only.\n",
      "2025-04-22 21:58:26,222 - INFO - GNN prediction (walkability_gnn) distribution:\n",
      "count    156206.000000\n",
      "mean          0.503724\n",
      "std           0.002088\n",
      "min           0.499809\n",
      "25%           0.503542\n",
      "50%           0.503557\n",
      "75%           0.503633\n",
      "max           0.620021\n",
      "dtype: float64\n",
      "2025-04-22 21:58:26,338 - INFO - Finished predicting walkability with GNN.\n",
      "2025-04-22 21:58:26,343 - INFO - Generating interactive Kepler.gl map...\n",
      "2025-04-22 21:58:26,468 - INFO - Neighborhood nodes count: 456\n",
      "2025-04-22 21:58:26,469 - INFO - Neighborhoods_gdf count: 456\n",
      "2025-04-22 21:58:26,469 - INFO - Sample LIE_NAME in nodes_df: ['南福里', '天母里', '松光里', '社子里', '新和里']\n",
      "2025-04-22 21:58:26,469 - INFO - Sample LIE_NAME in neighborhoods_gdf: ['南福里', '天母里', '松光里', '社子里', '新和里']\n",
      "2025-04-22 21:58:26,470 - INFO - Common LIE_NAMEs: 456\n",
      "2025-04-22 21:58:26,470 - INFO - Nodes LIE_NAMEs not in GDF: []\n",
      "2025-04-22 21:58:26,471 - INFO - GDF LIE_NAMEs not in nodes: []\n",
      "2025-04-22 21:58:26,472 - INFO - Nodes nulls: {'LIE_NAME': 0, 'walkability_score': 0, 'walkability_gnn': 0, 'walkability_category': 0}\n",
      "2025-04-22 21:58:26,472 - INFO - GDF geometry nulls: 0\n",
      "2025-04-22 21:58:26,478 - INFO - Merged map_data rows: 456\n",
      "2025-04-22 21:58:26,479 - INFO - Walkability score nulls: 0\n",
      "2025-04-22 21:58:26,480 - INFO - Walkability GNN nulls: 0\n",
      "2025-04-22 21:58:26,482 - INFO - Walkability score distribution in map_data:\n",
      "count    456.000000\n",
      "mean       0.250785\n",
      "std        0.065527\n",
      "min        0.046299\n",
      "25%        0.209132\n",
      "50%        0.254688\n",
      "75%        0.289512\n",
      "max        0.487566\n",
      "Name: walkability_score, dtype: float64\n",
      "2025-04-22 21:58:26,483 - INFO - Walkability GNN distribution in map_data:\n",
      "count    456.000000\n",
      "mean       0.537118\n",
      "std        0.018447\n",
      "min        0.499809\n",
      "25%        0.524181\n",
      "50%        0.533877\n",
      "75%        0.548079\n",
      "max        0.620021\n",
      "Name: walkability_gnn, dtype: float64\n",
      "2025-04-22 21:58:26,484 - INFO - Walkability category distribution in map_data:\n",
      "walkability_category\n",
      "low       413\n",
      "medium     43\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting create_interactive_map...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 21:58:26,607 - INFO - Interactive map generated and saved as /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/taipei_walkability_map.html\n",
      "2025-04-22 21:58:26,697 - INFO - Final validation - Walkability scores in neighborhood nodes:\n",
      "2025-04-22 21:58:26,698 - INFO - Walkability score distribution:\n",
      "count    456.000000\n",
      "mean       0.250785\n",
      "std        0.065527\n",
      "min        0.046299\n",
      "25%        0.209132\n",
      "50%        0.254688\n",
      "75%        0.289512\n",
      "max        0.487566\n",
      "Name: walkability_score, dtype: float64\n",
      "2025-04-22 21:58:26,698 - INFO - Walkability GNN distribution:\n",
      "count    456.000000\n",
      "mean       0.537118\n",
      "std        0.018447\n",
      "min        0.499809\n",
      "25%        0.524181\n",
      "50%        0.533877\n",
      "75%        0.548079\n",
      "max        0.620021\n",
      "Name: walkability_gnn, dtype: float64\n",
      "2025-04-22 21:58:26,699 - INFO - Walkability category distribution:\n",
      "walkability_category\n",
      "low       413\n",
      "medium     43\n",
      "Name: count, dtype: int64\n",
      "2025-04-22 21:58:26,699 - INFO - Number of neighborhood nodes with non-zero walkability_score: 456/456\n",
      "2025-04-22 21:58:26,700 - INFO - Number of neighborhood nodes with non-zero walkability_gnn: 456/456\n",
      "2025-04-22 21:58:26,700 - WARNING - GNN predictions have low variation (std < 0.05). Check edge creation and model training.\n",
      "2025-04-22 21:58:26,706 - INFO - Correlation between walkability_score and walkability_gnn: 0.09 (p-value: 0.06)\n",
      "2025-04-22 21:58:26,707 - WARNING - Low correlation between walkability_score and walkability_gnn. GNN predictions may not align well with rule-based scores.\n",
      "2025-04-22 21:58:26,708 - INFO - Processing complete. Timing summary:\n",
      "2025-04-22 21:58:26,709 - INFO - load_and_prepare_data: 34.09 seconds\n",
      "2025-04-22 21:58:26,710 - INFO - compute_road_type_accident_correlation: 6.44 seconds\n",
      "2025-04-22 21:58:26,710 - INFO - build_graph: 591.22 seconds\n",
      "2025-04-22 21:58:26,711 - INFO - compute_walkability_scores: 2.50 seconds\n",
      "2025-04-22 21:58:26,712 - INFO - prepare_gnn_data: 1.84 seconds\n",
      "2025-04-22 21:58:26,712 - INFO - train_gnn_model: 1.21 seconds\n",
      "2025-04-22 21:58:26,713 - INFO - predict_walkability: 2.18 seconds\n",
      "2025-04-22 21:58:26,714 - INFO - create_interactive_map: 0.27 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n",
      "Map saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/taipei_walkability_map.html!\n",
      "Map saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/taipei_walkability_map.html!\n",
      "Pipeline completed successfully.\n",
      "     src     dst\n",
      "0  41293    7328\n",
      "1  41293   54168\n",
      "2  41293  108167\n",
      "3  41293  120864\n",
      "4  41293   49042\n"
     ]
    }
   ],
   "source": [
    "def main(force_recompute_graph=False):\n",
    "    \"\"\"Main execution pipeline for the analysis.\"\"\"\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    os.makedirs(SUBGRAPH_DIR, exist_ok=True)\n",
    "    logging.info(f\"Ensured subgraph directory exists: {SUBGRAPH_DIR}\")\n",
    "\n",
    "    # Track timing for each step\n",
    "    timings = {}\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load and prepare data\n",
    "        start_time = time.time()\n",
    "        print(\"Starting load_and_prepare_data...\")\n",
    "        data = load_and_prepare_data()\n",
    "        timings['load_and_prepare_data'] = time.time() - start_time\n",
    "\n",
    "        # Step 2: Compute road type accident correlation\n",
    "        start_time = time.time()\n",
    "        print(\"Starting compute_road_type_accident_correlation...\")\n",
    "        road_accident_summary = compute_road_type_accident_correlation(\n",
    "            data['roads'], data['neighborhoods'], data['accidents']\n",
    "        )\n",
    "        timings['compute_road_type_accident_correlation'] = time.time() - start_time\n",
    "\n",
    "        # Step 3: Build graph\n",
    "        start_time = time.time()\n",
    "        print(\"Starting build_graph...\")\n",
    "        G = build_graph(data, force_recompute=force_recompute_graph)\n",
    "        timings['build_graph'] = time.time() - start_time\n",
    "\n",
    "        # Validate edge counts\n",
    "        edge_count = G.edgelist.edgelist_df.shape[0] if G.edgelist else 0\n",
    "        logging.info(f\"Graph edge count: {edge_count}\")\n",
    "        if edge_count == 0:\n",
    "            logging.warning(\"Graph has no edges. GNN will not utilize graph structure.\")\n",
    "\n",
    "        # Step 4: Compute walkability scores\n",
    "        start_time = time.time()\n",
    "        print(\"Starting compute_walkability_scores...\")\n",
    "        G = compute_walkability_scores(G, data)\n",
    "        timings['compute_walkability_scores'] = time.time() - start_time\n",
    "\n",
    "        # Step 5: Prepare GNN data\n",
    "        start_time = time.time()\n",
    "        print(\"Starting prepare_gnn_data...\")\n",
    "        data_gnn = prepare_gnn_data(G)\n",
    "        timings['prepare_gnn_data'] = time.time() - start_time\n",
    "\n",
    "        # Step 6: Train GNN model\n",
    "        start_time = time.time()\n",
    "        print(\"Starting train_gnn_model...\")\n",
    "        model = train_gnn_model(data_gnn)\n",
    "        timings['train_gnn_model'] = time.time() - start_time\n",
    "\n",
    "        # Step 7: Predict walkability\n",
    "        start_time = time.time()\n",
    "        print(\"Starting predict_walkability...\")\n",
    "        G = predict_walkability(G, model)\n",
    "        timings['predict_walkability'] = time.time() - start_time\n",
    "\n",
    "        # Step 8: Create interactive map\n",
    "        start_time = time.time()\n",
    "        print(\"Starting create_interactive_map...\")\n",
    "        create_interactive_map(G, data)\n",
    "        timings['create_interactive_map'] = time.time() - start_time\n",
    "\n",
    "        # Final validation: Check walkability scores\n",
    "        nodes_df = G._nodes.to_pandas()\n",
    "        neighborhood_nodes = nodes_df[nodes_df['type'] == 'neighborhood']\n",
    "        walkability_score_stats = neighborhood_nodes['walkability_score'].describe()\n",
    "        walkability_gnn_stats = neighborhood_nodes['walkability_gnn'].describe()\n",
    "        walkability_category_dist = neighborhood_nodes['walkability_category'].value_counts()\n",
    "        non_zero_walkability = (neighborhood_nodes['walkability_score'] > 0).sum()\n",
    "        non_zero_walkability_gnn = (neighborhood_nodes['walkability_gnn'] > 0).sum()\n",
    "        \n",
    "        logging.info(\"Final validation - Walkability scores in neighborhood nodes:\")\n",
    "        logging.info(f\"Walkability score distribution:\\n{walkability_score_stats}\")\n",
    "        logging.info(f\"Walkability GNN distribution:\\n{walkability_gnn_stats}\")\n",
    "        logging.info(f\"Walkability category distribution:\\n{walkability_category_dist}\")\n",
    "        logging.info(f\"Number of neighborhood nodes with non-zero walkability_score: {non_zero_walkability}/{len(neighborhood_nodes)}\")\n",
    "        logging.info(f\"Number of neighborhood nodes with non-zero walkability_gnn: {non_zero_walkability_gnn}/{len(neighborhood_nodes)}\")\n",
    "\n",
    "        # Check for low variation in walkability scores\n",
    "        if walkability_score_stats['std'] < 0.05:\n",
    "            logging.warning(\"Walkability scores have low variation (std < 0.05). Components may need adjustment.\")\n",
    "        if walkability_gnn_stats['std'] < 0.05:\n",
    "            logging.warning(\"GNN predictions have low variation (std < 0.05). Check edge creation and model training.\")\n",
    "\n",
    "        # Compute correlation between walkability_score and walkability_gnn\n",
    "        corr, p_value = pearsonr(neighborhood_nodes['walkability_score'], neighborhood_nodes['walkability_gnn'])\n",
    "        logging.info(f\"Correlation between walkability_score and walkability_gnn: {corr:.2f} (p-value: {p_value:.2f})\")\n",
    "        if corr < 0.5:\n",
    "            logging.warning(\"Low correlation between walkability_score and walkability_gnn. GNN predictions may not align well with rule-based scores.\")\n",
    "\n",
    "        # Log timing summary\n",
    "        logging.info(\"Processing complete. Timing summary:\")\n",
    "        for step, duration in timings.items():\n",
    "            logging.info(f\"{step}: {duration:.2f} seconds\")\n",
    "        \n",
    "        print(\"Pipeline completed successfully.\")\n",
    "        print(G.edgelist.edgelist_df.to_pandas().head())\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Pipeline failed with error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(force_recompute_graph=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
