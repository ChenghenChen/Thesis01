{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a986f754",
   "metadata": {},
   "source": [
    "Cell 0: CUDA Version check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dc2c30cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.02.02 25.02.00\n"
     ]
    }
   ],
   "source": [
    "import cudf, cugraph\n",
    "print(cudf.__version__, cugraph.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac3ad7",
   "metadata": {},
   "source": [
    "Cell 1: Imports ,Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e388b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import logging\n",
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Third-party imports\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import cugraph\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv, BatchNorm\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from shapely import make_valid\n",
    "from shapely.errors import GEOSException\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from keplergl import KeplerGl\n",
    "\n",
    "# Plotting configuration\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Noto Sans CJK TC', 'Noto Serif CJK TC', 'Noto Sans Mono CJK TC', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Directory and file paths\n",
    "BASE_DIR = \"/home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data\"\n",
    "LANDUSE_NDVI_PATH = os.path.join(BASE_DIR, \"neighborhoods_with_ndvi_numerical_corrected.geojson\")\n",
    "OSM_BUILDINGS_PATH = os.path.join(BASE_DIR, \"Taipei_Buildings_fulldata.geojson\")\n",
    "OSM_ROADS_PATH = os.path.join(BASE_DIR, \"taipei_segments_cleaned_verified.geoparquet\")\n",
    "OSM_TREES_PATH = os.path.join(BASE_DIR, \"taipei_land.geoparquet\")\n",
    "OSM_TRANSIT_PATH = os.path.join(BASE_DIR, \"taipei_infrastructure.geoparquet\")\n",
    "URBAN_MASTERPLAN_PATH = os.path.join(BASE_DIR, \"Taipei_urban_masterplan.geojson\")\n",
    "ACCIDENTS_PATH = os.path.join(BASE_DIR, \"2023_accidents.geojson\")\n",
    "POPULATION_PATH = os.path.join(BASE_DIR, \"population_corrected.json\")\n",
    "SUBGRAPH_DIR = os.path.join(BASE_DIR, \"subgraphs\")\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"checkpoints\")\n",
    "INTERSECTION_CACHE_PATH = os.path.join(BASE_DIR, \"neighborhoods_with_intersections.geoparquet\")\n",
    "GRAPH_NODES_CACHE_PATH = os.path.join(BASE_DIR, \"graph_nodes.parquet\")\n",
    "GRAPH_EDGES_CACHE_PATH = os.path.join(BASE_DIR, \"graph_edges.parquet\")\n",
    "GRAPH_NODE_ID_CACHE_PATH = os.path.join(BASE_DIR, \"graph_node_id_to_index.json\")\n",
    "GRAPH_DATA_HASH_PATH = os.path.join(BASE_DIR, \"graph_data_hash.txt\")\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(SUBGRAPH_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Constants for spatial analysis\n",
    "BUFFER_DISTANCE = 10  # Meters, buffer distance for border sharing of accidents (tunable based on spatial resolution)\n",
    "MIN_ROAD_LENGTH = 10  # Meters, minimum road length to avoid inflated accident density (tunable based on dataset)\n",
    "\n",
    "# Land use category priorities for area assignment\n",
    "CATEGORY_PRIORITY = {\n",
    "    'City_Open_Area': 10,\n",
    "    'Pedestrian': 9,\n",
    "    'Public_Transportation': 8,\n",
    "    'Amenity': 7,\n",
    "    'Education': 6,\n",
    "    'Medical': 5,\n",
    "    'Commercial': 4,\n",
    "    'Residential': 3,\n",
    "    'Natural': 2,\n",
    "    'Road': 1,\n",
    "    'River': 1,\n",
    "    'Infrastructure': 1,\n",
    "    'Government': 1,\n",
    "    'Special_Zone': 1,\n",
    "    'Military': 1,\n",
    "    'Industrial': 1,\n",
    "    'Agriculture': 1\n",
    "}\n",
    "\n",
    "# Weights for land use diversity in walkability scoring\n",
    "land_use_weights = {\n",
    "    'city_open_area': 0.8,\n",
    "    'commercial': 0.7,\n",
    "    'infrastructure': 0.4,\n",
    "    'government': 0.5,\n",
    "    'public_transportation': 0.8,\n",
    "    'education': 0.7,\n",
    "    'medical': 0.6,\n",
    "    'amenity': 0.8,\n",
    "    'road': 0.3,\n",
    "    'pedestrian': 1.0,\n",
    "    'natural': 0.7,\n",
    "    'special_zone': 0.4,\n",
    "    'river': 0.7,\n",
    "    'military': 0.2,\n",
    "    'residential': 0.6,\n",
    "    'industrial': 0.3,\n",
    "    'agriculture': 0.4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4722f69a",
   "metadata": {},
   "source": [
    "Cell 2: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "713654c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_walkability_components_all(neighborhoods_gdf, data):\n",
    "    \"\"\"\n",
    "    Compute walkability components and aggregate them into a walkability score for each neighborhood.\n",
    "    \n",
    "    Args:\n",
    "        neighborhoods_gdf (gpd.GeoDataFrame): GeoDataFrame containing neighborhood data.\n",
    "        data (dict): Dictionary containing other datasets (not used in this function).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'LIE_NAME', 'walkability_score', and 'walkability_category'.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import logging\n",
    "    \n",
    "    # Compute land_use_score\n",
    "    land_use_categories = list(land_use_weights.keys())\n",
    "    \n",
    "    def compute_land_use_score(row):\n",
    "        score = 0.0\n",
    "        for cat in land_use_categories:\n",
    "            col = f'land_use_{cat}_percent'\n",
    "            if col in row:\n",
    "                p = row[col] / 100.0\n",
    "                score += land_use_weights[cat] * p\n",
    "        return score\n",
    "    \n",
    "    neighborhoods_gdf['land_use_score'] = neighborhoods_gdf.apply(compute_land_use_score, axis=1)\n",
    "    \n",
    "    # Compute population_density\n",
    "    neighborhoods_gdf['population_density'] = neighborhoods_gdf['total_population'] / neighborhoods_gdf['area_km2'].replace(0, 1e-6)\n",
    "    \n",
    "    # Compute transit_density\n",
    "    neighborhoods_gdf['transit_density'] = neighborhoods_gdf['transit_count'] / neighborhoods_gdf['area_km2'].replace(0, 1e-6)\n",
    "    \n",
    "    # Use ndvi for green_space, fallback to tree_density if ndvi is missing\n",
    "    if 'ndvi' in neighborhoods_gdf.columns:\n",
    "        neighborhoods_gdf['green_space'] = neighborhoods_gdf['ndvi']\n",
    "    else:\n",
    "        logging.warning(\"'ndvi' column missing. Using tree_density instead.\")\n",
    "        neighborhoods_gdf['tree_density'] = neighborhoods_gdf['tree_count'] / neighborhoods_gdf['area_km2'].replace(0, 1e-6)\n",
    "        neighborhoods_gdf['green_space'] = neighborhoods_gdf['tree_density']\n",
    "    \n",
    "    # Compute accident_density\n",
    "    neighborhoods_gdf['accident_density'] = neighborhoods_gdf['accident_count'] / neighborhoods_gdf['area_km2'].replace(0, 1e-6)\n",
    "    \n",
    "    # Define components and their directions\n",
    "    components = {\n",
    "        'land_use_score': 'higher_better',\n",
    "        'intersection_density': 'higher_better',\n",
    "        'population_density': 'higher_better',\n",
    "        'transit_density': 'higher_better',\n",
    "        'green_space': 'higher_better',\n",
    "        'accident_density': 'lower_better'\n",
    "    }\n",
    "    \n",
    "    # Compute min and max for normalization\n",
    "    mins = {}\n",
    "    maxs = {}\n",
    "    for comp in components:\n",
    "        if comp == 'land_use_score':\n",
    "            mins[comp] = 0.0\n",
    "            maxs[comp] = 1.0\n",
    "        else:\n",
    "            mins[comp] = neighborhoods_gdf[comp].min()\n",
    "            maxs[comp] = neighborhoods_gdf[comp].max()\n",
    "            if mins[comp] == maxs[comp]:\n",
    "                logging.warning(f\"Component {comp} has no variation. Setting normalized value to 0.5\")\n",
    "                neighborhoods_gdf[f'{comp}_norm'] = 0.5\n",
    "            else:\n",
    "                if components[comp] == 'higher_better':\n",
    "                    neighborhoods_gdf[f'{comp}_norm'] = (neighborhoods_gdf[comp] - mins[comp]) / (maxs[comp] - mins[comp])\n",
    "                else:\n",
    "                    neighborhoods_gdf[f'{comp}_norm'] = (maxs[comp] - neighborhoods_gdf[comp]) / (maxs[comp] - mins[comp])\n",
    "    \n",
    "    # For land_use_score, since it's already [0,1]\n",
    "    neighborhoods_gdf['land_use_score_norm'] = neighborhoods_gdf['land_use_score']\n",
    "    \n",
    "    # Define equal weights for simplicity\n",
    "    num_components = len(components)\n",
    "    weight = 1.0 / num_components\n",
    "    \n",
    "    # Compute walkability_score as weighted sum\n",
    "    neighborhoods_gdf['walkability_score'] = 0.0\n",
    "    for comp in components:\n",
    "        norm_col = f'{comp}_norm'\n",
    "        if norm_col in neighborhoods_gdf.columns:\n",
    "            neighborhoods_gdf['walkability_score'] += weight * neighborhoods_gdf[norm_col]\n",
    "        else:\n",
    "            logging.error(f\"Normalized column {norm_col} not found.\")\n",
    "    \n",
    "    # Ensure it's between 0 and 1\n",
    "    neighborhoods_gdf['walkability_score'] = neighborhoods_gdf['walkability_score'].clip(0, 1)\n",
    "    \n",
    "    # Compute walkability_category\n",
    "    def categorize_score(score):\n",
    "        if score < 0.33:\n",
    "            return 'low'\n",
    "        elif score < 0.66:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'high'\n",
    "    \n",
    "    neighborhoods_gdf['walkability_category'] = neighborhoods_gdf['walkability_score'].apply(categorize_score)\n",
    "    \n",
    "    # Return relevant columns\n",
    "    return neighborhoods_gdf[['LIE_NAME', 'walkability_score', 'walkability_category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ae081",
   "metadata": {},
   "source": [
    "Cell 3: Walkability Computation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3b05a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "from shapely import make_valid\n",
    "from shapely.geometry import Point\n",
    "\n",
    "def fix_geometry(geom, buffer_size=1e-5):\n",
    "    \"\"\"Fix invalid geometries with logging for debugging.\"\"\"\n",
    "    if geom is None or geom.is_empty:\n",
    "        logging.debug(\"Geometry is None or empty, returning a default Point(0,0).\")\n",
    "        return Point(0, 0)\n",
    "    try:\n",
    "        geom = make_valid(geom)\n",
    "        if not geom.is_valid:\n",
    "            logging.debug(f\"Geometry invalid after make_valid, applying buffer(0): {geom.bounds}\")\n",
    "            geom = geom.buffer(0)\n",
    "            if not geom.is_valid:\n",
    "                logging.debug(f\"Geometry still invalid, applying buffer with size {buffer_size}: {geom.bounds}\")\n",
    "                geom = geom.buffer(buffer_size)\n",
    "                geom = make_valid(geom)\n",
    "        if not geom.is_valid:\n",
    "            logging.warning(f\"Geometry remains invalid after all attempts: {geom.bounds}. Returning default Point(0,0).\")\n",
    "            return Point(0, 0)\n",
    "        return geom\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fixing geometry: {e}. Returning default Point(0,0).\")\n",
    "        return Point(0, 0)\n",
    "\n",
    "def print_data_structure(data_dict):\n",
    "    \"\"\"Print a detailed summary of the data structure for each dataset.\"\"\"\n",
    "    print(\"\\n--- Data Structure Summary ---\")\n",
    "    for key, df in data_dict.items():\n",
    "        if isinstance(df, (gpd.GeoDataFrame, pd.DataFrame, cudf.DataFrame)):\n",
    "            df = df.to_pandas() if isinstance(df, cudf.DataFrame) else df\n",
    "            print(f\"\\nDataset: {key}\")\n",
    "            print(f\"Shape: {df.shape}\")\n",
    "            print(f\"Columns and Data Types:\\n{df.dtypes}\")\n",
    "            print(f\"Missing values (total): {df.isnull().sum().sum()}\")\n",
    "            print(f\"Missing values per column:\\n{df.isnull().sum()}\")\n",
    "            if 'LIE_NAME' in df.columns:\n",
    "                print(f\"Unique LIE_NAME: {df['LIE_NAME'].nunique()}\")\n",
    "            if 'class' in df.columns and key == 'roads':\n",
    "                print(f\"Road class counts:\\n{df['class'].value_counts()}\")\n",
    "            print(f\"Sample data (first 2 rows):\\n{df.head(2)}\")\n",
    "    print(\"--- End of Data Structure Summary ---\\n\")\n",
    "\n",
    "def print_percentage_calculation(neighborhoods_gdf, urban_masterplan_gdf, sample_size=3):\n",
    "    \"\"\"Print the land use percentage calculation process for a sample of neighborhoods.\"\"\"\n",
    "    print(\"\\n--- Percentage Calculation Process ---\")\n",
    "    sample_neighborhoods = neighborhoods_gdf.sample(min(sample_size, len(neighborhoods_gdf)), random_state=42)\n",
    "    \n",
    "    for idx, row in sample_neighborhoods.iterrows():\n",
    "        lie_name = row['LIE_NAME']\n",
    "        print(f\"\\nNeighborhood: {lie_name} (Index: {idx})\")\n",
    "        \n",
    "        neighborhood_geom = fix_geometry(row['geometry'])\n",
    "        if not neighborhood_geom.is_valid:\n",
    "            print(f\"Neighborhood geometry is invalid after fixing: {lie_name}\")\n",
    "            continue\n",
    "        \n",
    "        relevant_masterplan = urban_masterplan_gdf[urban_masterplan_gdf.intersects(neighborhood_geom)]\n",
    "        if relevant_masterplan.empty:\n",
    "            print(\"No master plan polygons intersect with this neighborhood.\")\n",
    "            continue\n",
    "        \n",
    "        temp_gdf = gpd.GeoDataFrame({'geometry': [neighborhood_geom]}, crs='EPSG:3826')\n",
    "        intersected = gpd.overlay(temp_gdf, relevant_masterplan, how='intersection', keep_geom_type=False)\n",
    "        if intersected.empty:\n",
    "            print(\"No valid intersections after overlay.\")\n",
    "            continue\n",
    "        \n",
    "        intersected['geometry'] = intersected['geometry'].apply(fix_geometry)\n",
    "        intersected = intersected[intersected.geometry.is_valid & ~intersected.geometry.is_empty]\n",
    "        if intersected.empty:\n",
    "            print(\"No valid geometries after fixing intersected polygons.\")\n",
    "            continue\n",
    "        \n",
    "        intersected['priority'] = intersected['Category'].map(CATEGORY_PRIORITY)\n",
    "        intersected = intersected.sort_values(by='priority', ascending=False)\n",
    "        \n",
    "        total_area_geom = intersected.geometry.union_all()\n",
    "        total_area = total_area_geom.area\n",
    "        print(f\"Total unique master plan area: {total_area:.2f} m²\")\n",
    "        \n",
    "        remaining_geom = total_area_geom\n",
    "        category_areas = {}\n",
    "        for category in intersected['Category'].unique():\n",
    "            category_rows = intersected[intersected['Category'] == category]\n",
    "            category_geom = category_rows.geometry.union_all()\n",
    "            try:\n",
    "                category_area_geom = category_geom.intersection(remaining_geom)\n",
    "                category_area = category_area_geom.area\n",
    "                category_areas[category] = category_area\n",
    "                print(f\"Area of {category} (priority {CATEGORY_PRIORITY.get(category, 0)}): {category_area:.2f} m²\")\n",
    "                remaining_geom = remaining_geom.difference(category_area_geom)\n",
    "            except GEOSException as e:\n",
    "                print(f\"Topology error for category {category}: {e}\")\n",
    "                category_areas[category] = 0.0\n",
    "        \n",
    "        print(\"\\nPercentages:\")\n",
    "        total_percentage = 0.0\n",
    "        for category, area in category_areas.items():\n",
    "            percentage = (area / total_area * 100) if total_area > 0 else 0.0\n",
    "            total_percentage += percentage\n",
    "            print(f\"{category}: {percentage:.2f}%\")\n",
    "        print(f\"Sum of percentages: {total_percentage:.2f}%\")\n",
    "    print(\"--- End of Percentage Calculation Process ---\\n\")\n",
    "\n",
    "def compute_data_hash(data_dict):\n",
    "    \"\"\"Compute a hash of the data for caching purposes.\"\"\"\n",
    "    hasher = hashlib.sha256()\n",
    "    for key, df in data_dict.items():\n",
    "        if isinstance(df, (gpd.GeoDataFrame, pd.DataFrame, cudf.DataFrame)):\n",
    "            df = df.to_pandas() if isinstance(df, cudf.DataFrame) else df\n",
    "            hasher.update(str(df.shape).encode('utf-8'))\n",
    "            hasher.update(str(sorted(df.columns)).encode('utf-8'))\n",
    "            \n",
    "            logging.info(f\"Dataset {key} column types:\\n{df.dtypes}\")\n",
    "            \n",
    "            sample_df = df.head(5).copy()\n",
    "            if 'geometry' in sample_df.columns:\n",
    "                sample_df = sample_df.drop(columns=['geometry'])\n",
    "            for col in sample_df.columns:\n",
    "                sample_df[col] = sample_df[col].apply(\n",
    "                    lambda x: x.tolist() if isinstance(x, np.ndarray) else\n",
    "                              float(x) if isinstance(x, (np.floating, np.integer)) else x\n",
    "                )\n",
    "            try:\n",
    "                sample = sample_df.to_json()\n",
    "                hasher.update(sample.encode('utf-8'))\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to serialize sample for dataset {key}: {e}\")\n",
    "                sample = str(sample_df.to_dict())\n",
    "                hasher.update(sample.encode('utf-8'))\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def check_spatial_overlap(gdf1, gdf2, label1=\"gdf1\", label2=\"gdf2\"):\n",
    "    \"\"\"Check for spatial overlap between two GeoDataFrames and log the results.\"\"\"\n",
    "    logging.info(f\"Checking spatial overlap between {label1} and {label2}...\")\n",
    "    gdf1 = gdf1.copy()\n",
    "    gdf2 = gdf2.copy()\n",
    "    \n",
    "    if gdf1.crs != gdf2.crs:\n",
    "        logging.warning(f\"CRS mismatch between {label1} ({gdf1.crs}) and {label2} ({gdf2.crs}). Aligning to {gdf1.crs}...\")\n",
    "        gdf2 = gdf2.to_crs(gdf1.crs)\n",
    "    \n",
    "    gdf1['geometry'] = gdf1['geometry'].apply(fix_geometry)\n",
    "    gdf2['geometry'] = gdf2['geometry'].apply(fix_geometry)\n",
    "    \n",
    "    gdf1_bounds = gdf1.total_bounds\n",
    "    gdf2_bounds = gdf2.total_bounds\n",
    "    logging.info(f\"{label1} bounds: {gdf1_bounds}\")\n",
    "    logging.info(f\"{label2} bounds: {gdf2_bounds}\")\n",
    "    \n",
    "    bounds_overlap = not (gdf1_bounds[2] < gdf2_bounds[0] or\n",
    "                         gdf1_bounds[0] > gdf2_bounds[2] or\n",
    "                         gdf1_bounds[3] < gdf2_bounds[1] or\n",
    "                         gdf1_bounds[1] > gdf2_bounds[3])\n",
    "    logging.info(f\"Bounding boxes overlap: {bounds_overlap}\")\n",
    "    \n",
    "    sample_size = min(10, len(gdf1), len(gdf2))\n",
    "    if sample_size > 0:\n",
    "        sample_gdf1 = gdf1.sample(sample_size, random_state=42)\n",
    "        intersects = gpd.sjoin(sample_gdf1, gdf2, how='inner', predicate='intersects')\n",
    "        logging.info(f\"Sample intersection check: {len(intersects)} intersections found out of {sample_size} samples.\")\n",
    "    \n",
    "    return bounds_overlap\n",
    "\n",
    "def validate_data(gdf, required_cols, name=\"GeoDataFrame\"):\n",
    "    \"\"\"Validate that the GeoDataFrame has all required columns, no missing geometries, and valid geometries.\"\"\"\n",
    "    if gdf.empty:\n",
    "        logging.error(f\"{name} is empty.\")\n",
    "        raise ValueError(f\"{name} is empty.\")\n",
    "    missing_cols = [col for col in required_cols if col not in gdf.columns]\n",
    "    if missing_cols:\n",
    "        logging.error(f\"Missing columns in {name}: {missing_cols}\")\n",
    "        raise KeyError(f\"Missing columns in {name}: {missing_cols}\")\n",
    "    if gdf.geometry.isna().any():\n",
    "        logging.error(f\"Missing geometries in {name}\")\n",
    "        raise ValueError(f\"Missing geometries in {name}\")\n",
    "    if not all(gdf.geometry.is_valid):\n",
    "        logging.error(f\"Invalid geometries in {name}\")\n",
    "        raise ValueError(f\"Invalid geometries in {name}\")\n",
    "\n",
    "def compute_road_type_accident_correlation(roads_gdf, neighborhoods_gdf, accidents_gdf):\n",
    "    \"\"\"\n",
    "    Compute correlation between OSM road types and accident density (accidents per km of road length).\n",
    "    Uses road class as a proxy for width, with ordinal ranking based on OSM hierarchy.\n",
    "    Generates bar, box, and scatter plots for visualization.\n",
    "    \"\"\"\n",
    "    logging.info(\"Computing correlation between road types and accident density...\")\n",
    "    \n",
    "    # Validate input data\n",
    "    validate_data(roads_gdf, ['class', 'geometry', 'length_m'], \"roads_gdf\")\n",
    "    validate_data(neighborhoods_gdf, ['LIE_NAME', 'geometry'], \"neighborhoods_gdf\")\n",
    "    validate_data(accidents_gdf, ['geometry'], \"accidents_gdf\")\n",
    "    \n",
    "    # Ensure correct CRS\n",
    "    target_crs = 'EPSG:3826'\n",
    "    source_crs = 'EPSG:4326'  # Assume data is in WGS84 if CRS is undefined\n",
    "    for gdf, name in [(roads_gdf, \"roads\"), (neighborhoods_gdf, \"neighborhoods\"), (accidents_gdf, \"accidents\")]:\n",
    "        if gdf.crs is None:\n",
    "            logging.warning(f\"{name} has no CRS defined. Assuming {source_crs}.\")\n",
    "            gdf.set_crs(source_crs, inplace=True)\n",
    "        if gdf.crs != target_crs:\n",
    "            logging.info(f\"Reprojecting {name} from {gdf.crs} to {target_crs}\")\n",
    "            gdf.to_crs(target_crs, inplace=True)\n",
    "    \n",
    "    # Log CRS, bounds, and sample geometries for debugging\n",
    "    logging.info(f\"Roads CRS: {roads_gdf.crs}, Bounds: {roads_gdf.total_bounds}\")\n",
    "    logging.info(f\"Neighborhoods CRS: {neighborhoods_gdf.crs}, Bounds: {neighborhoods_gdf.total_bounds}\")\n",
    "    logging.info(f\"Accidents CRS: {accidents_gdf.crs}, Bounds: {accidents_gdf.total_bounds}\")\n",
    "    \n",
    "    # Log geometry types\n",
    "    logging.info(f\"Roads geometry types: {roads_gdf.geometry.type.unique()}\")\n",
    "    logging.info(f\"Neighborhoods geometry types: {neighborhoods_gdf.geometry.type.unique()}\")\n",
    "    \n",
    "    # Log sample geometries\n",
    "    sample_roads = roads_gdf.head(5)['geometry'].apply(lambda x: str(x)[:100])\n",
    "    sample_neighborhoods = neighborhoods_gdf.head(5)['geometry'].apply(lambda x: str(x)[:100])\n",
    "    logging.info(f\"Sample road geometries:\\n{sample_roads}\")\n",
    "    logging.info(f\"Sample neighborhood geometries:\\n{sample_neighborhoods}\")\n",
    "    \n",
    "    # Visualize data for debugging\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    neighborhoods_gdf.plot(ax=ax, color='blue', alpha=0.5, label='Neighborhoods')\n",
    "    roads_gdf.plot(ax=ax, color='red', alpha=0.5, label='Roads')\n",
    "    plt.legend()\n",
    "    plt.title('Roads and Neighborhoods Overlay')\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'roads_neighborhoods_overlap.png'))\n",
    "    plt.close()\n",
    "    logging.info(f\"Overlay plot saved to {os.path.join(BASE_DIR, 'roads_neighborhoods_overlap.png')}\")\n",
    "    \n",
    "    # Make local copies for roads and accidents to avoid modifying originals\n",
    "    roads_gdf_local = roads_gdf.copy()\n",
    "    accidents_gdf_local = accidents_gdf.copy()\n",
    "    \n",
    "    # Add unique identifier to accidents\n",
    "    accidents_gdf_local['accident_id'] = range(len(accidents_gdf_local))\n",
    "    \n",
    "    # Define width ranking\n",
    "    width_ranking = {\n",
    "        'motorway': 5, 'trunk': 5, 'primary': 4, 'secondary': 4, 'tertiary': 3,\n",
    "        'residential': 3, 'living_street': 3, 'service': 2, 'track': 2,\n",
    "        'path': 1, 'footway': 1, 'cycleway': 1, 'steps': 1, 'pedestrian': 1,\n",
    "        'unclassified': 0, 'bridleway': 0, 'unknown': 0\n",
    "    }\n",
    "    roads_gdf_local['width_rank'] = roads_gdf_local['class'].map(width_ranking).fillna(0).astype(int)\n",
    "    \n",
    "    # Buffer wider roads for accident assignment\n",
    "    roads_gdf_buffered = roads_gdf_local.copy()\n",
    "    roads_gdf_buffered['geometry'] = roads_gdf_buffered.apply(\n",
    "        lambda row: row['geometry'].buffer(5) if row['width_rank'] >= 4 else row['geometry'], axis=1\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Assigning accidents to nearest road...\")\n",
    "    accidents_gdf_local['geometry'] = accidents_gdf_local['geometry'].apply(fix_geometry)\n",
    "    accidents_gdf_local = accidents_gdf_local[accidents_gdf_local['geometry'].is_valid & ~accidents_gdf_local['geometry'].is_empty]\n",
    "    \n",
    "    if accidents_gdf_local.empty:\n",
    "        logging.warning(\"No valid accidents after geometry fixing.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Assign accidents to nearest road\n",
    "    nearest = gpd.sjoin_nearest(\n",
    "        accidents_gdf_local,\n",
    "        roads_gdf_buffered[['geometry', 'class', 'width_rank']],\n",
    "        how='left',\n",
    "        distance_col='distance'\n",
    "    )\n",
    "    nearest['weighted_distance'] = nearest['distance'] / (nearest['width_rank'].replace(0, 1) ** 2)\n",
    "    nearest = nearest.sort_values('weighted_distance').drop_duplicates(subset=['accident_id'], keep='first')\n",
    "    \n",
    "    matched_accidents = nearest[['accident_id', 'index_right']].copy()\n",
    "    matched_accidents.columns = ['accident_id', 'road_idx']\n",
    "    matched_accidents = matched_accidents.dropna(subset=['road_idx'])\n",
    "    matched_accidents['road_idx'] = matched_accidents['road_idx'].astype(int)\n",
    "    \n",
    "    logging.info(f\"Matched {len(matched_accidents)} accidents out of {len(accidents_gdf_local)}\")\n",
    "    \n",
    "    # Reassign accidents from footway/cycleway to wider roads if possible\n",
    "    footway_cycleway_accidents = matched_accidents[\n",
    "        matched_accidents['road_idx'].isin(\n",
    "            roads_gdf_local[roads_gdf_local['class'].isin(['footway', 'cycleway'])].index\n",
    "        )\n",
    "    ]\n",
    "    if not footway_cycleway_accidents.empty:\n",
    "        logging.info(f\"Reassigning {len(footway_cycleway_accidents)} accidents from footway/cycleway...\")\n",
    "        accidents_to_reassign = accidents_gdf_local[accidents_gdf_local['accident_id'].isin(footway_cycleway_accidents['accident_id'])].copy()\n",
    "        wider_roads = roads_gdf_buffered[roads_gdf_buffered['width_rank'] >= 4]\n",
    "        if not wider_roads.empty:\n",
    "            reassigned = gpd.sjoin_nearest(\n",
    "                accidents_to_reassign,\n",
    "                wider_roads[['geometry', 'class']],\n",
    "                how='left',\n",
    "                max_distance=10\n",
    "            )\n",
    "            reassigned_matches = reassigned[['accident_id', 'index_right']].copy()\n",
    "            reassigned_matches.columns = ['accident_id', 'road_idx']\n",
    "            reassigned_matches = reassigned_matches.dropna(subset=['road_idx'])\n",
    "            reassigned_matches['road_idx'] = reassigned_matches['road_idx'].astype(int)\n",
    "            matched_accidents = matched_accidents[~matched_accidents['accident_id'].isin(reassigned_matches['accident_id'])]\n",
    "            matched_accidents = pd.concat([matched_accidents, reassigned_matches], ignore_index=True)\n",
    "            logging.info(f\"Reassigned {len(reassigned_matches)} accidents to wider roads\")\n",
    "    \n",
    "    # Count accidents per road\n",
    "    accident_counts = matched_accidents.groupby('road_idx').size().reindex(roads_gdf_local.index, fill_value=0)\n",
    "    roads_gdf_local['accident_count'] = accident_counts\n",
    "    \n",
    "    logging.info(f\"Accidents by road type:\\n{roads_gdf_local.groupby('class')['accident_count'].sum()}\")\n",
    "    \n",
    "    # Filter roads by minimum length\n",
    "    roads_gdf_local = roads_gdf_local[roads_gdf_local['length_m'] >= MIN_ROAD_LENGTH]\n",
    "    \n",
    "    # Calculate accident density\n",
    "    roads_gdf_local['accident_density'] = roads_gdf_local['accident_count'] / (roads_gdf_local['length_m'] / 1000)\n",
    "    roads_gdf_local['accident_density'] = roads_gdf_local['accident_density'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # Adjust density by width rank\n",
    "    roads_gdf_local['accident_density'] = roads_gdf_local['accident_density'] * (roads_gdf_local['width_rank'].replace(0, 1) / 5)\n",
    "    \n",
    "    logging.info(f\"Road type counts:\\n{roads_gdf_local['class'].value_counts()}\")\n",
    "    print(f\"Road type counts:\\n{roads_gdf_local['class'].value_counts()}\")\n",
    "    \n",
    "    # Summarize by road type\n",
    "    summary = roads_gdf_local.groupby('class').agg({\n",
    "        'length_m': 'sum',\n",
    "        'accident_count': 'sum',\n",
    "        'accident_density': 'mean',\n",
    "        'width_rank': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    summary = summary[summary['length_m'] > 1000]\n",
    "    summary = summary[summary['width_rank'] > 0]\n",
    "    \n",
    "    print(\"\\n--- Road Type Accident Density Summary ---\")\n",
    "    print(summary[['class', 'length_m', 'accident_count', 'accident_density', 'width_rank']].round(2))\n",
    "    \n",
    "    if len(summary) >= 2:\n",
    "        corr, p_value = spearmanr(summary['width_rank'], summary['accident_density'])\n",
    "        logging.info(f\"Spearman's correlation between road width rank and accident density: {corr:.3f} (p-value: {p_value:.3f})\")\n",
    "        print(f\"Spearman's correlation: {corr:.3f} (p-value: {p_value:.3f})\")\n",
    "    else:\n",
    "        logging.warning(\"Insufficient road types for correlation analysis.\")\n",
    "        print(\"Insufficient road types for correlation analysis.\")\n",
    "    \n",
    "    # Compute average road accident density per neighborhood\n",
    "    logging.info(\"Computing average road accident density per neighborhood...\")\n",
    "    logging.info(f\"Roads DataFrame shape before join: {roads_gdf_local.shape}\")\n",
    "    logging.info(f\"Neighborhoods DataFrame shape before join: {neighborhoods_gdf.shape}\")\n",
    "    \n",
    "    roads_gdf_with_idx = roads_gdf_local[['geometry', 'class', 'length_m', 'width_rank', 'accident_density']].reset_index()\n",
    "    neighborhoods_gdf_with_idx = neighborhoods_gdf[['geometry', 'LIE_NAME']].reset_index()\n",
    "    \n",
    "    # Perform spatial join\n",
    "    road_neighborhoods = gpd.sjoin(\n",
    "        roads_gdf_with_idx,\n",
    "        neighborhoods_gdf_with_idx,\n",
    "        how='left',\n",
    "        predicate='intersects'\n",
    "    )\n",
    "    logging.info(f\"Road-neighborhood join resulted in {len(road_neighborhoods)} matches with columns: {road_neighborhoods.columns.tolist()}\")\n",
    "    \n",
    "    if road_neighborhoods['LIE_NAME'].isna().all():\n",
    "        logging.warning(\"No roads intersect with neighborhoods. Checking for geometry validity and CRS mismatch...\")\n",
    "        # Check geometry validity\n",
    "        logging.info(f\"Roads geometry validity: {roads_gdf_with_idx.geometry.is_valid.all()}\")\n",
    "        logging.info(f\"Neighborhoods geometry validity: {neighborhoods_gdf_with_idx.geometry.is_valid.all()}\")\n",
    "        \n",
    "        # Fix geometries if necessary\n",
    "        roads_gdf_with_idx['geometry'] = roads_gdf_with_idx['geometry'].apply(fix_geometry)\n",
    "        neighborhoods_gdf_with_idx['geometry'] = neighborhoods_gdf_with_idx['geometry'].apply(fix_geometry)\n",
    "        \n",
    "        # Retry spatial join\n",
    "        road_neighborhoods = gpd.sjoin(\n",
    "            roads_gdf_with_idx,\n",
    "            neighborhoods_gdf_with_idx,\n",
    "            how='left',\n",
    "            predicate='intersects'\n",
    "        )\n",
    "        logging.info(f\"Retried road-neighborhood join resulted in {len(road_neighborhoods)} matches\")\n",
    "        \n",
    "        if road_neighborhoods['LIE_NAME'].isna().all():\n",
    "            logging.warning(\"Still no intersections found. Buffering geometries by 200 meters...\")\n",
    "            roads_buffered = roads_gdf_with_idx.copy()\n",
    "            neighborhoods_buffered = neighborhoods_gdf_with_idx.copy()\n",
    "            roads_buffered['geometry'] = roads_buffered['geometry'].buffer(200)\n",
    "            neighborhoods_buffered['geometry'] = neighborhoods_buffered['geometry'].buffer(200)\n",
    "            \n",
    "            road_neighborhoods = gpd.sjoin(\n",
    "                roads_buffered,\n",
    "                neighborhoods_buffered,\n",
    "                how='left',\n",
    "                predicate='intersects'\n",
    "            )\n",
    "            logging.info(f\"Buffered road-neighborhood join resulted in {len(road_neighborhoods)} matches\")\n",
    "            \n",
    "            if road_neighborhoods['LIE_NAME'].isna().all():\n",
    "                logging.error(\"No intersections found even after buffering. Assigning default 0.\")\n",
    "                neighborhoods_gdf['avg_road_accident_density'] = 0\n",
    "            else:\n",
    "                avg_accident_density = road_neighborhoods.groupby('LIE_NAME')['accident_density'].mean()\n",
    "                avg_accident_density = avg_accident_density.reindex(neighborhoods_gdf['LIE_NAME'], fill_value=0)\n",
    "                neighborhoods_gdf['avg_road_accident_density'] = avg_accident_density.fillna(0)\n",
    "        else:\n",
    "            avg_accident_density = road_neighborhoods.groupby('LIE_NAME')['accident_density'].mean()\n",
    "            avg_accident_density = avg_accident_density.reindex(neighborhoods_gdf['LIE_NAME'], fill_value=0)\n",
    "            neighborhoods_gdf['avg_road_accident_density'] = avg_accident_density.fillna(0)\n",
    "    else:\n",
    "        avg_accident_density = road_neighborhoods.groupby('LIE_NAME')['accident_density'].mean()\n",
    "        avg_accident_density = avg_accident_density.reindex(neighborhoods_gdf['LIE_NAME'], fill_value=0)\n",
    "        neighborhoods_gdf['avg_road_accident_density'] = avg_accident_density.fillna(0)\n",
    "    \n",
    "    assigned_count = sum(~neighborhoods_gdf['avg_road_accident_density'].isna())\n",
    "    logging.info(f\"Assigned avg_road_accident_density to {assigned_count} neighborhoods\")\n",
    "    logging.info(f\"Avg road accident density stats:\\n{neighborhoods_gdf['avg_road_accident_density'].describe()}\")\n",
    "    \n",
    "    # Generate plots\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    summary_sorted = summary.sort_values('width_rank', ascending=False)\n",
    "    sns.barplot(data=summary_sorted, x='class', y='accident_density', hue='width_rank', dodge=False)\n",
    "    plt.xlabel('Road Type')\n",
    "    plt.ylabel('Mean Accident Density (Accidents per km)')\n",
    "    plt.title('Mean Accident Density by Road Type')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Width Rank')\n",
    "    plt.tight_layout()\n",
    "    bar_path = os.path.join(BASE_DIR, 'road_type_accident_bar.png')\n",
    "    plt.savefig(bar_path)\n",
    "    plt.close()\n",
    "    logging.info(f\"Bar chart saved to {bar_path}\")\n",
    "    print(f\"Bar chart saved to {bar_path}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=roads_gdf_local[roads_gdf_local['class'].isin(summary['class'])], \n",
    "                x='class', y='accident_density', hue='width_rank', dodge=False)\n",
    "    plt.xlabel('Road Type')\n",
    "    plt.ylabel('Accident Density (Accidents per km)')\n",
    "    plt.title('Distribution of Accident Density by Road Type')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yscale('log')\n",
    "    plt.legend(title='Width Rank')\n",
    "    plt.tight_layout()\n",
    "    box_path = os.path.join(BASE_DIR, 'road_type_accident_box.png')\n",
    "    plt.savefig(box_path)\n",
    "    plt.close()\n",
    "    logging.info(f\"Box chart saved to {box_path}\")\n",
    "    print(f\"Box chart saved to {box_path}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=summary, x='width_rank', y='accident_density', \n",
    "                    size='length_m', sizes=(50, 500), hue='class', style='class', alpha=0.7)\n",
    "    z = np.polyfit(summary['width_rank'], summary['accident_density'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(summary['width_rank'], p(summary['width_rank']), \"r--\", alpha=0.5)\n",
    "    plt.xlabel('Road Width Rank (1=Path, 5=Motorway)')\n",
    "    plt.ylabel('Mean Accident Density (Accidents per km)')\n",
    "    plt.title('Road Type vs. Accident Density')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    scatter_path = os.path.join(BASE_DIR, 'road_type_accident_scatter.png')\n",
    "    plt.savefig(scatter_path)\n",
    "    plt.close()\n",
    "    logging.info(f\"Scatter plot saved to {scatter_path}\")\n",
    "    print(f\"Scatter plot saved to {scatter_path}\")\n",
    "    \n",
    "    top_types = summary.nlargest(3, 'accident_density')[['class', 'accident_density']]\n",
    "    logging.info(f\"Top 3 road types by accident density:\\n{top_types.round(2)}\")\n",
    "    print(f\"Top 3 road types by accident density:\\n{top_types.round(2)}\")\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d0683",
   "metadata": {},
   "source": [
    "Cell 4 Main Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c3d3ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    logging.info(\"Stage 1: Loading and preparing data...\")\n",
    "    \n",
    "    # Define file paths and their corresponding keys\n",
    "    data_files = {\n",
    "        'neighborhoods': LANDUSE_NDVI_PATH,\n",
    "        'buildings': OSM_BUILDINGS_PATH,\n",
    "        'roads': OSM_ROADS_PATH,\n",
    "        'trees': OSM_TREES_PATH,\n",
    "        'transit': OSM_TRANSIT_PATH,\n",
    "        'urban_masterplan': URBAN_MASTERPLAN_PATH,\n",
    "        'accidents': ACCIDENTS_PATH,\n",
    "        'population': POPULATION_PATH\n",
    "    }\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # Load data with progress bar\n",
    "    for key, path in tqdm(data_files.items(), desc=\"Loading files\"):\n",
    "        try:\n",
    "            if key == 'population':\n",
    "                with open(path, 'r') as f:\n",
    "                    data[key] = pd.DataFrame(json.load(f))\n",
    "                # Log columns of population_df to debug missing columns\n",
    "                logging.info(f\"Columns in population_df after loading: {list(data[key].columns)}\")\n",
    "            elif path.endswith('.geoparquet'):\n",
    "                data[key] = gpd.read_parquet(path)\n",
    "            else:\n",
    "                data[key] = gpd.read_file(path)\n",
    "            logging.info(f\"Loaded {key} with shape {data[key].shape}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load {key} from {path}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Log columns of neighborhoods_gdf to debug missing 'area_km2'\n",
    "    neighborhoods_gdf = data['neighborhoods']\n",
    "    logging.info(f\"Columns in neighborhoods_gdf after loading: {list(neighborhoods_gdf.columns)}\")\n",
    "    \n",
    "    # Ensure all GeoDataFrames are in the same CRS\n",
    "    target_crs = 'EPSG:3826'\n",
    "    for key in ['neighborhoods', 'buildings', 'roads', 'trees', 'transit', 'urban_masterplan', 'accidents']:\n",
    "        if key in data and isinstance(data[key], gpd.GeoDataFrame):\n",
    "            if data[key].crs != target_crs:\n",
    "                data[key] = data[key].to_crs(target_crs)\n",
    "                logging.info(f\"Converted {key} to CRS {target_crs}\")\n",
    "    \n",
    "    # Fix geometries in all GeoDataFrames\n",
    "    for key in ['neighborhoods', 'buildings', 'roads', 'trees', 'transit', 'urban_masterplan', 'accidents']:\n",
    "        if key in data and isinstance(data[key], gpd.GeoDataFrame):\n",
    "            data[key]['geometry'] = data[key]['geometry'].apply(fix_geometry)\n",
    "            invalid_geoms = data[key][~data[key].geometry.is_valid]\n",
    "            if not invalid_geoms.empty:\n",
    "                logging.warning(f\"Found {len(invalid_geoms)} invalid geometries in {key} after fixing.\")\n",
    "                data[key] = data[key][data[key].geometry.is_valid]\n",
    "    \n",
    "    # Compute intersections for neighborhoods\n",
    "    logging.info(\"Computing intersections for neighborhoods...\")\n",
    "    roads_gdf = data['roads']\n",
    "    \n",
    "    # Log columns of roads_gdf to debug missing 'length_m'\n",
    "    logging.info(f\"Columns in roads_gdf after loading: {list(roads_gdf.columns)}\")\n",
    "    \n",
    "    # Extract endpoints from road segments\n",
    "    logging.info(\"Extracting endpoints from road segments...\")\n",
    "    endpoints = []\n",
    "    road_indices = []\n",
    "    for idx, row in tqdm(roads_gdf.iterrows(), total=len(roads_gdf), desc=\"Extracting endpoints\"):\n",
    "        geom = row['geometry']\n",
    "        if geom.geom_type == 'LineString':\n",
    "            coords = list(geom.coords)\n",
    "            start_point = Point(coords[0])\n",
    "            end_point = Point(coords[-1])\n",
    "            if start_point.is_valid and end_point.is_valid:\n",
    "                endpoints.extend([start_point, end_point])\n",
    "                road_indices.extend([idx, idx])\n",
    "        elif geom.geom_type == 'MultiLineString':\n",
    "            for line in geom.geoms:\n",
    "                coords = list(line.coords)\n",
    "                start_point = Point(coords[0])\n",
    "                end_point = Point(coords[-1])\n",
    "                if start_point.is_valid and end_point.is_valid:\n",
    "                    endpoints.extend([start_point, end_point])\n",
    "                    road_indices.extend([idx, idx])\n",
    "    \n",
    "    if not endpoints:\n",
    "        logging.warning(\"No valid endpoints extracted from road segments. Using fallback method for intersections.\")\n",
    "        neighborhoods_gdf = data['neighborhoods']\n",
    "        road_neighborhoods = gpd.sjoin(roads_gdf[['geometry']], neighborhoods_gdf[['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "        intersection_counts = road_neighborhoods.groupby('index_right').size()\n",
    "        neighborhoods_gdf['intersection_count'] = intersection_counts.reindex(neighborhoods_gdf.index, fill_value=0)\n",
    "    else:\n",
    "        endpoints_gdf = gpd.GeoDataFrame({'geometry': endpoints, 'road_idx': road_indices}, crs=target_crs)\n",
    "        \n",
    "        # Create a spatial index for endpoints\n",
    "        endpoints_sindex = endpoints_gdf.sindex\n",
    "        \n",
    "        # Cluster endpoints to identify intersections (points shared by 3 or more roads)\n",
    "        logging.info(\"Building endpoint-to-road mapping...\")\n",
    "        endpoint_to_roads = {}\n",
    "        for idx, point in tqdm(endpoints_gdf.iterrows(), total=len(endpoints_gdf), desc=\"Building endpoint-to-road mapping\"):\n",
    "            point_geom = point['geometry']\n",
    "            road_idx = point['road_idx']\n",
    "            point_key = (round(point_geom.x, 6), round(point_geom.y, 6))  # Round to avoid floating-point precision issues\n",
    "            if point_key not in endpoint_to_roads:\n",
    "                endpoint_to_roads[point_key] = set()\n",
    "            endpoint_to_roads[point_key].add(road_idx)\n",
    "        \n",
    "        logging.info(\"Identifying intersections...\")\n",
    "        intersections = []\n",
    "        for point_key, road_ids in tqdm(endpoint_to_roads.items(), desc=\"Identifying intersections\"):\n",
    "            if len(road_ids) >= 3:  # Intersection if shared by 3 or more roads\n",
    "                intersections.append(Point(point_key))\n",
    "        \n",
    "        if not intersections:\n",
    "            logging.warning(\"No intersections found using endpoint clustering. Using fallback method.\")\n",
    "            neighborhoods_gdf = data['neighborhoods']\n",
    "            road_neighborhoods = gpd.sjoin(roads_gdf[['geometry']], neighborhoods_gdf[['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "            intersection_counts = road_neighborhoods.groupby('index_right').size()\n",
    "            neighborhoods_gdf['intersection_count'] = intersection_counts.reindex(neighborhoods_gdf.index, fill_value=0)\n",
    "        else:\n",
    "            intersections_gdf = gpd.GeoDataFrame({'geometry': intersections}, crs=target_crs)\n",
    "            \n",
    "            # Count intersections per neighborhood\n",
    "            logging.info(\"Counting intersections per neighborhood...\")\n",
    "            neighborhoods_gdf = data['neighborhoods']\n",
    "            intersections_joined = gpd.sjoin(intersections_gdf, neighborhoods_gdf[['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "            intersection_counts = intersections_joined.groupby('index_right').size()\n",
    "            neighborhoods_gdf['intersection_count'] = intersection_counts.reindex(neighborhoods_gdf.index, fill_value=0)\n",
    "    \n",
    "    # Compute or verify area_km2\n",
    "    if 'area_km2' not in neighborhoods_gdf.columns:\n",
    "        logging.warning(\"'area_km2' column missing in neighborhoods_gdf. Computing from geometry...\")\n",
    "        # Compute area in square meters, then convert to square kilometers\n",
    "        neighborhoods_gdf['area_m2'] = neighborhoods_gdf['geometry'].area\n",
    "        neighborhoods_gdf['area_km2'] = neighborhoods_gdf['area_m2'] / 1_000_000  # Convert m² to km²\n",
    "        logging.info(f\"Computed area_km2 stats:\\n{neighborhoods_gdf['area_km2'].describe()}\")\n",
    "    else:\n",
    "        logging.info(f\"area_km2 already present. Stats:\\n{neighborhoods_gdf['area_km2'].describe()}\")\n",
    "    \n",
    "    # Compute intersection density\n",
    "    neighborhoods_gdf['intersection_density'] = neighborhoods_gdf['intersection_count'] / neighborhoods_gdf['area_km2'].replace(0, 1e-6)\n",
    "    logging.info(f\"Intersection count stats:\\n{neighborhoods_gdf['intersection_count'].describe()}\")\n",
    "    logging.info(f\"Intersection density stats:\\n{neighborhoods_gdf['intersection_density'].describe()}\")\n",
    "    \n",
    "    # Cache the result\n",
    "    try:\n",
    "        neighborhoods_gdf.to_parquet(INTERSECTION_CACHE_PATH)\n",
    "        logging.info(f\"Saved neighborhoods with intersections to {INTERSECTION_CACHE_PATH}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save neighborhoods with intersections: {e}\")\n",
    "    \n",
    "    data['neighborhoods'] = neighborhoods_gdf\n",
    "    \n",
    "    # Compute tree count per neighborhood\n",
    "    logging.info(\"Computing tree count per neighborhood...\")\n",
    "    trees_gdf = data['trees']\n",
    "    trees_joined = gpd.sjoin(trees_gdf[['geometry']], data['neighborhoods'][['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "    tree_counts = trees_joined.groupby('index_right').size()\n",
    "    data['neighborhoods']['tree_count'] = tree_counts.reindex(data['neighborhoods'].index, fill_value=0)\n",
    "    \n",
    "    # Compute transit count per neighborhood\n",
    "    logging.info(\"Computing transit count per neighborhood...\")\n",
    "    transit_gdf = data['transit']\n",
    "    transit_joined = gpd.sjoin(transit_gdf[['geometry']], data['neighborhoods'][['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "    transit_counts = transit_joined.groupby('index_right').size()\n",
    "    data['neighborhoods']['transit_count'] = transit_counts.reindex(data['neighborhoods'].index, fill_value=0)\n",
    "    \n",
    "    # Compute accident count per neighborhood\n",
    "    logging.info(\"Computing accident count per neighborhood...\")\n",
    "    accidents_gdf = data['accidents']\n",
    "    accidents_buffered = accidents_gdf.copy()\n",
    "    accidents_buffered['geometry'] = accidents_buffered['geometry'].buffer(BUFFER_DISTANCE)\n",
    "    accidents_joined = gpd.sjoin(accidents_buffered[['geometry']], data['neighborhoods'][['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "    accident_counts = accidents_joined.groupby('index_right').size()\n",
    "    data['neighborhoods']['accident_count'] = accident_counts.reindex(data['neighborhoods'].index, fill_value=0)\n",
    "    \n",
    "    # Compute road density per neighborhood\n",
    "    logging.info(\"Computing road density per neighborhood...\")\n",
    "    roads_gdf = data['roads']\n",
    "    \n",
    "    # Log columns of roads_gdf to debug missing 'length_m'\n",
    "    logging.info(f\"Columns in roads_gdf before computing road density: {list(roads_gdf.columns)}\")\n",
    "    \n",
    "    # Compute length_m if missing\n",
    "    if 'length_m' not in roads_gdf.columns:\n",
    "        logging.warning(\"'length_m' column missing in roads_gdf. Computing from geometry...\")\n",
    "        roads_gdf['length_m'] = roads_gdf['geometry'].length  # Length in meters (since CRS is EPSG:3826)\n",
    "        logging.info(f\"Computed length_m stats:\\n{roads_gdf['length_m'].describe()}\")\n",
    "    \n",
    "    roads_joined = gpd.sjoin(roads_gdf[['geometry', 'length_m']], data['neighborhoods'][['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "    road_lengths = roads_joined.groupby('index_right')['length_m'].sum()\n",
    "    data['neighborhoods']['road_density'] = road_lengths.reindex(data['neighborhoods'].index, fill_value=0) / (data['neighborhoods']['area_km2'] * 1000)\n",
    "    logging.info(f\"Road density stats:\\n{data['neighborhoods']['road_density'].describe()}\")\n",
    "    \n",
    "    # Merge population data\n",
    "    logging.info(\"Merging population data...\")\n",
    "    population_df = data['population']\n",
    "    population_df['LIE_NAME'] = population_df['LIE_NAME'].astype(str).str.strip()\n",
    "    data['neighborhoods']['LIE_NAME'] = data['neighborhoods']['LIE_NAME'].astype(str).str.strip()\n",
    "    \n",
    "    # Check for possible column names for total_population and elderly_percentage\n",
    "    expected_cols = ['total_population', 'elderly_percentage']\n",
    "    population_cols = list(population_df.columns)\n",
    "    missing_cols = [col for col in expected_cols if col not in population_cols]\n",
    "    \n",
    "    if missing_cols:\n",
    "        logging.warning(f\"Expected columns {missing_cols} not found in population_df. Attempting to find alternatives...\")\n",
    "        # Possible alternative names\n",
    "        total_pop_alt = None\n",
    "        elderly_alt = None\n",
    "        for col in population_cols:\n",
    "            col_lower = col.lower()\n",
    "            if 'population' in col_lower and total_pop_alt is None:\n",
    "                total_pop_alt = col\n",
    "                logging.info(f\"Found alternative for total_population: {col}\")\n",
    "            if 'elderly' in col_lower and elderly_alt is None:\n",
    "                elderly_alt = col\n",
    "                logging.info(f\"Found alternative for elderly_percentage: {col}\")\n",
    "        \n",
    "        # Rename columns if alternatives are found\n",
    "        if total_pop_alt:\n",
    "            population_df = population_df.rename(columns={total_pop_alt: 'total_population'})\n",
    "        else:\n",
    "            logging.warning(\"No alternative found for total_population. Setting to 0.\")\n",
    "            population_df['total_population'] = 0\n",
    "        if elderly_alt:\n",
    "            population_df = population_df.rename(columns={elderly_alt: 'elderly_percentage'})\n",
    "        else:\n",
    "            logging.warning(\"No alternative found for elderly_percentage. Setting to 0.\")\n",
    "            population_df['elderly_percentage'] = 0\n",
    "    \n",
    "    # Perform the merge\n",
    "    data['neighborhoods'] = data['neighborhoods'].merge(\n",
    "        population_df[['LIE_NAME', 'total_population', 'elderly_percentage']],\n",
    "        on='LIE_NAME',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Compute land use percentages\n",
    "    logging.info(\"Computing land use percentages for neighborhoods...\")\n",
    "    urban_masterplan_gdf = data['urban_masterplan']\n",
    "    print_percentage_calculation(data['neighborhoods'], urban_masterplan_gdf, sample_size=3)\n",
    "    \n",
    "    for idx, row in data['neighborhoods'].iterrows():\n",
    "        neighborhood_geom = row['geometry']\n",
    "        if not neighborhood_geom.is_valid:\n",
    "            continue\n",
    "        relevant_masterplan = urban_masterplan_gdf[urban_masterplan_gdf.intersects(neighborhood_geom)]\n",
    "        if relevant_masterplan.empty:\n",
    "            continue\n",
    "        \n",
    "        temp_gdf = gpd.GeoDataFrame({'geometry': [neighborhood_geom]}, crs='EPSG:3826')\n",
    "        intersected = gpd.overlay(temp_gdf, relevant_masterplan, how='intersection', keep_geom_type=False)\n",
    "        if intersected.empty:\n",
    "            continue\n",
    "        \n",
    "        intersected['geometry'] = intersected['geometry'].apply(fix_geometry)\n",
    "        intersected = intersected[intersected.geometry.is_valid & ~intersected.geometry.is_empty]\n",
    "        if intersected.empty:\n",
    "            continue\n",
    "        \n",
    "        intersected['priority'] = intersected['Category'].map(CATEGORY_PRIORITY)\n",
    "        intersected = intersected.sort_values(by='priority', ascending=False)\n",
    "        \n",
    "        total_area = intersected.geometry.union_all().area\n",
    "        remaining_geom = intersected.geometry.union_all()\n",
    "        category_areas = {}\n",
    "        for category in intersected['Category'].unique():\n",
    "            category_rows = intersected[intersected['Category'] == category]\n",
    "            category_geom = category_rows.geometry.union_all()\n",
    "            try:\n",
    "                category_area_geom = category_geom.intersection(remaining_geom)\n",
    "                category_area = category_area_geom.area\n",
    "                category_areas[category] = category_area\n",
    "                remaining_geom = remaining_geom.difference(category_area_geom)\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Topology error for category {category} in neighborhood {row['LIE_NAME']}: {e}\")\n",
    "                category_areas[category] = 0.0\n",
    "        \n",
    "        for category in CATEGORY_PRIORITY.keys():\n",
    "            area = category_areas.get(category, 0.0)\n",
    "            percentage = (area / total_area * 100) if total_area > 0 else 0.0\n",
    "            data['neighborhoods'].at[idx, f'land_use_{category.lower()}_percent'] = percentage\n",
    "    \n",
    "    # Fill NaN values in land use percentages\n",
    "    for category in CATEGORY_PRIORITY.keys():\n",
    "        col = f'land_use_{category.lower()}_percent'\n",
    "        data['neighborhoods'][col] = data['neighborhoods'][col].fillna(0.0)\n",
    "    \n",
    "    # Fill NaN values in other columns\n",
    "    for col in ['intersection_count', 'intersection_density', 'tree_count', 'transit_count', 'accident_count', 'road_density', 'total_population', 'elderly_percentage']:\n",
    "        data['neighborhoods'][col] = data['neighborhoods'][col].fillna(0)\n",
    "    \n",
    "    # Print data structure summary\n",
    "    print_data_structure(data)\n",
    "    \n",
    "    logging.info(\"Finished loading and preparing data.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48993e82",
   "metadata": {},
   "source": [
    "Cell 5 compute_intersection_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d2d1971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intersection_counts(neighborhoods_gdf, roads_gdf):\n",
    "    logging.info(\"Computing intersection counts for neighborhoods...\")\n",
    "    \n",
    "    # Extract endpoints from road segments\n",
    "    def get_endpoints(line):\n",
    "        if line is None or line.is_empty:\n",
    "            return []\n",
    "        coords = list(line.coords)\n",
    "        return [Point(coords[0]), Point(coords[-1])]\n",
    "    \n",
    "    endpoints = []\n",
    "    for idx, row in tqdm(roads_gdf.iterrows(), total=len(roads_gdf), desc=\"Extracting endpoints\"):\n",
    "        points = get_endpoints(row['geometry'])\n",
    "        for point in points:\n",
    "            endpoints.append({'geometry': point, 'road_idx': idx})\n",
    "    \n",
    "    endpoints_gdf = gpd.GeoDataFrame(endpoints, crs='EPSG:3826')\n",
    "    \n",
    "    # Build a mapping of endpoints to road indices\n",
    "    endpoint_to_roads = {}\n",
    "    for idx, row in tqdm(endpoints_gdf.iterrows(), total=len(endpoints_gdf), desc=\"Building endpoint-to-road mapping\"):\n",
    "        point = row['geometry']\n",
    "        road_idx = row['road_idx']\n",
    "        point_tuple = (point.x, point.y)\n",
    "        if point_tuple not in endpoint_to_roads:\n",
    "            endpoint_to_roads[point_tuple] = set()\n",
    "        endpoint_to_roads[point_tuple].add(road_idx)\n",
    "    \n",
    "    # Identify intersections (endpoints shared by 3 or more roads)\n",
    "    intersections = []\n",
    "    for point_tuple, road_indices in tqdm(endpoint_to_roads.items(), desc=\"Identifying intersections\"):\n",
    "        if len(road_indices) >= 3:  # Intersection if 3 or more roads share the endpoint\n",
    "            intersections.append({'geometry': Point(point_tuple)})\n",
    "    \n",
    "    if not intersections:\n",
    "        logging.warning(\"No intersections found. Setting intersection counts to 0.\")\n",
    "        neighborhoods_gdf['intersection_count'] = 0\n",
    "        neighborhoods_gdf['intersection_density'] = 0.0\n",
    "        return neighborhoods_gdf\n",
    "    \n",
    "    intersections_gdf = gpd.GeoDataFrame(intersections, crs='EPSG:3826')\n",
    "    \n",
    "    # Spatial join to count intersections per neighborhood\n",
    "    intersection_counts = gpd.sjoin(\n",
    "        neighborhoods_gdf[['geometry', 'LIE_NAME']],\n",
    "        intersections_gdf,\n",
    "        how='left',\n",
    "        predicate='contains'\n",
    "    )\n",
    "    intersection_counts = intersection_counts.groupby('LIE_NAME').size().reindex(neighborhoods_gdf['LIE_NAME'], fill_value=0)\n",
    "    neighborhoods_gdf['intersection_count'] = intersection_counts\n",
    "    \n",
    "    # Compute intersection density (intersections per km²)\n",
    "    neighborhoods_gdf['intersection_density'] = neighborhoods_gdf['intersection_count'] / neighborhoods_gdf['area_km2']\n",
    "    neighborhoods_gdf['intersection_density'] = neighborhoods_gdf['intersection_density'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    logging.info(f\"Intersection count stats: {neighborhoods_gdf['intersection_count'].describe()}\")\n",
    "    logging.info(f\"Intersection density stats: {neighborhoods_gdf['intersection_density'].describe()}\")\n",
    "    \n",
    "    return neighborhoods_gdf\n",
    "\n",
    "def build_graph(data, force_recompute=False):\n",
    "    logging.info(\"Stage 2: Building the graph...\")\n",
    "    \n",
    "    # Compute data hash to check if graph needs recomputing\n",
    "    data_hash = compute_data_hash(data)\n",
    "    cached_hash = None\n",
    "    if os.path.exists(GRAPH_DATA_HASH_PATH):\n",
    "        with open(GRAPH_DATA_HASH_PATH, 'r') as f:\n",
    "            cached_hash = f.read().strip()\n",
    "    \n",
    "    if not force_recompute and cached_hash == data_hash and all(\n",
    "        os.path.exists(path) for path in [GRAPH_NODES_CACHE_PATH, GRAPH_EDGES_CACHE_PATH, GRAPH_NODE_ID_CACHE_PATH]\n",
    "    ):\n",
    "        logging.info(\"Data unchanged. Loading graph from cache...\")\n",
    "        nodes_df = cudf.read_parquet(GRAPH_NODES_CACHE_PATH)\n",
    "        edges_df = cudf.read_parquet(GRAPH_EDGES_CACHE_PATH)\n",
    "        with open(GRAPH_NODE_ID_CACHE_PATH, 'r') as f:\n",
    "            node_id_to_index = json.load(f)\n",
    "        G = cugraph.Graph()\n",
    "        G.from_cudf_edgelist(\n",
    "            edges_df,\n",
    "            source='src',\n",
    "            destination='dst',\n",
    "            edge_attr='weight'\n",
    "        )\n",
    "        G._nodes = nodes_df\n",
    "        logging.info(\"Graph loaded from cache.\")\n",
    "        return G\n",
    "    \n",
    "    neighborhoods_gdf = data['neighborhoods'].copy()\n",
    "    buildings_gdf = data['buildings'].copy()\n",
    "    roads_gdf = data['roads'].copy()\n",
    "    trees_gdf = data['trees'].copy()\n",
    "    transit_gdf = data['transit'].copy()\n",
    "    \n",
    "    # Create nodes for neighborhoods, buildings, roads, trees, and transit\n",
    "    nodes = []\n",
    "    node_id_to_index = {}\n",
    "    current_idx = 0\n",
    "    \n",
    "    # Neighborhood nodes\n",
    "    for idx, row in neighborhoods_gdf.iterrows():\n",
    "        node_id = f\"neighborhood_{row['LIE_NAME']}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'neighborhood',\n",
    "            'LIE_NAME': row['LIE_NAME'],\n",
    "            'geometry': row['geometry'].centroid,\n",
    "            'ndvi_mean': row['ndvi_mean'],\n",
    "            'total_population': row['total_population'],\n",
    "            'elderly_percentage': row['elderly_percentage'],\n",
    "            'area_km2': row['area_km2']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    # Building nodes\n",
    "    for idx, row in buildings_gdf.iterrows():\n",
    "        node_id = f\"building_{idx}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'building',\n",
    "            'LIE_NAME': None,\n",
    "            'geometry': row['geometry'].centroid,\n",
    "            'building_type': row['building'],\n",
    "            'area_m2': row['area_m2']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    # Road nodes\n",
    "    for idx, row in roads_gdf.iterrows():\n",
    "        node_id = f\"road_{idx}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'road',\n",
    "            'LIE_NAME': None,\n",
    "            'geometry': row['geometry'].centroid,\n",
    "            'class': row['class'],\n",
    "            'length_m': row['length_m']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    # Tree nodes\n",
    "    for idx, row in trees_gdf.iterrows():\n",
    "        node_id = f\"tree_{idx}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'tree',\n",
    "            'LIE_NAME': None,\n",
    "            'geometry': row['geometry']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    # Transit nodes\n",
    "    for idx, row in transit_gdf.iterrows():\n",
    "        node_id = f\"transit_{idx}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'transit',\n",
    "            'LIE_NAME': None,\n",
    "            'geometry': row['geometry'],\n",
    "            'class': row['class']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    nodes_df = pd.DataFrame(nodes)\n",
    "    nodes_gdf = gpd.GeoDataFrame(nodes_df, geometry='geometry', crs='EPSG:3826')\n",
    "    nodes_df = cudf.from_pandas(nodes_df.drop(columns=['geometry']))\n",
    "    \n",
    "    # Create edges based on spatial proximity\n",
    "    edges = []\n",
    "    nodes_gdf_sindex = nodes_gdf.sindex\n",
    "    \n",
    "    # Neighborhood-to-neighborhood edges (shared borders)\n",
    "    logging.info(\"Creating neighborhood-to-neighborhood edges...\")\n",
    "    for idx1, row1 in neighborhoods_gdf.iterrows():\n",
    "        geom1 = row1['geometry']\n",
    "        node_idx1 = node_id_to_index[f\"neighborhood_{row1['LIE_NAME']}\"]\n",
    "        possible_matches = list(nodes_gdf_sindex.query(geom1, predicate='intersects'))\n",
    "        for idx2 in possible_matches:\n",
    "            row2 = nodes_gdf.iloc[idx2]\n",
    "            if row2['type'] != 'neighborhood':\n",
    "                continue\n",
    "            if row1['LIE_NAME'] == row2['LIE_NAME']:\n",
    "                continue\n",
    "            geom2 = neighborhoods_gdf[neighborhoods_gdf['LIE_NAME'] == row2['LIE_NAME']]['geometry'].iloc[0]\n",
    "            if geom1.intersects(geom2):\n",
    "                node_idx2 = node_id_to_index[f\"neighborhood_{row2['LIE_NAME']}\"]\n",
    "                edges.append({\n",
    "                    'src': node_idx1,\n",
    "                    'dst': node_idx2,\n",
    "                    'weight': 1.0\n",
    "                })\n",
    "    \n",
    "    # Other edges (neighborhood to building, road, tree, transit)\n",
    "    logging.info(\"Creating edges between neighborhoods and other entities...\")\n",
    "    for idx, row in tqdm(nodes_gdf.iterrows(), total=len(nodes_gdf), desc=\"Creating edges\"):\n",
    "        if row['type'] == 'neighborhood':\n",
    "            continue\n",
    "        geom = row['geometry']\n",
    "        possible_matches = list(neighborhoods_gdf.sindex.query(geom, predicate='contains'))\n",
    "        for match_idx in possible_matches:\n",
    "            neighborhood = neighborhoods_gdf.iloc[match_idx]\n",
    "            if neighborhood['geometry'].contains(geom):\n",
    "                node_idx1 = node_id_to_index[row['node_id']]\n",
    "                node_idx2 = node_id_to_index[f\"neighborhood_{neighborhood['LIE_NAME']}\"]\n",
    "                weight = 1.0\n",
    "                if row['type'] == 'transit':\n",
    "                    weight = 2.0  # Higher weight for transit nodes\n",
    "                edges.append({\n",
    "                    'src': node_idx1,\n",
    "                    'dst': node_idx2,\n",
    "                    'weight': weight\n",
    "                })\n",
    "                edges.append({\n",
    "                    'src': node_idx2,\n",
    "                    'dst': node_idx1,\n",
    "                    'weight': weight\n",
    "                })\n",
    "    \n",
    "    edges_df = cudf.DataFrame(edges)\n",
    "    \n",
    "    # Build the graph\n",
    "    G = cugraph.Graph()\n",
    "    G.from_cudf_edgelist(\n",
    "        edges_df,\n",
    "        source='src',\n",
    "        destination='dst',\n",
    "        edge_attr='weight'\n",
    "    )\n",
    "    G._nodes = nodes_df\n",
    "    \n",
    "    # Cache the graph\n",
    "    nodes_df.to_parquet(GRAPH_NODES_CACHE_PATH)\n",
    "    edges_df.to_parquet(GRAPH_EDGES_CACHE_PATH)\n",
    "    with open(GRAPH_NODE_ID_CACHE_PATH, 'w') as f:\n",
    "        json.dump(node_id_to_index, f)\n",
    "    with open(GRAPH_DATA_HASH_PATH, 'w') as f:\n",
    "        f.write(data_hash)\n",
    "    \n",
    "    logging.info(\"Graph construction completed.\")\n",
    "    return G\n",
    "\n",
    "def prepare_gnn_data(G):\n",
    "    logging.info(\"Stage 3: Preparing data for GNN...\")\n",
    "    \n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    edges_df = G.edgelist.edgelist_df.to_pandas()\n",
    "    \n",
    "    # Create node features\n",
    "    feature_columns = [\n",
    "        'ndvi_mean', 'total_population', 'elderly_percentage', 'area_km2',\n",
    "        'area_m2', 'length_m'\n",
    "    ]\n",
    "    features = []\n",
    "    for idx, row in nodes_df.iterrows():\n",
    "        node_features = []\n",
    "        for col in feature_columns:\n",
    "            value = row.get(col, 0.0)\n",
    "            if pd.isna(value):\n",
    "                value = 0.0\n",
    "            node_features.append(value)\n",
    "        \n",
    "        # One-hot encode node type\n",
    "        node_type = row['type']\n",
    "        type_encoding = [0] * 5  # 5 types: neighborhood, building, road, tree, transit\n",
    "        type_mapping = {\n",
    "            'neighborhood': 0,\n",
    "            'building': 1,\n",
    "            'road': 2,\n",
    "            'tree': 3,\n",
    "            'transit': 4\n",
    "        }\n",
    "        type_idx = type_mapping.get(node_type, 0)\n",
    "        type_encoding[type_idx] = 1\n",
    "        node_features.extend(type_encoding)\n",
    "        \n",
    "        features.append(node_features)\n",
    "    \n",
    "    feature_matrix = np.array(features, dtype=np.float32)\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    numerical_features = feature_matrix[:, :len(feature_columns)]\n",
    "    means = numerical_features.mean(axis=0)\n",
    "    stds = numerical_features.std(axis=0)\n",
    "    stds[stds == 0] = 1  # Avoid division by zero\n",
    "    numerical_features = (numerical_features - means) / stds\n",
    "    feature_matrix[:, :len(feature_columns)] = numerical_features\n",
    "    \n",
    "    # Create edge indices for PyG\n",
    "    edge_index = torch.tensor(\n",
    "        np.array([edges_df['src'].values, edges_df['dst'].values]),\n",
    "        dtype=torch.long\n",
    "    )\n",
    "    edge_attr = torch.tensor(edges_df['weight'].values, dtype=torch.float)\n",
    "    \n",
    "    # Create target (walkability score) for neighborhood nodes\n",
    "    y = np.zeros(len(nodes_df), dtype=np.float32)\n",
    "    if 'walkability_score' in nodes_df.columns:\n",
    "        walkability_scores = nodes_df['walkability_score'].fillna(0).values\n",
    "        mask = nodes_df['type'] == 'neighborhood'\n",
    "        y[mask] = walkability_scores[mask]\n",
    "    else:\n",
    "        logging.warning(\"Walkability scores not found in nodes_df. Setting targets to 0.\")\n",
    "    \n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    \n",
    "    node_type_mapping = {\n",
    "        'neighborhood': 0,\n",
    "        'building': 1,\n",
    "        'road': 2,\n",
    "        'tree': 3,\n",
    "        'transit': 4\n",
    "    }\n",
    "    node_type = nodes_df['type'].map(node_type_mapping).fillna(-1).astype(int).values\n",
    "    node_type = torch.tensor(node_type, dtype=torch.long)\n",
    "    \n",
    "    data = Data(\n",
    "        x=torch.tensor(feature_matrix, dtype=torch.float),\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=y,\n",
    "        node_type=node_type\n",
    "    )\n",
    "    \n",
    "    logging.info(\"GNN data prepared.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b04d9d",
   "metadata": {},
   "source": [
    "Cell 6: Graph Construction (build_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4158e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def compute_neighborhood_neighborhood_edges(args):\n",
    "    idx, row, neighborhoods_gdf, neighborhood_sindex = args\n",
    "    edges = []\n",
    "    geom = row['geometry']\n",
    "    possible_matches_index = list(neighborhood_sindex.intersection(geom.bounds))\n",
    "    for other_idx in possible_matches_index:\n",
    "        if other_idx != idx:\n",
    "            other_row = neighborhoods_gdf.iloc[other_idx]\n",
    "            other_geom = other_row['geometry']\n",
    "            try:\n",
    "                if geom.buffer(1e-3).intersects(other_geom.buffer(1e-3)) or geom.buffer(1e-3).touches(other_geom.buffer(1e-3)):\n",
    "                    src = f\"neighborhood_{idx}\"\n",
    "                    dst = f\"neighborhood_{other_idx}\"\n",
    "                    edges.append({'src': src, 'dst': dst})\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error checking intersection between neighborhood {idx} and {other_idx}: {e}\")\n",
    "    return edges\n",
    "\n",
    "def compute_neighborhood_building_edges(args):\n",
    "    idx, row, buildings_gdf, building_sindex = args\n",
    "    edges = []\n",
    "    geom = row['geometry']\n",
    "    possible_matches_index = list(building_sindex.intersection(geom.bounds))\n",
    "    for building_idx in possible_matches_index:\n",
    "        building_row = buildings_gdf.iloc[building_idx]\n",
    "        building_geom = building_row['geometry']\n",
    "        try:\n",
    "            if geom.buffer(1e-3).intersects(building_geom.buffer(1e-3)):\n",
    "                src = f\"neighborhood_{idx}\"\n",
    "                dst = f\"building_{building_idx}\"\n",
    "                edges.append({'src': src, 'dst': dst})\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error checking intersection between neighborhood {idx} and building {building_idx}: {e}\")\n",
    "    return edges\n",
    "\n",
    "def compute_neighborhood_road_edges(args):\n",
    "    idx, row, roads_gdf, road_sindex = args\n",
    "    edges = []\n",
    "    geom = row['geometry']\n",
    "    possible_matches_index = list(road_sindex.intersection(geom.bounds))\n",
    "    for road_idx in possible_matches_index:\n",
    "        road_row = roads_gdf.iloc[road_idx]\n",
    "        road_geom = road_row['geometry']\n",
    "        try:\n",
    "            if geom.buffer(1e-3).intersects(road_geom.buffer(1e-3)):\n",
    "                src = f\"neighborhood_{idx}\"\n",
    "                dst = f\"road_{road_idx}\"\n",
    "                edges.append({'src': src, 'dst': dst})\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error checking intersection between neighborhood {idx} and road {road_idx}: {e}\")\n",
    "    return edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0cdc1557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(data, force_recompute=False):\n",
    "    logging.info(\"Stage 2: Building city graph...\")\n",
    "    current_hash = compute_data_hash(data)\n",
    "    nodes_df = None\n",
    "    edges_df = None\n",
    "    node_id_to_vertex = {}\n",
    "    vertex_to_index = {}\n",
    "\n",
    "    if not force_recompute and os.path.exists(GRAPH_NODES_CACHE_PATH) and os.path.exists(GRAPH_EDGES_CACHE_PATH):\n",
    "        try:\n",
    "            with open(GRAPH_DATA_HASH_PATH, 'r') as f:\n",
    "                cached_hash = f.read()\n",
    "            if cached_hash == current_hash:\n",
    "                logging.info(\"Data hash matches cached hash. Loading graph from cache...\")\n",
    "                nodes_df = cudf.read_parquet(GRAPH_NODES_CACHE_PATH)\n",
    "                edges_df = cudf.read_parquet(GRAPH_EDGES_CACHE_PATH)\n",
    "                with open(GRAPH_NODE_ID_CACHE_PATH, 'r') as f:\n",
    "                    node_id_to_vertex = json.load(f)\n",
    "                G = cugraph.Graph()\n",
    "                G._nodes = nodes_df\n",
    "                if not edges_df.empty:\n",
    "                    G.from_cudf_edgelist(edges_df, source='src', destination='dst')\n",
    "                logging.info(f\"Loaded graph from cache: {len(nodes_df)} nodes, {len(edges_df)} edges\")\n",
    "                return G\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to load cached graph: {e}. Recomputing graph...\")\n",
    "\n",
    "    logging.info(\"Constructing graph nodes...\")\n",
    "    neighborhoods_gdf = data['neighborhoods']\n",
    "    buildings_gdf = data['buildings']\n",
    "    roads_gdf = data['roads']\n",
    "\n",
    "    # Compute area_m2 for buildings if not present\n",
    "    if 'area_m2' not in buildings_gdf.columns:\n",
    "        logging.warning(\"'area_m2' column missing in buildings_gdf. Computing from geometry...\")\n",
    "        buildings_gdf['area_m2'] = buildings_gdf['geometry'].area\n",
    "        logging.info(f\"Computed area_m2 stats:\\n{buildings_gdf['area_m2'].describe()}\")\n",
    "\n",
    "    # Create nodes with both vertex IDs and integer indices\n",
    "    logging.info(\"Adding neighborhood nodes...\")\n",
    "    neighborhood_nodes = []\n",
    "    for idx, row in tqdm(neighborhoods_gdf.iterrows(), total=len(neighborhoods_gdf), desc=\"Neighborhood nodes\"):\n",
    "        vertex = f\"neighborhood_{idx}\"\n",
    "        node_id_to_vertex[idx] = vertex\n",
    "        vertex_to_index[vertex] = idx  # Map vertex to integer index\n",
    "        neighborhood_nodes.append({\n",
    "            'index': idx,  # Integer index for GNN\n",
    "            'vertex': vertex,  # String vertex ID\n",
    "            'type': 'neighborhood',\n",
    "            'LIE_NAME': row['LIE_NAME'],\n",
    "            'ndvi_mean': row['ndvi_mean'],\n",
    "            'tree_count': row['tree_count'],\n",
    "            'transit_count': row['transit_count'],\n",
    "            'accident_count': row['accident_count'],\n",
    "            'road_density': row['road_density'],\n",
    "            'intersection_density': row['intersection_density'],\n",
    "            'total_population': row['total_population'],\n",
    "            'elderly_percentage': row['elderly_percentage'],\n",
    "            **{f'land_use_{category.lower()}_percent': row.get(f'land_use_{category.lower()}_percent', 0.0) for category in CATEGORY_PRIORITY.keys()}\n",
    "        })\n",
    "\n",
    "    logging.info(\"Adding building nodes...\")\n",
    "    building_nodes = []\n",
    "    for idx, row in tqdm(buildings_gdf.iterrows(), total=len(buildings_gdf), desc=\"Building nodes\"):\n",
    "        offset = len(neighborhoods_gdf)\n",
    "        vertex = f\"building_{idx}\"\n",
    "        node_id_to_vertex[idx + offset] = vertex\n",
    "        vertex_to_index[vertex] = idx + offset\n",
    "        building_nodes.append({\n",
    "            'index': idx + offset,\n",
    "            'vertex': vertex,\n",
    "            'type': 'building',\n",
    "            'building': row['building'] if pd.notna(row['building']) else 'unknown',\n",
    "            'area_m2': row['area_m2']\n",
    "        })\n",
    "\n",
    "    logging.info(\"Adding road nodes...\")\n",
    "    road_nodes = []\n",
    "    for idx, row in tqdm(roads_gdf.iterrows(), total=len(roads_gdf), desc=\"Road nodes\"):\n",
    "        offset = len(neighborhoods_gdf) + len(buildings_gdf)\n",
    "        vertex = f\"road_{idx}\"\n",
    "        node_id_to_vertex[idx + offset] = vertex\n",
    "        vertex_to_index[vertex] = idx + offset\n",
    "        road_nodes.append({\n",
    "            'index': idx + offset,\n",
    "            'vertex': vertex,\n",
    "            'type': 'road',\n",
    "            'class': row['class'] if pd.notna(row['class']) else 'unknown',\n",
    "            'length_m': row['length_m']\n",
    "        })\n",
    "\n",
    "    # Combine all nodes\n",
    "    nodes = neighborhood_nodes + building_nodes + road_nodes\n",
    "    nodes_df = cudf.DataFrame(nodes)\n",
    "\n",
    "    # Convert GeoDataFrames to cudf for GPU-accelerated operations\n",
    "    logging.info(\"Converting GeoDataFrames to cudf for GPU processing...\")\n",
    "    neighborhoods_cudf = cudf.from_pandas(neighborhoods_gdf.drop(columns=['geometry']))\n",
    "    buildings_cudf = cudf.from_pandas(buildings_gdf.drop(columns=['geometry']))\n",
    "    roads_cudf = cudf.from_pandas(roads_gdf.drop(columns=['geometry']))\n",
    "\n",
    "    # Extract bounding box coordinates as separate columns, ensuring scalar values\n",
    "    logging.info(\"Extracting bounding box coordinates...\")\n",
    "    # Apply fix_geometry to ensure all geometries are valid\n",
    "    neighborhoods_gdf['geometry'] = neighborhoods_gdf['geometry'].apply(fix_geometry)\n",
    "    buildings_gdf['geometry'] = buildings_gdf['geometry'].apply(fix_geometry)\n",
    "    roads_gdf['geometry'] = roads_gdf['geometry'].apply(fix_geometry)\n",
    "\n",
    "    # Extract bounds and convert to scalar floats\n",
    "    bounds_df = neighborhoods_gdf['geometry'].apply(lambda geom: pd.Series(geom.bounds, index=['min_x', 'min_y', 'max_x', 'max_y'])).astype(float).fillna(0.0)\n",
    "    neighborhoods_cudf['min_x'] = cudf.Series(bounds_df['min_x'].values, dtype='float64')\n",
    "    neighborhoods_cudf['min_y'] = cudf.Series(bounds_df['min_y'].values, dtype='float64')\n",
    "    neighborhoods_cudf['max_x'] = cudf.Series(bounds_df['max_x'].values, dtype='float64')\n",
    "    neighborhoods_cudf['max_y'] = cudf.Series(bounds_df['max_y'].values, dtype='float64')\n",
    "\n",
    "    bounds_df = buildings_gdf['geometry'].apply(lambda geom: pd.Series(geom.bounds, index=['min_x', 'min_y', 'max_x', 'max_y'])).astype(float).fillna(0.0)\n",
    "    buildings_cudf['min_x'] = cudf.Series(bounds_df['min_x'].values, dtype='float64')\n",
    "    buildings_cudf['min_y'] = cudf.Series(bounds_df['min_y'].values, dtype='float64')\n",
    "    buildings_cudf['max_x'] = cudf.Series(bounds_df['max_x'].values, dtype='float64')\n",
    "    buildings_cudf['max_y'] = cudf.Series(bounds_df['max_y'].values, dtype='float64')\n",
    "\n",
    "    bounds_df = roads_gdf['geometry'].apply(lambda geom: pd.Series(geom.bounds, index=['min_x', 'min_y', 'max_x', 'max_y'])).astype(float).fillna(0.0)\n",
    "    roads_cudf['min_x'] = cudf.Series(bounds_df['min_x'].values, dtype='float64')\n",
    "    roads_cudf['min_y'] = cudf.Series(bounds_df['min_y'].values, dtype='float64')\n",
    "    roads_cudf['max_x'] = cudf.Series(bounds_df['max_x'].values, dtype='float64')\n",
    "    roads_cudf['max_y'] = cudf.Series(bounds_df['max_y'].values, dtype='float64')\n",
    "\n",
    "    # Log the data types to debug\n",
    "    logging.info(f\"neighborhoods_cudf['min_x'] dtype: {neighborhoods_cudf['min_x'].dtype}\")\n",
    "    logging.info(f\"buildings_cudf['min_x'] dtype: {buildings_cudf['min_x'].dtype}\")\n",
    "    logging.info(f\"roads_cudf['min_x'] dtype: {roads_cudf['min_x'].dtype}\")\n",
    "\n",
    "    # Create edges using integer indices\n",
    "    logging.info(\"Creating edges using GPU-accelerated spatial joins...\")\n",
    "    edges = []\n",
    "\n",
    "    # Neighborhood-Neighborhood edges\n",
    "    logging.info(\"Computing neighborhood-neighborhood edges...\")\n",
    "    for i in tqdm(range(len(neighborhoods_cudf)), desc=\"Neighborhood-Neighborhood edges\"):\n",
    "        row = neighborhoods_cudf.iloc[i]\n",
    "        # Extract scalar bounding box coordinates and convert to Python scalar\n",
    "        geom_min_x = float(row['min_x'].values[0])\n",
    "        geom_min_y = float(row['min_y'].values[0])\n",
    "        geom_max_x = float(row['max_x'].values[0])\n",
    "        geom_max_y = float(row['max_y'].values[0])\n",
    "        # Log the type of geom_min_x for debugging\n",
    "        logging.debug(f\"geom_min_x type: {type(geom_min_x)}, value: {geom_min_x}\")\n",
    "        # Find potential matches based on bounding box overlap using direct scalar comparisons\n",
    "        matches = neighborhoods_cudf[\n",
    "            ~((geom_max_x < neighborhoods_cudf['min_x']) |\n",
    "              (geom_min_x > neighborhoods_cudf['max_x']) |\n",
    "              (geom_max_y < neighborhoods_cudf['min_y']) |\n",
    "              (geom_min_y > neighborhoods_cudf['max_y'])) &\n",
    "            (neighborhoods_cudf.index != i)\n",
    "        ]\n",
    "        for j in matches.index.values_host:\n",
    "            src_vertex = f\"neighborhood_{i}\"\n",
    "            dst_vertex = f\"neighborhood_{j}\"\n",
    "            src = vertex_to_index[src_vertex]\n",
    "            dst = vertex_to_index[dst_vertex]\n",
    "            edges.append({'src': src, 'dst': dst})\n",
    "\n",
    "    # Neighborhood-Building edges\n",
    "    logging.info(\"Computing neighborhood-building edges...\")\n",
    "    for i in tqdm(range(len(neighborhoods_cudf)), desc=\"Neighborhood-Building edges\"):\n",
    "        row = neighborhoods_cudf.iloc[i]\n",
    "        geom_min_x = float(row['min_x'].values[0])\n",
    "        geom_min_y = float(row['min_y'].values[0])\n",
    "        geom_max_x = float(row['max_x'].values[0])\n",
    "        geom_max_y = float(row['max_y'].values[0])\n",
    "        matches = buildings_cudf[\n",
    "            ~((geom_max_x < buildings_cudf['min_x']) |\n",
    "              (geom_min_x > buildings_cudf['max_x']) |\n",
    "              (geom_max_y < buildings_cudf['min_y']) |\n",
    "              (geom_min_y > buildings_cudf['max_y']))\n",
    "        ]\n",
    "        for j in matches.index.values_host:\n",
    "            src_vertex = f\"neighborhood_{i}\"\n",
    "            dst_vertex = f\"building_{j}\"\n",
    "            src = vertex_to_index[src_vertex]\n",
    "            dst = vertex_to_index[dst_vertex]\n",
    "            edges.append({'src': src, 'dst': dst})\n",
    "\n",
    "    # Neighborhood-Road edges\n",
    "    logging.info(\"Computing neighborhood-road edges...\")\n",
    "    for i in tqdm(range(len(neighborhoods_cudf)), desc=\"Neighborhood-Road edges\"):\n",
    "        row = neighborhoods_cudf.iloc[i]\n",
    "        geom_min_x = float(row['min_x'].values[0])\n",
    "        geom_min_y = float(row['min_y'].values[0])\n",
    "        geom_max_x = float(row['max_x'].values[0])\n",
    "        geom_max_y = float(row['max_y'].values[0])\n",
    "        matches = roads_cudf[\n",
    "            ~((geom_max_x < roads_cudf['min_x']) |\n",
    "              (geom_min_x > roads_cudf['max_x']) |\n",
    "              (geom_max_y < roads_cudf['min_y']) |\n",
    "              (geom_min_y > roads_cudf['max_y']))\n",
    "        ]\n",
    "        for j in matches.index.values_host:\n",
    "            src_vertex = f\"neighborhood_{i}\"\n",
    "            dst_vertex = f\"road_{j}\"\n",
    "            src = vertex_to_index[src_vertex]\n",
    "            dst = vertex_to_index[dst_vertex]\n",
    "            edges.append({'src': src, 'dst': dst})\n",
    "\n",
    "    edges_df = cudf.DataFrame(edges)\n",
    "    logging.info(f\"Created {len(edges_df)} total edges\")\n",
    "\n",
    "    # Validate edges\n",
    "    valid_indices = set(nodes_df['index'].to_pandas())\n",
    "    if edges_df.empty:\n",
    "        logging.warning(\"No edges created. Graph will have nodes but no edges.\")\n",
    "    else:\n",
    "        edges_df = edges_df[edges_df['src'].isin(valid_indices) & edges_df['dst'].isin(valid_indices)]\n",
    "        logging.info(f\"After validation, {len(edges_df)} edges remain\")\n",
    "        if not edges_df.empty:\n",
    "            logging.info(f\"Sample edges after validation:\\n{edges_df.head().to_pandas()}\")\n",
    "\n",
    "    # Create the graph\n",
    "    G = cugraph.Graph()\n",
    "    G._nodes = nodes_df\n",
    "    if not edges_df.empty:\n",
    "        G.from_cudf_edgelist(edges_df, source='src', destination='dst')\n",
    "    else:\n",
    "        logging.warning(\"No valid edges created. Graph will have nodes but no edges.\")\n",
    "\n",
    "    # Save graph data to cache\n",
    "    logging.info(\"Saving graph data to cache...\")\n",
    "    try:\n",
    "        nodes_df.to_parquet(GRAPH_NODES_CACHE_PATH)\n",
    "        edges_df.to_parquet(GRAPH_EDGES_CACHE_PATH)\n",
    "        with open(GRAPH_DATA_HASH_PATH, 'w') as f:\n",
    "            f.write(current_hash)\n",
    "        with open(GRAPH_NODE_ID_CACHE_PATH, 'w') as f:\n",
    "            json.dump(node_id_to_vertex, f)\n",
    "        logging.info(\"Successfully saved graph data to cache.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save graph data to cache: {e}\")\n",
    "\n",
    "    logging.info(f\"City graph constructed: {len(nodes_df)} nodes, {len(edges_df)} edges\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14e080",
   "metadata": {},
   "source": [
    "Cell 7: Rule-Based Walkability Scores (compute_walkability_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3ef4aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_walkability_scores(G, data):\n",
    "    \"\"\"\n",
    "    Compute walkability scores for neighborhood nodes in the graph and assign them.\n",
    "    \n",
    "    Args:\n",
    "        G (cugraph.Graph): The city graph with nodes and edges.\n",
    "        data (dict): Dictionary containing roads and other datasets.\n",
    "    \n",
    "    Returns:\n",
    "        cugraph.Graph: Updated graph with walkability scores assigned to neighborhood nodes.\n",
    "    \"\"\"\n",
    "    logging.info(\"Computing walkability scores for neighborhoods...\")\n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    \n",
    "    walkability_components = compute_walkability_components_all(data['neighborhoods'], data)\n",
    "    \n",
    "    logging.info(f\"Number of neighborhood nodes in nodes_df: {len(nodes_df[nodes_df['type'] == 'neighborhood'])}\")\n",
    "    logging.info(f\"Number of entries in walkability_components: {len(walkability_components)}\")\n",
    "    logging.info(f\"Sample LIE_NAME in nodes_df: {nodes_df[nodes_df['type'] == 'neighborhood']['LIE_NAME'].head().tolist()}\")\n",
    "    logging.info(f\"Sample LIE_NAME in walkability_components: {walkability_components['LIE_NAME'].head().tolist()}\")\n",
    "    \n",
    "    nodes_df = nodes_df.merge(\n",
    "        walkability_components[['LIE_NAME', 'walkability_score', 'walkability_category']],\n",
    "        on='LIE_NAME',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    unmatched = nodes_df[(nodes_df['type'] == 'neighborhood') & (nodes_df['walkability_score'].isna())]\n",
    "    if len(unmatched) > 0:\n",
    "        logging.warning(f\"Found {len(unmatched)} neighborhood nodes without walkability scores. Filling with 0.\")\n",
    "        nodes_df.loc[nodes_df['type'] == 'neighborhood', 'walkability_score'] = nodes_df['walkability_score'].fillna(0)\n",
    "        nodes_df.loc[nodes_df['type'] == 'neighborhood', 'walkability_category'] = nodes_df['walkability_category'].fillna('low')\n",
    "    \n",
    "    G._nodes = cudf.from_pandas(nodes_df)\n",
    "    \n",
    "    logging.info(\"Finished computing walkability scores.\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989dbf84",
   "metadata": {},
   "source": [
    "Cell 8 prepare_gnn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8a057108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_gnn_data(G):\n",
    "    logging.info(\"Preparing data for GNN training...\")\n",
    "    nodes_df = G._nodes\n",
    "    edges_df = G.edgelist.edgelist_df if G.edgelist else cudf.DataFrame()\n",
    "\n",
    "    numerical_features = [\n",
    "        'ndvi_mean', 'tree_count', 'transit_count', 'accident_count', \n",
    "        'road_density', 'intersection_density', 'total_population', \n",
    "        'elderly_percentage', 'area_m2', 'length_m', 'avg_road_accident_density', \n",
    "        'pedestrian_road_density'\n",
    "    ] + [f'land_use_{cat.lower()}_percent' for cat in CATEGORY_PRIORITY.keys()]\n",
    "\n",
    "    building_types = nodes_df[nodes_df['type'] == 'building']['building'].to_pandas().unique()\n",
    "    road_classes = nodes_df[nodes_df['type'] == 'road']['class'].to_pandas().unique()\n",
    "    categorical_features = (\n",
    "        [f'building_{bt}' for bt in building_types if pd.notna(bt)] +\n",
    "        [f'road_class_{rc}' for rc in road_classes if pd.notna(rc)]\n",
    "    )\n",
    "\n",
    "    all_features = numerical_features + categorical_features\n",
    "\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    node_types = []\n",
    "\n",
    "    for node_type in tqdm(['neighborhood', 'building', 'road'], desc=\"Normalizing features by node type\"):\n",
    "        subset = nodes_df[nodes_df['type'] == node_type].to_pandas()\n",
    "        if subset.empty:\n",
    "            logging.warning(f\"No nodes of type {node_type} found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        subset_features = pd.DataFrame(0.0, index=subset.index, columns=all_features)\n",
    "\n",
    "        if node_type == 'neighborhood':\n",
    "            for col in numerical_features:\n",
    "                if col in subset.columns:\n",
    "                    subset_features[col] = subset[col].astype(float).fillna(0)\n",
    "                else:\n",
    "                    logging.warning(f\"Column {col} missing in neighborhood nodes. Setting to 0.\")\n",
    "        elif node_type == 'building':\n",
    "            if 'area_m2' in subset.columns:\n",
    "                subset_features['area_m2'] = subset['area_m2'].astype(float).fillna(0)\n",
    "        else:  # road\n",
    "            if 'length_m' in subset.columns:\n",
    "                subset_features['length_m'] = subset['length_m'].astype(float).fillna(0)\n",
    "\n",
    "        if node_type == 'building':\n",
    "            for bt in building_types:\n",
    "                if pd.notna(bt):\n",
    "                    subset_features[f'building_{bt}'] = (subset['building'] == bt).astype(float)\n",
    "        elif node_type == 'road':\n",
    "            for rc in road_classes:\n",
    "                if pd.notna(rc):\n",
    "                    subset_features[f'road_class_{rc}'] = (subset['class'] == rc).astype(float)\n",
    "\n",
    "        # Z-score normalization for numerical features\n",
    "        for col in numerical_features:\n",
    "            if col in subset_features.columns and subset_features[col].std() > 0:\n",
    "                subset_features[col] = (\n",
    "                    (subset_features[col] - subset_features[col].mean()) / subset_features[col].std()\n",
    "                ).fillna(0)\n",
    "            else:\n",
    "                logging.debug(f\"Column {col} has zero variance or is missing for {node_type}. Setting to 0.\")\n",
    "\n",
    "        logging.info(f\"Node type {node_type}: {len(subset)} nodes, feature shape: {subset_features.shape}\")\n",
    "\n",
    "        features_list.append(subset_features.values)\n",
    "\n",
    "        if node_type == 'neighborhood':\n",
    "            labels = subset['walkability_score'].astype(float).fillna(0).values\n",
    "            labels_list.append(labels[:, None])  # Shape [n, 1]\n",
    "        else:\n",
    "            labels_list.append(np.zeros((len(subset), 1)))\n",
    "\n",
    "        node_types.extend([node_type] * len(subset))\n",
    "\n",
    "    try:\n",
    "        features = np.vstack(features_list)\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Failed to stack features: {e}\")\n",
    "        raise\n",
    "\n",
    "    labels = np.vstack(labels_list)\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "    if not edges_df.empty:\n",
    "        edge_index = torch.tensor(edges_df[['src', 'dst']].to_pandas().values.T, dtype=torch.long)\n",
    "        logging.info(f\"Edge index created with {edge_index.shape[1]} edges\")\n",
    "        max_index = nodes_df['index'].max()\n",
    "        if edge_index.max() > max_index or edge_index.min() < 0:\n",
    "            logging.warning(f\"Edge indices out of bounds: min={edge_index.min()}, max={edge_index.max()}, expected max={max_index}\")\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "    else:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        logging.warning(\"No edges found in graph.\")\n",
    "\n",
    "    data = Data(\n",
    "        x=features_tensor,\n",
    "        edge_index=edge_index,\n",
    "        y=labels_tensor\n",
    "    )\n",
    "\n",
    "    data.node_types = node_types\n",
    "\n",
    "    logging.info(f\"Prepared GNN data: {features_tensor.shape[0]} nodes, {edge_index.shape[1]} edges\")\n",
    "    logging.info(f\"Feature matrix shape: {features_tensor.shape}\")\n",
    "    logging.info(f\"Label tensor shape: {labels_tensor.shape}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a9a6b5",
   "metadata": {},
   "source": [
    "Cell 9: WalkabilityGNN, train_gnn_model, predict_walkability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "635ecc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNWalkabilityPredictor(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(GNNWalkabilityPredictor, self).__init__()\n",
    "        self.conv1 = GATConv(num_features, 64, heads=2, concat=True)\n",
    "        self.bn1 = BatchNorm(64 * 2)\n",
    "        self.conv2 = GATConv(64 * 2, 32, heads=1, concat=True)\n",
    "        self.bn2 = BatchNorm(32)\n",
    "        self.fc1 = torch.nn.Linear(32, 16)\n",
    "        self.fc2 = torch.nn.Linear(16, 1)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        if edge_index.numel() > 0:\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = self.bn1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            x = self.bn2(x)\n",
    "            x = F.relu(x)\n",
    "        else:\n",
    "            logging.warning(\"No edges in the graph. Using linear layer for node features only.\")\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "def train_gnn_model(data_gnn):\n",
    "    logging.info(\"Stage 4: Training GNN model...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_gnn = data_gnn.to(device)\n",
    "    \n",
    "    neighborhood_mask = np.array([t == 'neighborhood' for t in data_gnn.node_types])\n",
    "    train_indices = np.where(neighborhood_mask)[0]\n",
    "    \n",
    "    if len(train_indices) == 0:\n",
    "        logging.error(\"No neighborhood nodes found for training.\")\n",
    "        raise ValueError(\"No neighborhood nodes found for training.\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    train_idx = np.random.choice(train_indices, size=int(0.8 * len(train_indices)), replace=False)\n",
    "    val_idx = np.setdiff1d(train_indices, train_idx)\n",
    "    \n",
    "    train_mask = torch.zeros(data_gnn.num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(data_gnn.num_nodes, dtype=torch.bool)\n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    data_gnn.train_mask = train_mask\n",
    "    data_gnn.val_mask = val_mask\n",
    "    \n",
    "    neighborhood_labels = data_gnn.y[neighborhood_mask].cpu().numpy()\n",
    "    logging.info(f\"Target (walkability_score) distribution for neighborhood nodes:\\n{pd.Series(neighborhood_labels.flatten()).describe()}\")\n",
    "    \n",
    "    model = GNNWalkabilityPredictor(num_features=data_gnn.x.shape[1]).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = os.path.join(CHECKPOINT_DIR, 'best_gnn_model.pth')\n",
    "    epochs = 300\n",
    "    patience = 20\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training epochs\"):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data_gnn)\n",
    "        loss = criterion(out[data_gnn.train_mask], data_gnn.y[data_gnn.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(data_gnn)\n",
    "            val_loss = criterion(val_out[data_gnn.val_mask], data_gnn.y[data_gnn.val_mask])\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            logging.info(f\"Epoch {epoch}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                logging.info(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    logging.info(\"Finished training GNN model.\")\n",
    "    return model\n",
    "\n",
    "def predict_walkability(G, model):\n",
    "    \"\"\"\n",
    "    Predict walkability scores using the trained GNN model.\n",
    "    \n",
    "    Args:\n",
    "        G (cugraph.Graph): The city graph with nodes and edges.\n",
    "        model (GNNWalkabilityPredictor): The trained GNN model.\n",
    "    \n",
    "    Returns:\n",
    "        cugraph.Graph: Updated graph with GNN-predicted walkability scores.\n",
    "    \"\"\"\n",
    "    logging.info(\"Predicting walkability with GNN...\")\n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    \n",
    "    data = prepare_gnn_data(G)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    data = data.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(data).squeeze()\n",
    "    \n",
    "    predictions = predictions.cpu().numpy()\n",
    "    predictions = np.clip(predictions, 0, 1)\n",
    "    \n",
    "    nodes_df['walkability_gnn'] = np.nan\n",
    "    nodes_df.loc[nodes_df['type'] == 'neighborhood', 'walkability_gnn'] = predictions[nodes_df['type'] == 'neighborhood']\n",
    "    \n",
    "    G._nodes = cudf.from_pandas(nodes_df)\n",
    "    logging.info(f\"Walkability GNN stats after prediction:\\n{nodes_df[nodes_df['type'] == 'neighborhood']['walkability_gnn'].describe()}\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5406821f",
   "metadata": {},
   "source": [
    "Cell 10: Interactive Map Generation (create_interactive_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9d059c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_map(G, data):\n",
    "    \"\"\"Generate an interactive Kepler.gl map to visualize walkability scores.\"\"\"\n",
    "    logging.info(\"Generating interactive Kepler.gl map...\")\n",
    "    \n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    neighborhoods_gdf = data['neighborhoods'].copy()\n",
    "\n",
    "    # Standardize LIE_NAME for merging\n",
    "    nodes_df['LIE_NAME'] = nodes_df['LIE_NAME'].astype(str).str.strip()\n",
    "    neighborhoods_gdf['LIE_NAME'] = neighborhoods_gdf['LIE_NAME'].astype(str).str.strip()\n",
    "\n",
    "    # Filter for neighborhood nodes and select necessary columns\n",
    "    neighborhood_nodes = nodes_df[nodes_df['type'] == 'neighborhood'][['LIE_NAME', 'walkability_score', 'walkability_gnn', 'walkability_category']]\n",
    "\n",
    "    # Log for debugging\n",
    "    nodes_lie_names = set(neighborhood_nodes['LIE_NAME'])\n",
    "    gdf_lie_names = set(neighborhoods_gdf['LIE_NAME'])\n",
    "    logging.info(f\"Neighborhood nodes count: {len(neighborhood_nodes)}\")\n",
    "    logging.info(f\"Neighborhoods_gdf count: {len(neighborhoods_gdf)}\")\n",
    "    logging.info(f\"Sample LIE_NAME in nodes_df: {list(nodes_lie_names)[:5]}\")\n",
    "    logging.info(f\"Sample LIE_NAME in neighborhoods_gdf: {list(gdf_lie_names)[:5]}\")\n",
    "    logging.info(f\"Common LIE_NAMEs: {len(nodes_lie_names & gdf_lie_names)}\")\n",
    "    logging.info(f\"Nodes LIE_NAMEs not in GDF: {list(nodes_lie_names - gdf_lie_names)}\")\n",
    "    logging.info(f\"GDF LIE_NAMEs not in nodes: {list(gdf_lie_names - nodes_lie_names)}\")\n",
    "    logging.info(f\"Nodes nulls: {neighborhood_nodes.isna().sum().to_dict()}\")\n",
    "    logging.info(f\"GDF geometry nulls: {neighborhoods_gdf['geometry'].isna().sum()}\")\n",
    "\n",
    "    # Merge data\n",
    "    map_data = neighborhoods_gdf[['LIE_NAME', 'geometry']].merge(\n",
    "        neighborhood_nodes,\n",
    "        on='LIE_NAME',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Drop duplicates in-place\n",
    "    map_data.drop_duplicates(subset=['LIE_NAME'], keep='first', inplace=True)\n",
    "\n",
    "    # Log merge results and score distributions\n",
    "    logging.info(f\"Merged map_data rows: {len(map_data)}\")\n",
    "    logging.info(f\"Walkability score nulls: {map_data['walkability_score'].isna().sum()}\")\n",
    "    logging.info(f\"Walkability GNN nulls: {map_data['walkability_gnn'].isna().sum()}\")\n",
    "    logging.info(f\"Walkability score distribution in map_data:\\n{map_data['walkability_score'].describe()}\")\n",
    "    logging.info(f\"Walkability GNN distribution in map_data:\\n{map_data['walkability_gnn'].describe()}\")\n",
    "    logging.info(f\"Walkability category distribution in map_data:\\n{map_data['walkability_category'].value_counts()}\")\n",
    "\n",
    "    # Fill NaN values\n",
    "    map_data['walkability_score'] = map_data['walkability_score'].fillna(0)\n",
    "    map_data['walkability_gnn'] = map_data['walkability_gnn'].fillna(0)\n",
    "    map_data['walkability_category'] = map_data['walkability_category'].fillna('low')\n",
    "\n",
    "    # Convert to GeoDataFrame and transform CRS\n",
    "    map_data = gpd.GeoDataFrame(map_data, geometry='geometry', crs='EPSG:3826')\n",
    "    map_data['geometry'] = map_data['geometry'].to_crs('EPSG:4326')\n",
    "    \n",
    "    # Prepare data for Kepler.gl\n",
    "    kepler_data = {\n",
    "        'neighborhoods': map_data[['LIE_NAME', 'walkability_score', 'walkability_gnn', 'walkability_category', 'geometry']].to_json()\n",
    "    }\n",
    "\n",
    "    # Kepler.gl configuration\n",
    "    config = {\n",
    "        \"version\": \"v1\",\n",
    "        \"config\": {\n",
    "            \"visState\": {\n",
    "                \"layers\": [\n",
    "                    {\n",
    "                        \"id\": \"neighborhoods\",\n",
    "                        \"type\": \"geojson\",\n",
    "                        \"config\": {\n",
    "                            \"dataId\": \"neighborhoods\",\n",
    "                            \"label\": \"Neighborhoods\",\n",
    "                            \"color\": [18, 147, 154],\n",
    "                            \"columns\": {\n",
    "                                \"geojson\": \"geometry\"\n",
    "                            },\n",
    "                            \"isVisible\": True,\n",
    "                            \"visConfig\": {\n",
    "                                \"opacity\": 0.7,\n",
    "                                \"strokeOpacity\": 0.9,\n",
    "                                \"thickness\": 1,\n",
    "                                \"strokeColor\": [255, 255, 255],\n",
    "                                \"colorRange\": {\n",
    "                                    \"name\": \"Global Warming\",\n",
    "                                    \"type\": \"sequential\",\n",
    "                                    \"colors\": [\n",
    "                                        \"#5A1846\", \"#900C3F\", \"#C70039\",\n",
    "                                        \"#E3611C\", \"#F1920E\", \"#FFC107\"\n",
    "                                    ]\n",
    "                                },\n",
    "                                \"strokeColorRange\": {\n",
    "                                    \"name\": \"Global Warming\",\n",
    "                                    \"type\": \"sequential\",\n",
    "                                    \"colors\": [\n",
    "                                        \"#5A1846\", \"#900C3F\", \"#C70039\",\n",
    "                                        \"#E3611C\", \"#F1920E\", \"#FFC107\"\n",
    "                                    ]\n",
    "                                },\n",
    "                                \"colorField\": {\n",
    "                                    \"name\": \"walkability_gnn\",\n",
    "                                    \"type\": \"real\"\n",
    "                                },\n",
    "                                \"colorScale\": \"quantile\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"visualChannels\": {\n",
    "                            \"colorField\": {\n",
    "                                \"name\": \"walkability_gnn\",\n",
    "                                \"type\": \"real\"\n",
    "                            },\n",
    "                            \"colorScale\": \"quantile\"\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"interactionConfig\": {\n",
    "                    \"tooltip\": {\n",
    "                        \"fieldsToShow\": {\n",
    "                            \"neighborhoods\": [\n",
    "                                {\"name\": \"LIE_NAME\", \"format\": None},\n",
    "                                {\"name\": \"walkability_score\", \"format\": \"{:.3f}\"},\n",
    "                                {\"name\": \"walkability_gnn\", \"format\": \"{:.3f}\"},\n",
    "                                {\"name\": \"walkability_category\", \"format\": None}\n",
    "                            ]\n",
    "                        },\n",
    "                        \"enabled\": True\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"mapState\": {\n",
    "                \"latitude\": 25.0330,\n",
    "                \"longitude\": 121.5654,\n",
    "                \"zoom\": 11\n",
    "            },\n",
    "            \"mapStyle\": {\n",
    "                \"styleType\": \"dark\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    map_1 = KeplerGl(height=800, data=kepler_data, config=config)\n",
    "    map_path = os.path.join(BASE_DIR, 'taipei_walkability_map.html')\n",
    "    map_1.save_to_html(file_name=map_path)\n",
    "    logging.info(f\"Interactive map generated and saved as {map_path}\")\n",
    "    print(f\"Map saved to {map_path}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0683d",
   "metadata": {},
   "source": [
    "Cell 11: Main Execution (main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f13b11d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 13:29:53,092 - INFO - Ensured subgraph directory exists: /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/subgraphs\n",
      "2025-04-23 13:29:53,093 - INFO - Stage 1: Loading and preparing data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting load_and_prepare_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:   0%|          | 0/8 [00:00<?, ?it/s]2025-04-23 13:29:53,208 - INFO - Loaded neighborhoods with shape (456, 57)\n",
      "Loading files:  12%|█▎        | 1/8 [00:00<00:00,  8.77it/s]2025-04-23 13:29:54,541 - INFO - Loaded buildings with shape (74306, 9)\n",
      "Loading files:  25%|██▌       | 2/8 [00:01<00:04,  1.20it/s]2025-04-23 13:29:54,602 - INFO - Loaded roads with shape (81444, 2)\n",
      "2025-04-23 13:29:54,644 - INFO - Loaded trees with shape (5019, 12)\n",
      "Loading files:  50%|█████     | 4/8 [00:01<00:01,  2.92it/s]2025-04-23 13:29:54,745 - INFO - Loaded transit with shape (29892, 11)\n",
      "2025-04-23 13:29:55,516 - INFO - Loaded urban_masterplan with shape (15521, 15)\n",
      "Loading files:  75%|███████▌  | 6/8 [00:02<00:00,  2.59it/s]2025-04-23 13:29:56,309 - INFO - Loaded accidents with shape (56133, 8)\n",
      "Loading files:  88%|████████▊ | 7/8 [00:03<00:00,  2.04it/s]2025-04-23 13:29:56,313 - INFO - Columns in population_df after loading: ['LIE_NAME', 'Total_Population', 'Elderly_Percentage']\n",
      "2025-04-23 13:29:56,313 - INFO - Loaded population with shape (456, 3)\n",
      "Loading files: 100%|██████████| 8/8 [00:03<00:00,  2.48it/s]\n",
      "2025-04-23 13:29:56,314 - INFO - Columns in neighborhoods_gdf after loading: ['LIE_NAME', 'SECT_NAME', '2024population', 'land_use_city_open_area_count', 'land_use_city_open_area_area_m2', 'land_use_city_open_area_percent', 'land_use_commercial_count', 'land_use_commercial_area_m2', 'land_use_commercial_percent', 'land_use_infrastructure_count', 'land_use_infrastructure_area_m2', 'land_use_infrastructure_percent', 'land_use_government_count', 'land_use_government_area_m2', 'land_use_government_percent', 'land_use_public_transportation_count', 'land_use_public_transportation_area_m2', 'land_use_public_transportation_percent', 'land_use_education_count', 'land_use_education_area_m2', 'land_use_education_percent', 'land_use_medical_count', 'land_use_medical_area_m2', 'land_use_medical_percent', 'land_use_amenity_count', 'land_use_amenity_area_m2', 'land_use_amenity_percent', 'land_use_road_count', 'land_use_road_area_m2', 'land_use_road_percent', 'land_use_pedestrian_count', 'land_use_pedestrian_area_m2', 'land_use_pedestrian_percent', 'land_use_natural_count', 'land_use_natural_area_m2', 'land_use_natural_percent', 'land_use_special_zone_count', 'land_use_special_zone_area_m2', 'land_use_special_zone_percent', 'land_use_river_count', 'land_use_river_area_m2', 'land_use_river_percent', 'land_use_military_count', 'land_use_military_area_m2', 'land_use_military_percent', 'land_use_residential_count', 'land_use_residential_area_m2', 'land_use_residential_percent', 'land_use_industrial_count', 'land_use_industrial_area_m2', 'land_use_industrial_percent', 'land_use_agriculture_count', 'land_use_agriculture_area_m2', 'land_use_agriculture_percent', 'ndvi_mean', 'ndvi_median', 'geometry']\n",
      "2025-04-23 13:29:56,435 - INFO - Converted buildings to CRS EPSG:3826\n",
      "2025-04-23 13:29:56,468 - INFO - Converted trees to CRS EPSG:3826\n",
      "2025-04-23 13:29:56,500 - INFO - Converted transit to CRS EPSG:3826\n",
      "2025-04-23 13:29:56,576 - INFO - Converted urban_masterplan to CRS EPSG:3826\n",
      "2025-04-23 13:29:56,606 - INFO - Converted accidents to CRS EPSG:3826\n",
      "2025-04-23 13:29:58,976 - INFO - Computing intersections for neighborhoods...\n",
      "2025-04-23 13:29:58,977 - INFO - Columns in roads_gdf after loading: ['class', 'geometry']\n",
      "2025-04-23 13:29:58,977 - INFO - Extracting endpoints from road segments...\n",
      "Extracting endpoints: 100%|██████████| 81444/81444 [00:04<00:00, 18387.70it/s]\n",
      "2025-04-23 13:30:03,556 - INFO - Building endpoint-to-road mapping...\n",
      "Building endpoint-to-road mapping: 100%|██████████| 162888/162888 [00:04<00:00, 33201.70it/s]\n",
      "2025-04-23 13:30:08,464 - INFO - Identifying intersections...\n",
      "Identifying intersections: 100%|██████████| 101237/101237 [00:00<00:00, 1265260.09it/s]\n",
      "2025-04-23 13:30:08,562 - INFO - Counting intersections per neighborhood...\n",
      "2025-04-23 13:30:08,612 - WARNING - 'area_km2' column missing in neighborhoods_gdf. Computing from geometry...\n",
      "2025-04-23 13:30:08,614 - INFO - Computed area_km2 stats:\n",
      "count    456.000000\n",
      "mean       0.588925\n",
      "std        1.351428\n",
      "min        0.031744\n",
      "25%        0.134566\n",
      "50%        0.209650\n",
      "75%        0.425264\n",
      "max       16.324434\n",
      "Name: area_km2, dtype: float64\n",
      "2025-04-23 13:30:08,617 - INFO - Intersection count stats:\n",
      "count    456.000000\n",
      "mean      33.800439\n",
      "std       24.763978\n",
      "min        1.000000\n",
      "25%       16.000000\n",
      "50%       26.000000\n",
      "75%       46.000000\n",
      "max      185.000000\n",
      "Name: intersection_count, dtype: float64\n",
      "2025-04-23 13:30:08,618 - INFO - Intersection density stats:\n",
      "count    456.000000\n",
      "mean     126.190854\n",
      "std       83.149602\n",
      "min        2.960154\n",
      "25%       66.874411\n",
      "50%      111.292996\n",
      "75%      165.079134\n",
      "max      490.471026\n",
      "Name: intersection_density, dtype: float64\n",
      "2025-04-23 13:30:08,632 - INFO - Saved neighborhoods with intersections to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/neighborhoods_with_intersections.geoparquet\n",
      "2025-04-23 13:30:08,633 - INFO - Computing tree count per neighborhood...\n",
      "2025-04-23 13:30:08,674 - INFO - Computing transit count per neighborhood...\n",
      "2025-04-23 13:30:08,784 - INFO - Computing accident count per neighborhood...\n",
      "2025-04-23 13:30:09,635 - INFO - Computing road density per neighborhood...\n",
      "2025-04-23 13:30:09,636 - INFO - Columns in roads_gdf before computing road density: ['class', 'geometry']\n",
      "2025-04-23 13:30:09,636 - WARNING - 'length_m' column missing in roads_gdf. Computing from geometry...\n",
      "2025-04-23 13:30:09,643 - INFO - Computed length_m stats:\n",
      "count     81444.000000\n",
      "mean        145.622456\n",
      "std        2304.902398\n",
      "min           0.030284\n",
      "25%          28.160770\n",
      "50%          61.698697\n",
      "75%         130.534001\n",
      "max      426414.891763\n",
      "Name: length_m, dtype: float64\n",
      "2025-04-23 13:30:09,999 - INFO - Road density stats:\n",
      "count     456.000000\n",
      "mean      158.024830\n",
      "std       171.789340\n",
      "min         7.561174\n",
      "25%        55.500294\n",
      "50%        96.985664\n",
      "75%       214.942848\n",
      "max      1657.324822\n",
      "Name: road_density, dtype: float64\n",
      "2025-04-23 13:30:10,000 - INFO - Merging population data...\n",
      "2025-04-23 13:30:10,002 - WARNING - Expected columns ['total_population', 'elderly_percentage'] not found in population_df. Attempting to find alternatives...\n",
      "2025-04-23 13:30:10,002 - INFO - Found alternative for total_population: Total_Population\n",
      "2025-04-23 13:30:10,003 - INFO - Found alternative for elderly_percentage: Elderly_Percentage\n",
      "2025-04-23 13:30:10,006 - INFO - Computing land use percentages for neighborhoods...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Percentage Calculation Process ---\n",
      "\n",
      "Neighborhood: 板溪里 (Index: 373)\n",
      "Total unique master plan area: 63777.59 m²\n",
      "Area of City_Open_Area (priority 10): 478.13 m²\n",
      "Area of Education (priority 6): 8173.36 m²\n",
      "Area of Commercial (priority 4): 23017.42 m²\n",
      "Area of Residential (priority 3): 32108.69 m²\n",
      "\n",
      "Percentages:\n",
      "City_Open_Area: 0.75%\n",
      "Education: 12.82%\n",
      "Commercial: 36.09%\n",
      "Residential: 50.34%\n",
      "Sum of percentages: 100.00%\n",
      "\n",
      "Neighborhood: 芝山里 (Index: 39)\n",
      "Total unique master plan area: 1061285.95 m²\n",
      "Area of Education (priority 6): 80442.15 m²\n",
      "Area of Residential (priority 3): 196204.41 m²\n",
      "Area of Natural (priority 2): 775753.11 m²\n",
      "Area of River (priority 1): 2754.93 m²\n",
      "Area of Government (priority 1): 6131.35 m²\n",
      "\n",
      "Percentages:\n",
      "Education: 7.58%\n",
      "Residential: 18.49%\n",
      "Natural: 73.10%\n",
      "River: 0.26%\n",
      "Government: 0.58%\n",
      "Sum of percentages: 100.00%\n",
      "\n",
      "Neighborhood: 和平里 (Index: 340)\n",
      "Total unique master plan area: 98073.11 m²\n",
      "Area of City_Open_Area (priority 10): 4608.53 m²\n",
      "Area of Public_Transportation (priority 8): 5304.93 m²\n",
      "Area of Commercial (priority 4): 15463.83 m²\n",
      "Area of Residential (priority 3): 50273.32 m²\n",
      "Area of Special_Zone (priority 1): 22422.50 m²\n",
      "\n",
      "Percentages:\n",
      "City_Open_Area: 4.70%\n",
      "Public_Transportation: 5.41%\n",
      "Commercial: 15.77%\n",
      "Residential: 51.26%\n",
      "Special_Zone: 22.86%\n",
      "Sum of percentages: 100.00%\n",
      "--- End of Percentage Calculation Process ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 13:30:10,667 - WARNING - Topology error for category Residential in neighborhood 陽明里: TopologyException: found non-noded intersection between LINESTRING (305235 2.7814e+06, 305232 2.7814e+06) and LINESTRING (305233 2.7814e+06, 305238 2.7814e+06) at 305235.20948094811 2781398.7662400398\n",
      "2025-04-23 13:30:10,929 - WARNING - Topology error for category Natural in neighborhood 陽明里: TopologyException: found non-noded intersection between LINESTRING (305351 2.78185e+06, 305350 2.78185e+06) and LINESTRING (305350 2.78185e+06, 305350 2.78185e+06) at 305349.93489947024 2781846.2793679931\n",
      "2025-04-23 13:30:11,814 - WARNING - Topology error for category Agriculture in neighborhood 豐年里: TopologyException: found non-noded intersection between LINESTRING (299444 2.78096e+06, 299444 2.78096e+06) and LINESTRING (299444 2.78096e+06, 299444 2.78096e+06) at 299443.97226843517 2780959.3288864125\n",
      "2025-04-23 13:30:12,531 - WARNING - Topology error for category Pedestrian in neighborhood 關渡里: TopologyException: found non-noded intersection between LINESTRING (296871 2.77919e+06, 296870 2.77919e+06) and LINESTRING (296871 2.77919e+06, 296869 2.77919e+06) at 296869.99175471725 2779190.4644867191\n",
      "2025-04-23 13:30:12,658 - WARNING - Topology error for category Infrastructure in neighborhood 關渡里: TopologyException: found non-noded intersection between LINESTRING (296911 2.77884e+06, 296911 2.77884e+06) and LINESTRING (296911 2.77884e+06, 296911 2.77884e+06) at 296911.44015440479 2778839.5138171767\n",
      "2025-04-23 13:30:15,213 - WARNING - Topology error for category Government in neighborhood 福順里: TopologyException: found non-noded intersection between LINESTRING (301533 2.77502e+06, 301533 2.77502e+06) and LINESTRING (301533 2.77502e+06, 301533 2.77502e+06) at 301532.96899694926 2775020.80209503\n",
      "2025-04-23 13:30:16,585 - WARNING - Topology error for category River in neighborhood 五分里: TopologyException: found non-noded intersection between LINESTRING (312333 2.77328e+06, 312333 2.77328e+06) and LINESTRING (312332 2.77328e+06, 312333 2.77328e+06) at 312332.69765419257 2773277.367184672\n",
      "2025-04-23 13:30:17,546 - WARNING - Topology error for category Road in neighborhood 南港里: TopologyException: found non-noded intersection between LINESTRING (311644 2.77189e+06, 311657 2.7719e+06) and LINESTRING (311630 2.77189e+06, 311657 2.7719e+06) at 311657.42832653591 2771895.1006993004\n",
      "2025-04-23 13:30:18,240 - WARNING - Topology error for category Special_Zone in neighborhood 中南里: TopologyException: found non-noded intersection between LINESTRING (311955 2.77135e+06, 311955 2.77135e+06) and LINESTRING (311955 2.77135e+06, 311955 2.77135e+06) at 311955.22635489638 2771352.7457217239\n",
      "2025-04-23 13:30:18,256 - WARNING - Topology error for category Road in neighborhood 中南里: TopologyException: found non-noded intersection between LINESTRING (311345 2.77161e+06, 311344 2.77161e+06) and LINESTRING (311344 2.77161e+06, 311344 2.77161e+06) at 311344.20670072001 2771612.8331892812\n",
      "2025-04-23 13:30:18,266 - WARNING - Topology error for category Infrastructure in neighborhood 中南里: TopologyException: found non-noded intersection between LINESTRING (311955 2.77135e+06, 311955 2.77135e+06) and LINESTRING (311955 2.77135e+06, 311955 2.77135e+06) at 311955.22635489638 2771352.7457217239\n",
      "2025-04-23 13:30:18,289 - WARNING - Topology error for category Government in neighborhood 中南里: TopologyException: found non-noded intersection between LINESTRING (311038 2.77019e+06, 311035 2.77019e+06) and LINESTRING (311038 2.77019e+06, 311035 2.77019e+06) at 311037.75737428927 2770193.7686056904\n",
      "2025-04-23 13:30:18,299 - WARNING - Topology error for category River in neighborhood 中南里: TopologyException: found non-noded intersection between LINESTRING (311955 2.77135e+06, 311955 2.77135e+06) and LINESTRING (311955 2.77135e+06, 311955 2.77135e+06) at 311955.22635489638 2771352.7457217239\n",
      "2025-04-23 13:30:20,139 - WARNING - Topology error for category Infrastructure in neighborhood 舊莊里: TopologyException: found non-noded intersection between LINESTRING (312826 2.76904e+06, 312824 2.76904e+06) and LINESTRING (312825 2.76904e+06, 312824 2.76904e+06) at 312824.50008723215 2769040.8990552658\n",
      "2025-04-23 13:30:20,726 - WARNING - Topology error for category Commercial in neighborhood 安康里: TopologyException: found non-noded intersection between LINESTRING (307950 2.76963e+06, 307949 2.76962e+06) and LINESTRING (307949 2.76962e+06, 307949 2.76962e+06) at 307948.62715117587 2769624.3576414455\n",
      "2025-04-23 13:30:20,864 - WARNING - Topology error for category River in neighborhood 九如里: TopologyException: found non-noded intersection between LINESTRING (312138 2.77015e+06, 312138 2.77014e+06) and LINESTRING (312138 2.77015e+06, 312138 2.77014e+06) at 312138.34700008662 2770150.6440004231\n",
      "2025-04-23 13:30:23,393 - WARNING - Topology error for category Road in neighborhood 博嘉里: TopologyException: found non-noded intersection between LINESTRING (308001 2.76569e+06, 308001 2.76569e+06) and LINESTRING (308001 2.76569e+06, 308000 2.76568e+06) at 308000.95038661739 2765685.1649155417\n",
      "2025-04-23 13:30:23,759 - WARNING - Topology error for category Residential in neighborhood 萬盛里: TopologyException: found non-noded intersection between LINESTRING (304449 2.76649e+06, 304450 2.7665e+06) and LINESTRING (304449 2.76648e+06, 304450 2.7665e+06) at 304449.97013593506 2766495.7391462782\n",
      "2025-04-23 13:30:24,088 - WARNING - Topology error for category Commercial in neighborhood 萬興里: TopologyException: found non-noded intersection between LINESTRING (307982 2.76418e+06, 307986 2.76418e+06) and LINESTRING (307982 2.76418e+06, 307984 2.76418e+06) at 307982.32430909772 2764175.6883213609\n",
      "2025-04-23 13:30:24,306 - WARNING - Topology error for category Residential in neighborhood 萬興里: TopologyException: found non-noded intersection between LINESTRING (307982 2.76418e+06, 307986 2.76418e+06) and LINESTRING (307982 2.76418e+06, 307984 2.76418e+06) at 307982.32430909772 2764175.6883213609\n",
      "2025-04-23 13:30:24,362 - WARNING - Topology error for category Natural in neighborhood 萬興里: TopologyException: found non-noded intersection between LINESTRING (310276 2.76631e+06, 310277 2.76631e+06) and LINESTRING (310277 2.76631e+06, 310277 2.76631e+06) at 310277.46693728981 2766309.5977024199\n",
      "2025-04-23 13:30:24,485 - WARNING - Topology error for category River in neighborhood 萬興里: TopologyException: found non-noded intersection between LINESTRING (307907 2.76419e+06, 307910 2.76418e+06) and LINESTRING (307907 2.76419e+06, 307907 2.76419e+06) at 307906.68941059953 2764186.3632669998\n",
      "2025-04-23 13:30:24,523 - WARNING - Topology error for category Infrastructure in neighborhood 萬興里: TopologyException: found non-noded intersection between LINESTRING (307982 2.76418e+06, 307986 2.76418e+06) and LINESTRING (307982 2.76418e+06, 307984 2.76418e+06) at 307982.32430909772 2764175.6883213609\n",
      "2025-04-23 13:30:24,584 - WARNING - Topology error for category Road in neighborhood 萬興里: TopologyException: found non-noded intersection between LINESTRING (309012 2.76609e+06, 309011 2.76609e+06) and LINESTRING (309011 2.76609e+06, 309016 2.76609e+06) at 309011.33430958074 2766091.7154930364\n",
      "2025-04-23 13:30:25,994 - INFO - Finished loading and preparing data.\n",
      "2025-04-23 13:30:26,034 - INFO - Computing correlation between road types and accident density...\n",
      "2025-04-23 13:30:26,057 - INFO - Roads CRS: {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"ProjectedCRS\", \"name\": \"TWD97 / TM2 zone 121\", \"base_crs\": {\"name\": \"TWD97\", \"datum\": {\"type\": \"GeodeticReferenceFrame\", \"name\": \"Taiwan Datum 1997\", \"ellipsoid\": {\"name\": \"GRS 1980\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257222101}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"id\": {\"authority\": \"EPSG\", \"code\": 3824}}, \"conversion\": {\"name\": \"Taiwan 2-degree TM zone 121\", \"method\": {\"name\": \"Transverse Mercator\", \"id\": {\"authority\": \"EPSG\", \"code\": 9807}}, \"parameters\": [{\"name\": \"Latitude of natural origin\", \"value\": 0, \"unit\": \"degree\", \"id\": {\"authority\": \"EPSG\", \"code\": 8801}}, {\"name\": \"Longitude of natural origin\", \"value\": 121, \"unit\": \"degree\", \"id\": {\"authority\": \"EPSG\", \"code\": 8802}}, {\"name\": \"Scale factor at natural origin\", \"value\": 0.9999, \"unit\": \"unity\", \"id\": {\"authority\": \"EPSG\", \"code\": 8805}}, {\"name\": \"False easting\", \"value\": 250000, \"unit\": \"metre\", \"id\": {\"authority\": \"EPSG\", \"code\": 8806}}, {\"name\": \"False northing\", \"value\": 0, \"unit\": \"metre\", \"id\": {\"authority\": \"EPSG\", \"code\": 8807}}]}, \"coordinate_system\": {\"subtype\": \"Cartesian\", \"axis\": [{\"name\": \"Easting\", \"abbreviation\": \"X\", \"direction\": \"east\", \"unit\": \"metre\"}, {\"name\": \"Northing\", \"abbreviation\": \"Y\", \"direction\": \"north\", \"unit\": \"metre\"}]}, \"scope\": \"Engineering survey, topographic mapping.\", \"area\": \"Taiwan, Republic of China - between 120\\u00b0E and 122\\u00b0E, onshore and offshore - Taiwan Island.\", \"bbox\": {\"south_latitude\": 20.41, \"west_longitude\": 119.99, \"north_latitude\": 26.72, \"east_longitude\": 122.06}, \"id\": {\"authority\": \"EPSG\", \"code\": 3826}}, Bounds: [ -49201.34316331 2687086.8646322   340402.47812579 3107530.84473387]\n",
      "2025-04-23 13:30:26,058 - INFO - Neighborhoods CRS: EPSG:3826, Bounds: [ 296266.05303084 2761514.89561711  317197.26073793 2789176.16901603]\n",
      "2025-04-23 13:30:26,063 - INFO - Accidents CRS: EPSG:3826, Bounds: [ 295756.69149719 2761989.77126713  315058.49224312 2786732.8587656 ]\n",
      "2025-04-23 13:30:26,068 - INFO - Roads geometry types: ['LineString']\n",
      "2025-04-23 13:30:26,070 - INFO - Neighborhoods geometry types: ['Polygon']\n",
      "2025-04-23 13:30:26,074 - INFO - Sample road geometries:\n",
      "0    LINESTRING (324778.51079599274 2780945.2627613...\n",
      "1    LINESTRING (296169.2235130913 2759463.11395603...\n",
      "2    LINESTRING (296381.58548612776 2758713.2679554...\n",
      "3    LINESTRING (297130.61534516735 2759165.1029101...\n",
      "4    LINESTRING (296863.93588555494 2759022.6435459...\n",
      "Name: geometry, dtype: object\n",
      "2025-04-23 13:30:26,074 - INFO - Sample neighborhood geometries:\n",
      "0    POLYGON ((302666.54271737445 2785226.842290448...\n",
      "1    POLYGON ((307802.16998795647 2787372.759303745...\n",
      "2    POLYGON ((302320.04429191677 2784654.093771082...\n",
      "3    POLYGON ((308159.7721281759 2784411.3655833476...\n",
      "4    POLYGON ((302319.03440019337 2784368.948626034...\n",
      "Name: geometry, dtype: object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Structure Summary ---\n",
      "\n",
      "Dataset: neighborhoods\n",
      "Shape: (456, 67)\n",
      "Columns and Data Types:\n",
      "LIE_NAME                            object\n",
      "SECT_NAME                           object\n",
      "2024population                       int32\n",
      "land_use_city_open_area_count        int32\n",
      "land_use_city_open_area_area_m2    float64\n",
      "                                    ...   \n",
      "transit_count                        int64\n",
      "accident_count                       int64\n",
      "road_density                       float64\n",
      "total_population                     int64\n",
      "elderly_percentage                 float64\n",
      "Length: 67, dtype: object\n",
      "Missing values (total): 0\n",
      "Missing values per column:\n",
      "LIE_NAME                           0\n",
      "SECT_NAME                          0\n",
      "2024population                     0\n",
      "land_use_city_open_area_count      0\n",
      "land_use_city_open_area_area_m2    0\n",
      "                                  ..\n",
      "transit_count                      0\n",
      "accident_count                     0\n",
      "road_density                       0\n",
      "total_population                   0\n",
      "elderly_percentage                 0\n",
      "Length: 67, dtype: int64\n",
      "Unique LIE_NAME: 456\n",
      "Sample data (first 2 rows):\n",
      "  LIE_NAME SECT_NAME  2024population  land_use_city_open_area_count  \\\n",
      "0      湖田里       北投區             856                              0   \n",
      "1      菁山里       士林區            1509                              0   \n",
      "\n",
      "   land_use_city_open_area_area_m2  land_use_city_open_area_percent  \\\n",
      "0                              0.0                              0.0   \n",
      "1                              0.0                              0.0   \n",
      "\n",
      "   land_use_commercial_count  land_use_commercial_area_m2  \\\n",
      "0                          0                          0.0   \n",
      "1                          0                          0.0   \n",
      "\n",
      "   land_use_commercial_percent  land_use_infrastructure_count  ...  \\\n",
      "0                          0.0                              0  ...   \n",
      "1                          0.0                              4  ...   \n",
      "\n",
      "   intersection_count       area_m2   area_km2  intersection_density  \\\n",
      "0                 110  1.632443e+07  16.324434              6.738365   \n",
      "1                  94  1.171922e+07  11.719218              8.021013   \n",
      "\n",
      "   tree_count  transit_count  accident_count  road_density  total_population  \\\n",
      "0         106            400              70      7.561174               857   \n",
      "1          54            313              33      9.913151              1485   \n",
      "\n",
      "   elderly_percentage  \n",
      "0               28.24  \n",
      "1               25.72  \n",
      "\n",
      "[2 rows x 67 columns]\n",
      "\n",
      "Dataset: buildings\n",
      "Shape: (74306, 9)\n",
      "Columns and Data Types:\n",
      "full_id       object\n",
      "osm_id        object\n",
      "building      object\n",
      "屋齡            object\n",
      "建物高度          object\n",
      "地上層數          object\n",
      "構造種類          object\n",
      "使用分區          object\n",
      "geometry    geometry\n",
      "dtype: object\n",
      "Missing values (total): 49\n",
      "Missing values per column:\n",
      "full_id      0\n",
      "osm_id       0\n",
      "building    31\n",
      "屋齡           0\n",
      "建物高度         0\n",
      "地上層數        18\n",
      "構造種類         0\n",
      "使用分區         0\n",
      "geometry     0\n",
      "dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "    full_id   osm_id    building    屋齡  建物高度  地上層數     構造種類     使用分區  \\\n",
      "0  r2633015  2633015   dormitory  <NA>  <NA>  <NA>  Unknown  Unknown   \n",
      "1  r2633016  2633016  university  <NA>  <NA>  <NA>  Unknown  Unknown   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((304401.483 2780904.12, 304430.995 27...  \n",
      "1  POLYGON ((304357.549 2780790.65, 304352.56 278...  \n",
      "\n",
      "Dataset: roads\n",
      "Shape: (81444, 3)\n",
      "Columns and Data Types:\n",
      "class         object\n",
      "geometry    geometry\n",
      "length_m     float64\n",
      "dtype: object\n",
      "Missing values (total): 604\n",
      "Missing values per column:\n",
      "class       604\n",
      "geometry      0\n",
      "length_m      0\n",
      "dtype: int64\n",
      "Road class counts:\n",
      "class\n",
      "service          21575\n",
      "footway          19776\n",
      "residential      15429\n",
      "tertiary          5402\n",
      "steps             4301\n",
      "secondary         4059\n",
      "path              3857\n",
      "unclassified      1957\n",
      "primary           1292\n",
      "cycleway           878\n",
      "track              741\n",
      "trunk              609\n",
      "pedestrian         323\n",
      "motorway           316\n",
      "living_street      267\n",
      "unknown             56\n",
      "bridleway            2\n",
      "Name: count, dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "     class                                           geometry       length_m\n",
      "0     None  LINESTRING (324778.511 2780945.263, 324826.86 ...  426414.891763\n",
      "1  service  LINESTRING (296169.224 2759463.114, 296160.343...    1055.960489\n",
      "\n",
      "Dataset: trees\n",
      "Shape: (5019, 12)\n",
      "Columns and Data Types:\n",
      "id               object\n",
      "geometry       geometry\n",
      "version           int32\n",
      "sources          object\n",
      "subtype          object\n",
      "class            object\n",
      "surface          object\n",
      "names            object\n",
      "level           float64\n",
      "source_tags      object\n",
      "wikidata         object\n",
      "elevation       float64\n",
      "dtype: object\n",
      "Missing values (total): 24487\n",
      "Missing values per column:\n",
      "id                0\n",
      "geometry          0\n",
      "version           0\n",
      "sources           0\n",
      "subtype           0\n",
      "class             0\n",
      "surface        5013\n",
      "names          4553\n",
      "level          5015\n",
      "source_tags       0\n",
      "wikidata       4943\n",
      "elevation      4963\n",
      "dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "                                 id  \\\n",
      "0  08b4ba2399d31fff0003c5f62a41c335   \n",
      "1  08b4ba0ae3a22fff0003ca54f368c81d   \n",
      "\n",
      "                                            geometry  version  \\\n",
      "0  POLYGON ((161777.639 2544255.739, 161813.286 2...        0   \n",
      "1  POLYGON ((297620.639 2760911.484, 297592.092 2...        0   \n",
      "\n",
      "                                             sources subtype   class surface  \\\n",
      "0  [{'property': '', 'dataset': 'OpenStreetMap', ...    land  island    None   \n",
      "1  [{'property': '', 'dataset': 'OpenStreetMap', ...  forest    wood    None   \n",
      "\n",
      "                                               names  level  \\\n",
      "0  {'primary': '臺灣', 'common': [('af', 'Taiwan'),...    NaN   \n",
      "1                                               None    NaN   \n",
      "\n",
      "                               source_tags wikidata  elevation  \n",
      "0  [(place, island), (type, multipolygon)]   Q22502        NaN  \n",
      "1  [(natural, wood), (type, multipolygon)]     None        NaN  \n",
      "\n",
      "Dataset: transit\n",
      "Shape: (29892, 11)\n",
      "Columns and Data Types:\n",
      "id               object\n",
      "geometry       geometry\n",
      "version           int32\n",
      "sources          object\n",
      "subtype          object\n",
      "class            object\n",
      "surface          object\n",
      "names            object\n",
      "level           float64\n",
      "source_tags      object\n",
      "wikidata         object\n",
      "dtype: object\n",
      "Missing values (total): 101060\n",
      "Missing values per column:\n",
      "id                 0\n",
      "geometry           0\n",
      "version            0\n",
      "sources            0\n",
      "subtype            0\n",
      "class              0\n",
      "surface        28287\n",
      "names          17731\n",
      "level          25803\n",
      "source_tags        0\n",
      "wikidata       29239\n",
      "dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "                                 id  \\\n",
      "0  08b4ba0ac6758fff0001be0bbe7a4ada   \n",
      "1  08b4ba0ae332afff0001a76b5977464d   \n",
      "\n",
      "                                            geometry  version  \\\n",
      "0  LINESTRING (320296.896 2765488.056, 320042.372...        0   \n",
      "1                     POINT (296843.534 2759002.204)        0   \n",
      "\n",
      "                                             sources  subtype       class  \\\n",
      "0  [{'property': '', 'dataset': 'OpenStreetMap', ...    power  power_line   \n",
      "1  [{'property': '', 'dataset': 'OpenStreetMap', ...  barrier        gate   \n",
      "\n",
      "  surface names  level                         source_tags wikidata  \n",
      "0    None  None    NaN  [(power, line), (voltage, 345000)]     None  \n",
      "1    None  None    NaN                   [(barrier, gate)]     None  \n",
      "\n",
      "Dataset: urban_masterplan\n",
      "Shape: (15521, 15)\n",
      "Columns and Data Types:\n",
      "編號            object\n",
      "圖層            object\n",
      "顏色            object\n",
      "街廓編號          object\n",
      "分區代碼          object\n",
      "分區簡稱          object\n",
      "使用分區          object\n",
      "分區說明          object\n",
      "原屬分區          object\n",
      "變更前代碼         object\n",
      "變更前簡稱         object\n",
      "變更前分區         object\n",
      "Category      object\n",
      "Area         float64\n",
      "geometry    geometry\n",
      "dtype: object\n",
      "Missing values (total): 92705\n",
      "Missing values per column:\n",
      "編號              0\n",
      "圖層              5\n",
      "顏色              5\n",
      "街廓編號        15508\n",
      "分區代碼            5\n",
      "分區簡稱            5\n",
      "使用分區            0\n",
      "分區說明        15409\n",
      "原屬分區        15205\n",
      "變更前代碼       15521\n",
      "變更前簡稱       15521\n",
      "變更前分區       15521\n",
      "Category        0\n",
      "Area            0\n",
      "geometry        0\n",
      "dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "  編號  圖層  顏色  街廓編號 分區代碼 分區簡稱  使用分區  分區說明  原屬分區 變更前代碼 變更前簡稱 變更前分區  \\\n",
      "0  1  32  19  None  PEA    公  公園用地  None  None  None  None  None   \n",
      "1  2  32  19  None  PEA    公  公園用地  None  None  None  None  None   \n",
      "\n",
      "         Category      Area                                           geometry  \n",
      "0  City_Open_Area   823.190  MULTIPOLYGON (((303340.099 2771175.776, 303329...  \n",
      "1  City_Open_Area  4721.047  MULTIPOLYGON (((303230.925 2771108.301, 303088...  \n",
      "\n",
      "Dataset: accidents\n",
      "Shape: (56133, 8)\n",
      "Columns and Data Types:\n",
      "Month            object\n",
      "Day               int32\n",
      "Hours             int32\n",
      "Minute            int32\n",
      "Location         object\n",
      "Speed_limit       int32\n",
      "Roadtype          int32\n",
      "geometry       geometry\n",
      "dtype: object\n",
      "Missing values (total): 0\n",
      "Missing values per column:\n",
      "Month          0\n",
      "Day            0\n",
      "Hours          0\n",
      "Minute         0\n",
      "Location       0\n",
      "Speed_limit    0\n",
      "Roadtype       0\n",
      "geometry       0\n",
      "dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "     Month  Day  Hours  Minute              Location  Speed_limit  Roadtype  \\\n",
      "0  January    1      0      46  大安區仁愛路4段與忠孝東路4段216巷口           50         4   \n",
      "1  January    1      0      46  大安區仁愛路4段與忠孝東路4段216巷口           50         4   \n",
      "\n",
      "                        geometry  \n",
      "0  POINT (305807.42 2770096.759)  \n",
      "1  POINT (305807.42 2770096.759)  \n",
      "\n",
      "Dataset: population\n",
      "Shape: (456, 3)\n",
      "Columns and Data Types:\n",
      "LIE_NAME               object\n",
      "Total_Population        int64\n",
      "Elderly_Percentage    float64\n",
      "dtype: object\n",
      "Missing values (total): 0\n",
      "Missing values per column:\n",
      "LIE_NAME              0\n",
      "Total_Population      0\n",
      "Elderly_Percentage    0\n",
      "dtype: int64\n",
      "Unique LIE_NAME: 456\n",
      "Sample data (first 2 rows):\n",
      "  LIE_NAME  Total_Population  Elderly_Percentage\n",
      "0      南福里             12021               16.10\n",
      "1      奇岩里             11200               22.68\n",
      "--- End of Data Structure Summary ---\n",
      "\n",
      "Starting compute_road_type_accident_correlation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1356164/3813905347.py:233: UserWarning: Legend does not support handles for PatchCollection instances.\n",
      "See: https://matplotlib.org/stable/tutorials/intermediate/legend_guide.html#implementing-a-custom-legend-handler\n",
      "  plt.legend()\n",
      "2025-04-23 13:30:27,663 - INFO - Overlay plot saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/roads_neighborhoods_overlap.png\n",
      "2025-04-23 13:30:28,126 - INFO - Assigning accidents to nearest road...\n",
      "2025-04-23 13:30:31,082 - INFO - Matched 56133 accidents out of 56133\n",
      "2025-04-23 13:30:31,087 - INFO - Reassigning 4991 accidents from footway/cycleway...\n",
      "2025-04-23 13:30:31,182 - INFO - Reassigned 1429 accidents to wider roads\n",
      "2025-04-23 13:30:31,189 - INFO - Accidents by road type:\n",
      "class\n",
      "bridleway            0\n",
      "cycleway           247\n",
      "footway           3315\n",
      "living_street       79\n",
      "motorway           109\n",
      "path                86\n",
      "pedestrian          84\n",
      "primary           6535\n",
      "residential      10110\n",
      "secondary        16180\n",
      "service           5011\n",
      "steps               47\n",
      "tertiary          9135\n",
      "track                8\n",
      "trunk             2493\n",
      "unclassified      1665\n",
      "unknown             66\n",
      "Name: accident_count, dtype: int64\n",
      "2025-04-23 13:30:31,196 - INFO - Road type counts:\n",
      "class\n",
      "service          21204\n",
      "footway          16755\n",
      "residential      14861\n",
      "tertiary          5113\n",
      "secondary         3869\n",
      "path              3610\n",
      "steps             2968\n",
      "unclassified      1894\n",
      "primary           1209\n",
      "cycleway           825\n",
      "track              716\n",
      "trunk              593\n",
      "motorway           313\n",
      "pedestrian         297\n",
      "living_street      264\n",
      "unknown             54\n",
      "bridleway            2\n",
      "Name: count, dtype: int64\n",
      "2025-04-23 13:30:31,207 - INFO - Spearman's correlation between road width rank and accident density: 0.798 (p-value: 0.001)\n",
      "2025-04-23 13:30:31,207 - INFO - Computing average road accident density per neighborhood...\n",
      "2025-04-23 13:30:31,208 - INFO - Roads DataFrame shape before join: (75149, 6)\n",
      "2025-04-23 13:30:31,208 - INFO - Neighborhoods DataFrame shape before join: (456, 67)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Road type counts:\n",
      "class\n",
      "service          21204\n",
      "footway          16755\n",
      "residential      14861\n",
      "tertiary          5113\n",
      "secondary         3869\n",
      "path              3610\n",
      "steps             2968\n",
      "unclassified      1894\n",
      "primary           1209\n",
      "cycleway           825\n",
      "track              716\n",
      "trunk              593\n",
      "motorway           313\n",
      "pedestrian         297\n",
      "living_street      264\n",
      "unknown             54\n",
      "bridleway            2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Road Type Accident Density Summary ---\n",
      "            class    length_m  accident_count  accident_density  width_rank\n",
      "1        cycleway   263682.07             245              0.26           1\n",
      "2         footway  1767503.72            3235              0.73           1\n",
      "3   living_street    23929.78              77              1.58           3\n",
      "4        motorway   215317.82             109              2.74           5\n",
      "5            path   720602.60              84              0.09           1\n",
      "6      pedestrian    31830.26              82              0.47           1\n",
      "7         primary   212330.19            6433             44.39           4\n",
      "8     residential  1455020.72           10037              4.87           3\n",
      "9       secondary   534770.12           15993             36.32           4\n",
      "10        service  2502054.44            5001              0.68           2\n",
      "11          steps   161013.29              37              0.10           1\n",
      "12       tertiary   755660.86            9060             11.17           3\n",
      "13          track   159053.47               8              0.02           2\n",
      "14          trunk   265713.76            2469             11.69           5\n",
      "Spearman's correlation: 0.798 (p-value: 0.001)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 13:30:31,524 - INFO - Road-neighborhood join resulted in 85340 matches with columns: ['index_left', 'geometry', 'class', 'length_m', 'width_rank', 'accident_density', 'index_right0', 'index_right', 'LIE_NAME']\n",
      "2025-04-23 13:30:31,529 - INFO - Assigned avg_road_accident_density to 0 neighborhoods\n",
      "2025-04-23 13:30:31,530 - INFO - Avg road accident density stats:\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: avg_road_accident_density, dtype: float64\n",
      "2025-04-23 13:30:31,686 - INFO - Bar chart saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_bar.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bar chart saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_bar.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 13:30:32,142 - INFO - Box chart saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_box.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box chart saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_box.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 13:30:32,396 - INFO - Scatter plot saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_scatter.png\n",
      "2025-04-23 13:30:32,399 - INFO - Top 3 road types by accident density:\n",
      "        class  accident_density\n",
      "7     primary             44.39\n",
      "9   secondary             36.32\n",
      "14      trunk             11.69\n",
      "2025-04-23 13:30:32,400 - INFO - Stage 2: Building city graph...\n",
      "2025-04-23 13:30:32,400 - INFO - Dataset neighborhoods column types:\n",
      "LIE_NAME                            object\n",
      "SECT_NAME                           object\n",
      "2024population                       int32\n",
      "land_use_city_open_area_count        int32\n",
      "land_use_city_open_area_area_m2    float64\n",
      "                                    ...   \n",
      "accident_count                       int64\n",
      "road_density                       float64\n",
      "total_population                     int64\n",
      "elderly_percentage                 float64\n",
      "avg_road_accident_density          float64\n",
      "Length: 68, dtype: object\n",
      "2025-04-23 13:30:32,409 - INFO - Dataset buildings column types:\n",
      "full_id       object\n",
      "osm_id        object\n",
      "building      object\n",
      "屋齡            object\n",
      "建物高度          object\n",
      "地上層數          object\n",
      "構造種類          object\n",
      "使用分區          object\n",
      "geometry    geometry\n",
      "dtype: object\n",
      "2025-04-23 13:30:32,411 - INFO - Dataset roads column types:\n",
      "class         object\n",
      "geometry    geometry\n",
      "length_m     float64\n",
      "dtype: object\n",
      "2025-04-23 13:30:32,413 - INFO - Dataset trees column types:\n",
      "id               object\n",
      "geometry       geometry\n",
      "version           int32\n",
      "sources          object\n",
      "subtype          object\n",
      "class            object\n",
      "surface          object\n",
      "names            object\n",
      "level           float64\n",
      "source_tags      object\n",
      "wikidata         object\n",
      "elevation       float64\n",
      "dtype: object\n",
      "2025-04-23 13:30:32,417 - INFO - Dataset transit column types:\n",
      "id               object\n",
      "geometry       geometry\n",
      "version           int32\n",
      "sources          object\n",
      "subtype          object\n",
      "class            object\n",
      "surface          object\n",
      "names            object\n",
      "level           float64\n",
      "source_tags      object\n",
      "wikidata         object\n",
      "dtype: object\n",
      "2025-04-23 13:30:32,419 - INFO - Dataset urban_masterplan column types:\n",
      "編號            object\n",
      "圖層            object\n",
      "顏色            object\n",
      "街廓編號          object\n",
      "分區代碼          object\n",
      "分區簡稱          object\n",
      "使用分區          object\n",
      "分區說明          object\n",
      "原屬分區          object\n",
      "變更前代碼         object\n",
      "變更前簡稱         object\n",
      "變更前分區         object\n",
      "Category      object\n",
      "Area         float64\n",
      "geometry    geometry\n",
      "dtype: object\n",
      "2025-04-23 13:30:32,422 - INFO - Dataset accidents column types:\n",
      "Month            object\n",
      "Day               int32\n",
      "Hours             int32\n",
      "Minute            int32\n",
      "Location         object\n",
      "Speed_limit       int32\n",
      "Roadtype          int32\n",
      "geometry       geometry\n",
      "dtype: object\n",
      "2025-04-23 13:30:32,424 - INFO - Dataset population column types:\n",
      "LIE_NAME               object\n",
      "Total_Population        int64\n",
      "Elderly_Percentage    float64\n",
      "dtype: object\n",
      "2025-04-23 13:30:32,425 - INFO - Constructing graph nodes...\n",
      "2025-04-23 13:30:32,425 - WARNING - 'area_m2' column missing in buildings_gdf. Computing from geometry...\n",
      "2025-04-23 13:30:32,435 - INFO - Computed area_m2 stats:\n",
      "count    74306.000000\n",
      "mean       326.713464\n",
      "std        783.686503\n",
      "min          0.603543\n",
      "25%         97.315016\n",
      "50%        182.929099\n",
      "75%        304.603966\n",
      "max      62939.914744\n",
      "Name: area_m2, dtype: float64\n",
      "2025-04-23 13:30:32,435 - INFO - Adding neighborhood nodes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scatter plot saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_scatter.png\n",
      "Top 3 road types by accident density:\n",
      "        class  accident_density\n",
      "7     primary             44.39\n",
      "9   secondary             36.32\n",
      "14      trunk             11.69\n",
      "Starting build_graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neighborhood nodes: 100%|██████████| 456/456 [00:00<00:00, 23319.30it/s]\n",
      "2025-04-23 13:30:32,457 - INFO - Adding building nodes...\n",
      "Building nodes: 100%|██████████| 74306/74306 [00:01<00:00, 47613.10it/s]\n",
      "2025-04-23 13:30:34,020 - INFO - Adding road nodes...\n",
      "Road nodes: 100%|██████████| 81444/81444 [00:01<00:00, 45333.38it/s]\n",
      "2025-04-23 13:30:36,680 - INFO - Converting GeoDataFrames to cudf for GPU processing...\n",
      "2025-04-23 13:30:37,044 - INFO - Extracting bounding box coordinates...\n",
      "2025-04-23 13:30:50,243 - INFO - neighborhoods_cudf['min_x'] dtype: float64\n",
      "2025-04-23 13:30:50,244 - INFO - buildings_cudf['min_x'] dtype: float64\n",
      "2025-04-23 13:30:50,244 - INFO - roads_cudf['min_x'] dtype: float64\n",
      "2025-04-23 13:30:50,245 - INFO - Creating edges using GPU-accelerated spatial joins...\n",
      "2025-04-23 13:30:50,245 - INFO - Computing neighborhood-neighborhood edges...\n",
      "Neighborhood-Neighborhood edges: 100%|██████████| 456/456 [00:21<00:00, 21.06it/s] \n",
      "2025-04-23 13:31:11,898 - INFO - Computing neighborhood-building edges...\n",
      "Neighborhood-Building edges: 100%|██████████| 456/456 [00:36<00:00, 12.50it/s]\n",
      "2025-04-23 13:31:48,373 - INFO - Computing neighborhood-road edges...\n",
      "Neighborhood-Road edges: 100%|██████████| 456/456 [00:13<00:00, 34.76it/s] \n",
      "2025-04-23 13:32:01,588 - INFO - Created 270909 total edges\n",
      "2025-04-23 13:32:01,623 - INFO - After validation, 270909 edges remain\n",
      "2025-04-23 13:32:01,627 - INFO - Sample edges after validation:\n",
      "   src  dst\n",
      "0    0    1\n",
      "1    0    2\n",
      "2    0    3\n",
      "3    0    4\n",
      "4    0    5\n",
      "2025-04-23 13:32:01,785 - INFO - Saving graph data to cache...\n",
      "2025-04-23 13:32:02,152 - INFO - Successfully saved graph data to cache.\n",
      "2025-04-23 13:32:02,153 - INFO - City graph constructed: 156206 nodes, 270909 edges\n",
      "2025-04-23 13:32:02,183 - INFO - Graph edge count: 270909\n",
      "2025-04-23 13:32:02,183 - INFO - Computing walkability scores for neighborhoods...\n",
      "2025-04-23 13:32:02,283 - WARNING - 'ndvi' column missing. Using tree_density instead.\n",
      "2025-04-23 13:32:02,296 - INFO - Number of neighborhood nodes in nodes_df: 456\n",
      "2025-04-23 13:32:02,297 - INFO - Number of entries in walkability_components: 456\n",
      "2025-04-23 13:32:02,303 - INFO - Sample LIE_NAME in nodes_df: ['湖田里', '菁山里', '大屯里', '平等里', '泉源里']\n",
      "2025-04-23 13:32:02,304 - INFO - Sample LIE_NAME in walkability_components: ['湖田里', '菁山里', '大屯里', '平等里', '泉源里']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting compute_walkability_scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 13:32:02,525 - INFO - Finished computing walkability scores.\n",
      "2025-04-23 13:32:02,528 - INFO - Preparing data for GNN training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prepare_gnn_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing features by node type:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-23 13:32:02,605 - WARNING - Column avg_road_accident_density missing in neighborhood nodes. Setting to 0.\n",
      "2025-04-23 13:32:02,605 - WARNING - Column pedestrian_road_density missing in neighborhood nodes. Setting to 0.\n",
      "2025-04-23 13:32:02,613 - INFO - Node type neighborhood: 456 nodes, feature shape: (456, 127)\n",
      "2025-04-23 13:32:02,913 - INFO - Node type building: 74306 nodes, feature shape: (74306, 127)\n",
      "Normalizing features by node type:  67%|██████▋   | 2/3 [00:00<00:00,  5.44it/s]2025-04-23 13:32:03,142 - INFO - Node type road: 81444 nodes, feature shape: (81444, 127)\n",
      "Normalizing features by node type: 100%|██████████| 3/3 [00:00<00:00,  5.00it/s]\n",
      "2025-04-23 13:32:03,249 - INFO - Edge index created with 270909 edges\n",
      "2025-04-23 13:32:03,257 - INFO - Prepared GNN data: 156206 nodes, 270909 edges\n",
      "2025-04-23 13:32:03,258 - INFO - Feature matrix shape: torch.Size([156206, 127])\n",
      "2025-04-23 13:32:03,258 - INFO - Label tensor shape: torch.Size([156206, 1])\n",
      "2025-04-23 13:32:03,264 - INFO - Stage 4: Training GNN model...\n",
      "2025-04-23 13:32:03,440 - INFO - Target (walkability_score) distribution for neighborhood nodes:\n",
      "count    456.000000\n",
      "mean       0.337368\n",
      "std        0.046356\n",
      "min        0.195320\n",
      "25%        0.305536\n",
      "50%        0.330726\n",
      "75%        0.362302\n",
      "max        0.586699\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting train_gnn_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epochs:   0%|          | 0/300 [00:00<?, ?it/s]2025-04-23 13:32:03,833 - INFO - Epoch 0, Train Loss: 0.0332, Val Loss: 0.0157\n",
      "Training epochs:   3%|▎         | 9/300 [00:01<00:26, 10.83it/s]2025-04-23 13:32:04,618 - INFO - Epoch 10, Train Loss: 0.0031, Val Loss: 0.0043\n",
      "Training epochs:   6%|▋         | 19/300 [00:01<00:20, 13.82it/s]2025-04-23 13:32:05,309 - INFO - Epoch 20, Train Loss: 0.0021, Val Loss: 0.0016\n",
      "Training epochs:  10%|▉         | 29/300 [00:02<00:18, 14.63it/s]2025-04-23 13:32:05,993 - INFO - Epoch 30, Train Loss: 0.0018, Val Loss: 0.0014\n",
      "Training epochs:  13%|█▎        | 39/300 [00:03<00:18, 14.45it/s]2025-04-23 13:32:06,685 - INFO - Epoch 40, Train Loss: 0.0019, Val Loss: 0.0012\n",
      "Training epochs:  16%|█▋        | 49/300 [00:03<00:17, 14.59it/s]2025-04-23 13:32:07,378 - INFO - Epoch 50, Train Loss: 0.0016, Val Loss: 0.0011\n",
      "Training epochs:  20%|█▉        | 59/300 [00:04<00:16, 14.57it/s]2025-04-23 13:32:08,060 - INFO - Epoch 60, Train Loss: 0.0015, Val Loss: 0.0010\n",
      "Training epochs:  23%|██▎       | 69/300 [00:05<00:15, 14.85it/s]2025-04-23 13:32:08,731 - INFO - Epoch 70, Train Loss: 0.0014, Val Loss: 0.0011\n",
      "Training epochs:  26%|██▌       | 77/300 [00:05<00:14, 14.89it/s]2025-04-23 13:32:09,271 - INFO - Early stopping at epoch 78\n",
      "Training epochs:  26%|██▌       | 78/300 [00:05<00:16, 13.40it/s]\n",
      "2025-04-23 13:32:09,277 - INFO - Finished training GNN model.\n",
      "2025-04-23 13:32:09,278 - INFO - Predicting walkability with GNN...\n",
      "2025-04-23 13:32:09,356 - INFO - Preparing data for GNN training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting predict_walkability...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing features by node type:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-23 13:32:09,395 - WARNING - Column avg_road_accident_density missing in neighborhood nodes. Setting to 0.\n",
      "2025-04-23 13:32:09,396 - WARNING - Column pedestrian_road_density missing in neighborhood nodes. Setting to 0.\n",
      "2025-04-23 13:32:09,404 - INFO - Node type neighborhood: 456 nodes, feature shape: (456, 127)\n",
      "2025-04-23 13:32:09,682 - INFO - Node type building: 74306 nodes, feature shape: (74306, 127)\n",
      "Normalizing features by node type:  67%|██████▋   | 2/3 [00:00<00:00,  6.04it/s]2025-04-23 13:32:09,855 - INFO - Node type road: 81444 nodes, feature shape: (81444, 127)\n",
      "Normalizing features by node type: 100%|██████████| 3/3 [00:00<00:00,  5.96it/s]\n",
      "2025-04-23 13:32:09,966 - INFO - Edge index created with 270909 edges\n",
      "2025-04-23 13:32:09,970 - INFO - Prepared GNN data: 156206 nodes, 270909 edges\n",
      "2025-04-23 13:32:09,971 - INFO - Feature matrix shape: torch.Size([156206, 127])\n",
      "2025-04-23 13:32:09,971 - INFO - Label tensor shape: torch.Size([156206, 1])\n",
      "2025-04-23 13:32:10,266 - INFO - Walkability GNN stats after prediction:\n",
      "count    456.000000\n",
      "mean       0.343043\n",
      "std        0.023437\n",
      "min        0.279569\n",
      "25%        0.327192\n",
      "50%        0.342819\n",
      "75%        0.357627\n",
      "max        0.408939\n",
      "Name: walkability_gnn, dtype: float64\n",
      "2025-04-23 13:32:10,270 - INFO - Generating interactive Kepler.gl map...\n",
      "2025-04-23 13:32:10,380 - INFO - Neighborhood nodes count: 456\n",
      "2025-04-23 13:32:10,380 - INFO - Neighborhoods_gdf count: 456\n",
      "2025-04-23 13:32:10,381 - INFO - Sample LIE_NAME in nodes_df: ['鄰江里', '富錦里', '錦華里', '孝德里', '三重里']\n",
      "2025-04-23 13:32:10,381 - INFO - Sample LIE_NAME in neighborhoods_gdf: ['鄰江里', '富錦里', '錦華里', '孝德里', '三重里']\n",
      "2025-04-23 13:32:10,382 - INFO - Common LIE_NAMEs: 456\n",
      "2025-04-23 13:32:10,382 - INFO - Nodes LIE_NAMEs not in GDF: []\n",
      "2025-04-23 13:32:10,383 - INFO - GDF LIE_NAMEs not in nodes: []\n",
      "2025-04-23 13:32:10,384 - INFO - Nodes nulls: {'LIE_NAME': 0, 'walkability_score': 0, 'walkability_gnn': 0, 'walkability_category': 0}\n",
      "2025-04-23 13:32:10,385 - INFO - GDF geometry nulls: 0\n",
      "2025-04-23 13:32:10,388 - INFO - Merged map_data rows: 456\n",
      "2025-04-23 13:32:10,389 - INFO - Walkability score nulls: 0\n",
      "2025-04-23 13:32:10,389 - INFO - Walkability GNN nulls: 0\n",
      "2025-04-23 13:32:10,392 - INFO - Walkability score distribution in map_data:\n",
      "count    456.000000\n",
      "mean       0.337368\n",
      "std        0.046356\n",
      "min        0.195320\n",
      "25%        0.305536\n",
      "50%        0.330726\n",
      "75%        0.362302\n",
      "max        0.586699\n",
      "Name: walkability_score, dtype: float64\n",
      "2025-04-23 13:32:10,394 - INFO - Walkability GNN distribution in map_data:\n",
      "count    456.000000\n",
      "mean       0.343043\n",
      "std        0.023437\n",
      "min        0.279569\n",
      "25%        0.327192\n",
      "50%        0.342819\n",
      "75%        0.357627\n",
      "max        0.408939\n",
      "Name: walkability_gnn, dtype: float64\n",
      "2025-04-23 13:32:10,395 - INFO - Walkability category distribution in map_data:\n",
      "walkability_category\n",
      "medium    233\n",
      "low       223\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting create_interactive_map...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 13:32:10,504 - INFO - Interactive map generated and saved as /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/taipei_walkability_map.html\n",
      "2025-04-23 13:32:10,605 - INFO - Final validation - Walkability scores in neighborhood nodes:\n",
      "2025-04-23 13:32:10,606 - INFO - Walkability score distribution:\n",
      "count    456.000000\n",
      "mean       0.337368\n",
      "std        0.046356\n",
      "min        0.195320\n",
      "25%        0.305536\n",
      "50%        0.330726\n",
      "75%        0.362302\n",
      "max        0.586699\n",
      "Name: walkability_score, dtype: float64\n",
      "2025-04-23 13:32:10,607 - INFO - Walkability GNN distribution:\n",
      "count    456.000000\n",
      "mean       0.343043\n",
      "std        0.023437\n",
      "min        0.279569\n",
      "25%        0.327192\n",
      "50%        0.342819\n",
      "75%        0.357627\n",
      "max        0.408939\n",
      "Name: walkability_gnn, dtype: float64\n",
      "2025-04-23 13:32:10,608 - INFO - Walkability category distribution:\n",
      "walkability_category\n",
      "medium    233\n",
      "low       223\n",
      "Name: count, dtype: int64\n",
      "2025-04-23 13:32:10,608 - INFO - Number of neighborhood nodes with non-zero walkability_score: 456/456\n",
      "2025-04-23 13:32:10,608 - INFO - Number of neighborhood nodes with non-zero walkability_gnn: 456/456\n",
      "2025-04-23 13:32:10,609 - WARNING - Walkability scores have low variation (std < 0.05). Components may need adjustment.\n",
      "2025-04-23 13:32:10,609 - WARNING - GNN predictions have low variation (std < 0.05). Check edge creation and model training.\n",
      "2025-04-23 13:32:10,611 - INFO - Correlation between walkability_score and walkability_gnn: 0.71 (p-value: 0.00)\n",
      "2025-04-23 13:32:10,612 - INFO - Processing complete. Timing summary:\n",
      "2025-04-23 13:32:10,612 - INFO - load_and_prepare_data: 32.94 seconds\n",
      "2025-04-23 13:32:10,613 - INFO - compute_road_type_accident_correlation: 6.37 seconds\n",
      "2025-04-23 13:32:10,613 - INFO - build_graph: 89.78 seconds\n",
      "2025-04-23 13:32:10,613 - INFO - compute_walkability_scores: 0.34 seconds\n",
      "2025-04-23 13:32:10,614 - INFO - prepare_gnn_data: 0.74 seconds\n",
      "2025-04-23 13:32:10,614 - INFO - train_gnn_model: 6.01 seconds\n",
      "2025-04-23 13:32:10,616 - INFO - predict_walkability: 0.99 seconds\n",
      "2025-04-23 13:32:10,616 - INFO - create_interactive_map: 0.24 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n",
      "Map saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/taipei_walkability_map.html!\n",
      "Map saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/taipei_walkability_map.html!\n",
      "Pipeline completed successfully.\n",
      "   src  dst\n",
      "0    0    1\n",
      "1    0    2\n",
      "2    0    3\n",
      "3    0    4\n",
      "4    0    5\n"
     ]
    }
   ],
   "source": [
    "def main(force_recompute_graph=False):\n",
    "    \"\"\"Main execution pipeline for the analysis.\"\"\"\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    os.makedirs(SUBGRAPH_DIR, exist_ok=True)\n",
    "    logging.info(f\"Ensured subgraph directory exists: {SUBGRAPH_DIR}\")\n",
    "\n",
    "    # Track timing for each step\n",
    "    timings = {}\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load and prepare data\n",
    "        start_time = time.time()\n",
    "        print(\"Starting load_and_prepare_data...\")\n",
    "        data = load_and_prepare_data()\n",
    "        timings['load_and_prepare_data'] = time.time() - start_time\n",
    "\n",
    "        # Step 2: Compute road type accident correlation\n",
    "        start_time = time.time()\n",
    "        print(\"Starting compute_road_type_accident_correlation...\")\n",
    "        road_accident_summary = compute_road_type_accident_correlation(\n",
    "            data['roads'], data['neighborhoods'], data['accidents']\n",
    "        )\n",
    "        timings['compute_road_type_accident_correlation'] = time.time() - start_time\n",
    "\n",
    "        # Step 3: Build graph\n",
    "        start_time = time.time()\n",
    "        print(\"Starting build_graph...\")\n",
    "        G = build_graph(data, force_recompute=force_recompute_graph)\n",
    "        timings['build_graph'] = time.time() - start_time\n",
    "\n",
    "        # Validate edge counts\n",
    "        edge_count = G.edgelist.edgelist_df.shape[0] if G.edgelist else 0\n",
    "        logging.info(f\"Graph edge count: {edge_count}\")\n",
    "        if edge_count == 0:\n",
    "            logging.warning(\"Graph has no edges. GNN will not utilize graph structure.\")\n",
    "\n",
    "        # Step 4: Compute walkability scores\n",
    "        start_time = time.time()\n",
    "        print(\"Starting compute_walkability_scores...\")\n",
    "        G = compute_walkability_scores(G, data)\n",
    "        timings['compute_walkability_scores'] = time.time() - start_time\n",
    "\n",
    "        # Step 5: Prepare GNN data\n",
    "        start_time = time.time()\n",
    "        print(\"Starting prepare_gnn_data...\")\n",
    "        data_gnn = prepare_gnn_data(G)\n",
    "        timings['prepare_gnn_data'] = time.time() - start_time\n",
    "\n",
    "        # Step 6: Train GNN model\n",
    "        start_time = time.time()\n",
    "        print(\"Starting train_gnn_model...\")\n",
    "        model = train_gnn_model(data_gnn)\n",
    "        timings['train_gnn_model'] = time.time() - start_time\n",
    "\n",
    "        # Step 7: Predict walkability\n",
    "        start_time = time.time()\n",
    "        print(\"Starting predict_walkability...\")\n",
    "        G = predict_walkability(G, model)\n",
    "        timings['predict_walkability'] = time.time() - start_time\n",
    "\n",
    "        # Step 8: Create interactive map\n",
    "        start_time = time.time()\n",
    "        print(\"Starting create_interactive_map...\")\n",
    "        create_interactive_map(G, data)\n",
    "        timings['create_interactive_map'] = time.time() - start_time\n",
    "\n",
    "        # Final validation: Check walkability scores\n",
    "        nodes_df = G._nodes.to_pandas()\n",
    "        neighborhood_nodes = nodes_df[nodes_df['type'] == 'neighborhood']\n",
    "        walkability_score_stats = neighborhood_nodes['walkability_score'].describe()\n",
    "        walkability_gnn_stats = neighborhood_nodes['walkability_gnn'].describe()\n",
    "        walkability_category_dist = neighborhood_nodes['walkability_category'].value_counts()\n",
    "        non_zero_walkability = (neighborhood_nodes['walkability_score'] > 0).sum()\n",
    "        non_zero_walkability_gnn = (neighborhood_nodes['walkability_gnn'] > 0).sum()\n",
    "        \n",
    "        logging.info(\"Final validation - Walkability scores in neighborhood nodes:\")\n",
    "        logging.info(f\"Walkability score distribution:\\n{walkability_score_stats}\")\n",
    "        logging.info(f\"Walkability GNN distribution:\\n{walkability_gnn_stats}\")\n",
    "        logging.info(f\"Walkability category distribution:\\n{walkability_category_dist}\")\n",
    "        logging.info(f\"Number of neighborhood nodes with non-zero walkability_score: {non_zero_walkability}/{len(neighborhood_nodes)}\")\n",
    "        logging.info(f\"Number of neighborhood nodes with non-zero walkability_gnn: {non_zero_walkability_gnn}/{len(neighborhood_nodes)}\")\n",
    "\n",
    "        # Check for low variation in walkability scores\n",
    "        if walkability_score_stats['std'] < 0.05:\n",
    "            logging.warning(\"Walkability scores have low variation (std < 0.05). Components may need adjustment.\")\n",
    "        if walkability_gnn_stats['std'] < 0.05:\n",
    "            logging.warning(\"GNN predictions have low variation (std < 0.05). Check edge creation and model training.\")\n",
    "\n",
    "        # Compute correlation between walkability_score and walkability_gnn\n",
    "        corr, p_value = pearsonr(neighborhood_nodes['walkability_score'], neighborhood_nodes['walkability_gnn'])\n",
    "        logging.info(f\"Correlation between walkability_score and walkability_gnn: {corr:.2f} (p-value: {p_value:.2f})\")\n",
    "        if corr < 0.5:\n",
    "            logging.warning(\"Low correlation between walkability_score and walkability_gnn. GNN predictions may not align well with rule-based scores.\")\n",
    "\n",
    "        # Log timing summary\n",
    "        logging.info(\"Processing complete. Timing summary:\")\n",
    "        for step, duration in timings.items():\n",
    "            logging.info(f\"{step}: {duration:.2f} seconds\")\n",
    "        \n",
    "        print(\"Pipeline completed successfully.\")\n",
    "        print(G.edgelist.edgelist_df.to_pandas().head())\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Pipeline failed with error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(force_recompute_graph=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
