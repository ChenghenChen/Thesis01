{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a986f754",
   "metadata": {},
   "source": [
    "Cell 0: CUDA Version check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc2c30cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.02.02 25.02.00\n"
     ]
    }
   ],
   "source": [
    "import cudf, cugraph\n",
    "print(cudf.__version__, cugraph.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac3ad7",
   "metadata": {},
   "source": [
    "Cell 1: Imports ,Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e388b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import logging\n",
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Third-party imports\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import cugraph\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv, BatchNorm\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from shapely import make_valid\n",
    "from shapely.errors import GEOSException\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from keplergl import KeplerGl\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from torch_geometric.nn import GATConv, BatchNorm\n",
    "import torch.nn.functional as F\n",
    "# Plotting configuration\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Noto Sans CJK TC', 'Noto Serif CJK TC', 'Noto Sans Mono CJK TC', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Directory and file paths\n",
    "BASE_DIR = \"/home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data\"\n",
    "LANDUSE_NDVI_PATH = os.path.join(BASE_DIR, \"neighborhoods_with_ndvi_numerical_corrected.geojson\")\n",
    "OSM_BUILDINGS_PATH = os.path.join(BASE_DIR, \"Taipei_Buildings_fulldata.geojson\")\n",
    "OSM_ROADS_PATH = os.path.join(BASE_DIR, \"taipei_segments_cleaned_verified.geoparquet\")\n",
    "OSM_TREES_PATH = os.path.join(BASE_DIR, \"taipei_land.geoparquet\")\n",
    "OSM_TRANSIT_PATH = os.path.join(BASE_DIR, \"taipei_infrastructure.geoparquet\")\n",
    "URBAN_MASTERPLAN_PATH = os.path.join(BASE_DIR, \"Taipei_urban_masterplan.geojson\")\n",
    "ACCIDENTS_PATH = os.path.join(BASE_DIR, \"2023_accidents.geojson\")\n",
    "POPULATION_PATH = os.path.join(BASE_DIR, \"population_corrected.json\")\n",
    "SUBGRAPH_DIR = os.path.join(BASE_DIR, \"subgraphs\")\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"checkpoints\")\n",
    "INTERSECTION_CACHE_PATH = os.path.join(BASE_DIR, \"neighborhoods_with_intersections.geoparquet\")\n",
    "GRAPH_NODES_CACHE_PATH = os.path.join(BASE_DIR, \"graph_nodes.parquet\")\n",
    "GRAPH_EDGES_CACHE_PATH = os.path.join(BASE_DIR, \"graph_edges.parquet\")\n",
    "GRAPH_NODE_ID_CACHE_PATH = os.path.join(BASE_DIR, \"graph_node_id_to_index.json\")\n",
    "GRAPH_DATA_HASH_PATH = os.path.join(BASE_DIR, \"graph_data_hash.txt\")\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(SUBGRAPH_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Constants for spatial analysis\n",
    "BUFFER_DISTANCE = 10  # Meters, buffer distance for border sharing of accidents (tunable based on spatial resolution)\n",
    "MIN_ROAD_LENGTH = 10  # Meters, minimum road length to avoid inflated accident density (tunable based on dataset)\n",
    "\n",
    "# Land use category priorities for area assignment\n",
    "CATEGORY_PRIORITY = {\n",
    "    'City_Open_Area': 10,\n",
    "    'Pedestrian': 9,\n",
    "    'Public_Transportation': 8,\n",
    "    'Amenity': 7,\n",
    "    'Education': 6,\n",
    "    'Medical': 5,\n",
    "    'Commercial': 4,\n",
    "    'Residential': 3,\n",
    "    'Natural': 2,\n",
    "    'Road': 1,\n",
    "    'River': 1,\n",
    "    'Infrastructure': 1,\n",
    "    'Government': 1,\n",
    "    'Special_Zone': 1,\n",
    "    'Military': 1,\n",
    "    'Industrial': 1,\n",
    "    'Agriculture': 1\n",
    "}\n",
    "\n",
    "# Weights for land use diversity in walkability scoring\n",
    "land_use_weights = {\n",
    "    'city_open_area': 0.8,\n",
    "    'commercial': 0.7,\n",
    "    'infrastructure': 0.4,\n",
    "    'government': 0.5,\n",
    "    'public_transportation': 0.8,\n",
    "    'education': 0.7,\n",
    "    'medical': 0.6,\n",
    "    'amenity': 0.8,\n",
    "    'road': 0.3,\n",
    "    'pedestrian': 1.0,\n",
    "    'natural': 0.7,\n",
    "    'special_zone': 0.4,\n",
    "    'river': 0.7,\n",
    "    'military': 0.2,\n",
    "    'residential': 0.6,\n",
    "    'industrial': 0.3,\n",
    "    'agriculture': 0.4\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Normalise Information to Standard \n",
    "Read and Group to standard\n",
    "Merge to standard\n",
    "graph\n",
    "kepler merge\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b5eefec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_id</th>\n",
       "      <th>osm_id</th>\n",
       "      <th>building</th>\n",
       "      <th>屋齡</th>\n",
       "      <th>建物高度</th>\n",
       "      <th>地上層數</th>\n",
       "      <th>構造種類</th>\n",
       "      <th>使用分區</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r2633015</td>\n",
       "      <td>2633015</td>\n",
       "      <td>dormitory</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>POLYGON ((121.53954 25.13558, 121.53983 25.135...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r2633016</td>\n",
       "      <td>2633016</td>\n",
       "      <td>university</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>POLYGON ((121.5391 25.13456, 121.53905 25.1347...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r2633017</td>\n",
       "      <td>2633017</td>\n",
       "      <td>yes</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>POLYGON ((121.53777 25.13606, 121.53826 25.136...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r2633018</td>\n",
       "      <td>2633018</td>\n",
       "      <td>yes</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>POLYGON ((121.53752 25.13559, 121.53777 25.135...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r2633019</td>\n",
       "      <td>2633019</td>\n",
       "      <td>yes</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>POLYGON ((121.53791 25.13688, 121.53797 25.136...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74301</th>\n",
       "      <td>w1318077126</td>\n",
       "      <td>1318077126</td>\n",
       "      <td>house</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>POLYGON ((121.49534 25.02304, 121.49555 25.023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74302</th>\n",
       "      <td>w1324938980</td>\n",
       "      <td>1324938980</td>\n",
       "      <td>apartments</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>POLYGON ((121.50182 25.04497, 121.50201 25.045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74303</th>\n",
       "      <td>w1329134163</td>\n",
       "      <td>1329134163</td>\n",
       "      <td>residential</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>POLYGON ((121.50542 25.04627, 121.50548 25.046...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74304</th>\n",
       "      <td>w1329134164</td>\n",
       "      <td>1329134164</td>\n",
       "      <td>residential</td>\n",
       "      <td>28.0</td>\n",
       "      <td>36.85</td>\n",
       "      <td>12.0</td>\n",
       "      <td>RC造</td>\n",
       "      <td>商4</td>\n",
       "      <td>POLYGON ((121.50544 25.04632, 121.50542 25.046...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74305</th>\n",
       "      <td>w1329136164</td>\n",
       "      <td>1329136164</td>\n",
       "      <td>apartments</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>POLYGON ((121.50617 25.04728, 121.50654 25.047...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74306 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           full_id      osm_id     building    屋齡   建物高度  地上層數     構造種類  \\\n",
       "0         r2633015     2633015    dormitory  <NA>   <NA>  <NA>  Unknown   \n",
       "1         r2633016     2633016   university  <NA>   <NA>  <NA>  Unknown   \n",
       "2         r2633017     2633017          yes  <NA>   <NA>  <NA>  Unknown   \n",
       "3         r2633018     2633018          yes  <NA>   <NA>  <NA>  Unknown   \n",
       "4         r2633019     2633019          yes  <NA>   <NA>  <NA>  Unknown   \n",
       "...            ...         ...          ...   ...    ...   ...      ...   \n",
       "74301  w1318077126  1318077126        house  <NA>   <NA>  <NA>  Unknown   \n",
       "74302  w1324938980  1324938980   apartments  <NA>   <NA>  <NA>  Unknown   \n",
       "74303  w1329134163  1329134163  residential  <NA>   <NA>  <NA>  Unknown   \n",
       "74304  w1329134164  1329134164  residential  28.0  36.85  12.0      RC造   \n",
       "74305  w1329136164  1329136164   apartments  <NA>   <NA>  <NA>  Unknown   \n",
       "\n",
       "          使用分區                                           geometry  \n",
       "0      Unknown  POLYGON ((121.53954 25.13558, 121.53983 25.135...  \n",
       "1      Unknown  POLYGON ((121.5391 25.13456, 121.53905 25.1347...  \n",
       "2      Unknown  POLYGON ((121.53777 25.13606, 121.53826 25.136...  \n",
       "3      Unknown  POLYGON ((121.53752 25.13559, 121.53777 25.135...  \n",
       "4      Unknown  POLYGON ((121.53791 25.13688, 121.53797 25.136...  \n",
       "...        ...                                                ...  \n",
       "74301  Unknown  POLYGON ((121.49534 25.02304, 121.49555 25.023...  \n",
       "74302  Unknown  POLYGON ((121.50182 25.04497, 121.50201 25.045...  \n",
       "74303  Unknown  POLYGON ((121.50542 25.04627, 121.50548 25.046...  \n",
       "74304       商4  POLYGON ((121.50544 25.04632, 121.50542 25.046...  \n",
       "74305  Unknown  POLYGON ((121.50617 25.04728, 121.50654 25.047...  \n",
       "\n",
       "[74306 rows x 9 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpd.read_file(OSM_BUILDINGS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4722f69a",
   "metadata": {},
   "source": [
    "Cell 2: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "713654c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_walkability_components_all(neighborhoods_gdf, data):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import logging\n",
    "    \n",
    "    # Compute land_use_score\n",
    "    land_use_categories = list(land_use_weights.keys())\n",
    "    \n",
    "    def compute_land_use_score(row):\n",
    "        score = 0.0\n",
    "        for cat in land_use_categories:\n",
    "            col = f'land_use_{cat}_percent'\n",
    "            if col in row:\n",
    "                p = row[col] / 100.0\n",
    "                score += land_use_weights[cat] * p\n",
    "        return score\n",
    "    \n",
    "    neighborhoods_gdf['land_use_score'] = neighborhoods_gdf.apply(compute_land_use_score, axis=1)\n",
    "    logging.info(f\"land_use_score stats:\\n{neighborhoods_gdf['land_use_score'].describe()}\")\n",
    "    \n",
    "    # Compute population_density\n",
    "    neighborhoods_gdf['population_density'] = neighborhoods_gdf['total_population'] / neighborhoods_gdf['area_km2'].replace(0, 1e-6)\n",
    "    logging.info(f\"population_density stats:\\n{neighborhoods_gdf['population_density'].describe()}\")\n",
    "    \n",
    "    # Compute transit_density\n",
    "    neighborhoods_gdf['transit_density'] = neighborhoods_gdf['transit_count'] / neighborhoods_gdf['area_km2'].replace(0, 1e-6)\n",
    "    logging.info(f\"transit_density stats:\\n{neighborhoods_gdf['transit_density'].describe()}\")\n",
    "    \n",
    "    # Use ndvi for green_space, fallback to tree_density if ndvi is missing\n",
    "    if 'ndvi' in neighborhoods_gdf.columns:\n",
    "        neighborhoods_gdf['green_space'] = neighborhoods_gdf['ndvi']\n",
    "        logging.info(f\"ndvi stats:\\n{neighborhoods_gdf['ndvi'].describe()}\")\n",
    "    else:\n",
    "        logging.warning(\"'ndvi' column missing. Using tree_density instead.\")\n",
    "        neighborhoods_gdf['tree_density'] = neighborhoods_gdf['tree_count'] / neighborhoods_gdf['area_km2'].replace(0, 1e-6)\n",
    "        neighborhoods_gdf['green_space'] = neighborhoods_gdf['tree_density']\n",
    "    logging.info(f\"green_space stats:\\n{neighborhoods_gdf['green_space'].describe()}\")\n",
    "    \n",
    "    # Compute accident_density\n",
    "    neighborhoods_gdf['accident_density'] = neighborhoods_gdf['accident_count'] / neighborhoods_gdf['area_km2'].replace(0, 1e-6)\n",
    "    logging.info(f\"accident_density stats:\\n{neighborhoods_gdf['accident_density'].describe()}\")\n",
    "    \n",
    "    # Log intersection_density if present\n",
    "    if 'intersection_density' in neighborhoods_gdf.columns:\n",
    "        logging.info(f\"intersection_density stats:\\n{neighborhoods_gdf['intersection_density'].describe()}\")\n",
    "    else:\n",
    "        logging.warning(\"intersection_density column missing.\")\n",
    "    \n",
    "    # Define components and their directions\n",
    "    components = {\n",
    "        'land_use_score': 'higher_better',\n",
    "        'intersection_density': 'higher_better',\n",
    "        'population_density': 'higher_better',\n",
    "        'transit_density': 'higher_better',\n",
    "        'green_space': 'higher_better',\n",
    "        'accident_density': 'lower_better'\n",
    "    }\n",
    "    \n",
    "    # Define weights to emphasize high-variation components\n",
    "    component_weights = {\n",
    "        'land_use_score': 0.20,  # Increase if diversity is key\n",
    "        'intersection_density': 0.05,\n",
    "        'population_density': 0.25,\n",
    "        'transit_density': 0.25,\n",
    "        'green_space': 0.15,\n",
    "        'accident_density': 0.10  # Increase if safety is critical\n",
    "    }\n",
    "    \n",
    "    # Compute robust normalization (median and IQR)\n",
    "    medians = {}\n",
    "    iqrs = {}\n",
    "    for comp in components:\n",
    "        if comp == 'land_use_score':\n",
    "            medians[comp] = 0.0\n",
    "            iqrs[comp] = 1.0\n",
    "            neighborhoods_gdf[f'{comp}_norm'] = neighborhoods_gdf[comp]\n",
    "        else:\n",
    "            medians[comp] = neighborhoods_gdf[comp].median()\n",
    "            q75, q25 = neighborhoods_gdf[comp].quantile([0.75, 0.25])\n",
    "            iqrs[comp] = q75 - q25 if q75 != q25 else 1.0\n",
    "            if iqrs[comp] == 0:\n",
    "                logging.warning(f\"Component {comp} has no IQR variation. Setting normalized value to 0.5\")\n",
    "                neighborhoods_gdf[f'{comp}_norm'] = 0.5\n",
    "            else:\n",
    "                if components[comp] == 'higher_better':\n",
    "                    neighborhoods_gdf[f'{comp}_norm'] = (neighborhoods_gdf[comp] - neighborhoods_gdf[comp].min()) / (neighborhoods_gdf[comp].max() - neighborhoods_gdf[comp].min())\n",
    "                else:\n",
    "                    neighborhoods_gdf[f'{comp}_norm'] = (neighborhoods_gdf[comp].max() - neighborhoods_gdf[comp]) / (neighborhoods_gdf[comp].max() - neighborhoods_gdf[comp].min())\n",
    "    \n",
    "    logging.info(\"Normalization statistics:\")\n",
    "    for comp in components:\n",
    "        logging.info(f\"{comp} - median: {medians[comp]:.4f}, IQR: {iqrs[comp]:.4f}\")\n",
    "    \n",
    "    # Compute walkability_score as weighted sum\n",
    "    neighborhoods_gdf['walkability_score'] = 0.0\n",
    "    for comp in components:\n",
    "        norm_col = f'{comp}_norm'\n",
    "        if norm_col in neighborhoods_gdf.columns:\n",
    "            neighborhoods_gdf['walkability_score'] += component_weights[comp] * neighborhoods_gdf[norm_col]\n",
    "        else:\n",
    "            logging.error(f\"Normalized column {norm_col} not found.\")\n",
    "    \n",
    "    # Ensure it's between 0 and 1\n",
    "    neighborhoods_gdf['walkability_score'] = neighborhoods_gdf['walkability_score'].clip(0, 1)\n",
    "    logging.info(f\"walkability_score stats:\\n{neighborhoods_gdf['walkability_score'].describe()}\")\n",
    "    \n",
    "    # Compute walkability_category with dynamic thresholds\n",
    "    low_threshold = neighborhoods_gdf['walkability_score'].quantile(0.33)\n",
    "    high_threshold = neighborhoods_gdf['walkability_score'].quantile(0.66)\n",
    "    logging.info(f\"Walkability category thresholds - low: {low_threshold:.4f}, high: {high_threshold:.4f}\")\n",
    "    \n",
    "    def categorize_score(score):\n",
    "        if score < low_threshold:\n",
    "            return 'low'\n",
    "        elif score < high_threshold:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'high'\n",
    "    \n",
    "    neighborhoods_gdf['walkability_category'] = neighborhoods_gdf['walkability_score'].apply(categorize_score)\n",
    "    logging.info(f\"Walkability category distribution:\\n{neighborhoods_gdf['walkability_category'].value_counts()}\")\n",
    "    \n",
    "    # Return relevant columns\n",
    "    return neighborhoods_gdf[['LIE_NAME', 'walkability_score', 'walkability_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b05a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_geometry(geom, buffer_size=1e-5):\n",
    "    \"\"\"Fix invalid geometries with logging for debugging.\"\"\"\n",
    "    if geom is None or geom.is_empty:\n",
    "        logging.debug(\"Geometry is None or empty, returning a default Point(0,0).\")\n",
    "        return Point(0, 0)\n",
    "    try:\n",
    "        geom = make_valid(geom)\n",
    "        if not geom.is_valid:\n",
    "            logging.debug(f\"Geometry invalid after make_valid, applying buffer(0): {geom.bounds}\")\n",
    "            geom = geom.buffer(0)\n",
    "            if not geom.is_valid:\n",
    "                logging.debug(f\"Geometry still invalid, applying buffer with size {buffer_size}: {geom.bounds}\")\n",
    "                geom = geom.buffer(buffer_size)\n",
    "                geom = make_valid(geom)\n",
    "        if not geom.is_valid:\n",
    "            logging.warning(f\"Geometry remains invalid after all attempts: {geom.bounds}. Returning default Point(0,0).\")\n",
    "            return Point(0, 0)\n",
    "        return geom\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fixing geometry: {e}. Returning default Point(0,0).\")\n",
    "        return Point(0, 0)\n",
    "\n",
    "def print_data_structure(data_dict):\n",
    "    \"\"\"Print a detailed summary of the data structure for each dataset.\"\"\"\n",
    "    print(\"\\n--- Data Structure Summary ---\")\n",
    "    for key, df in data_dict.items():\n",
    "        if isinstance(df, (gpd.GeoDataFrame, pd.DataFrame, cudf.DataFrame)):\n",
    "            df = df.to_pandas() if isinstance(df, cudf.DataFrame) else df\n",
    "            print(f\"\\nDataset: {key}\")\n",
    "            print(f\"Shape: {df.shape}\")\n",
    "            print(f\"Columns and Data Types:\\n{df.dtypes}\")\n",
    "            print(f\"Missing values (total): {df.isnull().sum().sum()}\")\n",
    "            print(f\"Missing values per column:\\n{df.isnull().sum()}\")\n",
    "            if 'LIE_NAME' in df.columns:\n",
    "                print(f\"Unique LIE_NAME: {df['LIE_NAME'].nunique()}\")\n",
    "            if 'class' in df.columns and key == 'roads':\n",
    "                print(f\"Road class counts:\\n{df['class'].value_counts()}\")\n",
    "            print(f\"Sample data (first 2 rows):\\n{df.head(2)}\")\n",
    "    print(\"--- End of Data Structure Summary ---\\n\")\n",
    "\n",
    "def print_percentage_calculation(neighborhoods_gdf, urban_masterplan_gdf, sample_size=3):\n",
    "    \"\"\"Print the land use percentage calculation process for a sample of neighborhoods.\"\"\"\n",
    "    print(\"\\n--- Percentage Calculation Process ---\")\n",
    "    sample_neighborhoods = neighborhoods_gdf.sample(min(sample_size, len(neighborhoods_gdf)), random_state=42)\n",
    "    \n",
    "    for idx, row in sample_neighborhoods.iterrows():\n",
    "        lie_name = row['LIE_NAME']\n",
    "        print(f\"\\nNeighborhood: {lie_name} (Index: {idx})\")\n",
    "        \n",
    "        neighborhood_geom = fix_geometry(row['geometry'])\n",
    "        if not neighborhood_geom.is_valid:\n",
    "            print(f\"Neighborhood geometry is invalid after fixing: {lie_name}\")\n",
    "            continue\n",
    "        \n",
    "        relevant_masterplan = urban_masterplan_gdf[urban_masterplan_gdf.intersects(neighborhood_geom)]\n",
    "        if relevant_masterplan.empty:\n",
    "            print(\"No master plan polygons intersect with this neighborhood.\")\n",
    "            continue\n",
    "        \n",
    "        temp_gdf = gpd.GeoDataFrame({'geometry': [neighborhood_geom]}, crs='EPSG:3826')\n",
    "        intersected = gpd.overlay(temp_gdf, relevant_masterplan, how='intersection', keep_geom_type=False)\n",
    "        if intersected.empty:\n",
    "            print(\"No valid intersections after overlay.\")\n",
    "            continue\n",
    "        \n",
    "        intersected['geometry'] = intersected['geometry'].apply(fix_geometry)\n",
    "        intersected = intersected[intersected.geometry.is_valid & ~intersected.geometry.is_empty]\n",
    "        if intersected.empty:\n",
    "            print(\"No valid geometries after fixing intersected polygons.\")\n",
    "            continue\n",
    "        \n",
    "        intersected['priority'] = intersected['Category'].map(CATEGORY_PRIORITY)\n",
    "        intersected = intersected.sort_values(by='priority', ascending=False)\n",
    "        \n",
    "        total_area_geom = intersected.geometry.union_all()\n",
    "        total_area = total_area_geom.area\n",
    "        print(f\"Total unique master plan area: {total_area:.2f} m²\")\n",
    "        \n",
    "        remaining_geom = total_area_geom\n",
    "        category_areas = {}\n",
    "        for category in intersected['Category'].unique():\n",
    "            category_rows = intersected[intersected['Category'] == category]\n",
    "            category_geom = category_rows.geometry.union_all()\n",
    "            try:\n",
    "                category_area_geom = category_geom.intersection(remaining_geom)\n",
    "                category_area = category_area_geom.area\n",
    "                category_areas[category] = category_area\n",
    "                print(f\"Area of {category} (priority {CATEGORY_PRIORITY.get(category, 0)}): {category_area:.2f} m²\")\n",
    "                remaining_geom = remaining_geom.difference(category_area_geom)\n",
    "            except GEOSException as e:\n",
    "                print(f\"Topology error for category {category}: {e}\")\n",
    "                category_areas[category] = 0.0\n",
    "        \n",
    "        print(\"\\nPercentages:\")\n",
    "        total_percentage = 0.0\n",
    "        for category, area in category_areas.items():\n",
    "            percentage = (area / total_area * 100) if total_area > 0 else 0.0\n",
    "            total_percentage += percentage\n",
    "            print(f\"{category}: {percentage:.2f}%\")\n",
    "        print(f\"Sum of percentages: {total_percentage:.2f}%\")\n",
    "    print(\"--- End of Percentage Calculation Process ---\\n\")\n",
    "\n",
    "def compute_data_hash(data_dict):\n",
    "    \"\"\"Compute a hash of the data for caching purposes.\"\"\"\n",
    "    hasher = hashlib.sha256()\n",
    "    for key, df in data_dict.items():\n",
    "        if isinstance(df, (gpd.GeoDataFrame, pd.DataFrame, cudf.DataFrame)):\n",
    "            df = df.to_pandas() if isinstance(df, cudf.DataFrame) else df\n",
    "            hasher.update(str(df.shape).encode('utf-8'))\n",
    "            hasher.update(str(sorted(df.columns)).encode('utf-8'))\n",
    "            \n",
    "            logging.info(f\"Dataset {key} column types:\\n{df.dtypes}\")\n",
    "            \n",
    "            sample_df = df.head(5).copy()\n",
    "            if 'geometry' in sample_df.columns:\n",
    "                sample_df = sample_df.drop(columns=['geometry'])\n",
    "            for col in sample_df.columns:\n",
    "                sample_df[col] = sample_df[col].apply(\n",
    "                    lambda x: x.tolist() if isinstance(x, np.ndarray) else\n",
    "                              float(x) if isinstance(x, (np.floating, np.integer)) else x\n",
    "                )\n",
    "            try:\n",
    "                sample = sample_df.to_json()\n",
    "                hasher.update(sample.encode('utf-8'))\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to serialize sample for dataset {key}: {e}\")\n",
    "                sample = str(sample_df.to_dict())\n",
    "                hasher.update(sample.encode('utf-8'))\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def check_spatial_overlap(gdf1, gdf2, label1=\"gdf1\", label2=\"gdf2\"):\n",
    "    \"\"\"Check for spatial overlap between two GeoDataFrames and log the results.\"\"\"\n",
    "    logging.info(f\"Checking spatial overlap between {label1} and {label2}...\")\n",
    "    gdf1 = gdf1.copy()\n",
    "    gdf2 = gdf2.copy()\n",
    "    \n",
    "    if gdf1.crs != gdf2.crs:\n",
    "        logging.warning(f\"CRS mismatch between {label1} ({gdf1.crs}) and {label2} ({gdf2.crs}). Aligning to {gdf1.crs}...\")\n",
    "        gdf2 = gdf2.to_crs(gdf1.crs)\n",
    "    \n",
    "    gdf1['geometry'] = gdf1['geometry'].apply(fix_geometry)\n",
    "    gdf2['geometry'] = gdf2['geometry'].apply(fix_geometry)\n",
    "    \n",
    "    gdf1_bounds = gdf1.total_bounds\n",
    "    gdf2_bounds = gdf2.total_bounds\n",
    "    logging.info(f\"{label1} bounds: {gdf1_bounds}\")\n",
    "    logging.info(f\"{label2} bounds: {gdf2_bounds}\")\n",
    "    \n",
    "    bounds_overlap = not (gdf1_bounds[2] < gdf2_bounds[0] or\n",
    "                         gdf1_bounds[0] > gdf2_bounds[2] or\n",
    "                         gdf1_bounds[3] < gdf2_bounds[1] or\n",
    "                         gdf1_bounds[1] > gdf2_bounds[3])\n",
    "    logging.info(f\"Bounding boxes overlap: {bounds_overlap}\")\n",
    "    \n",
    "    sample_size = min(10, len(gdf1), len(gdf2))\n",
    "    if sample_size > 0:\n",
    "        sample_gdf1 = gdf1.sample(sample_size, random_state=42)\n",
    "        intersects = gpd.sjoin(sample_gdf1, gdf2, how='inner', predicate='intersects')\n",
    "        logging.info(f\"Sample intersection check: {len(intersects)} intersections found out of {sample_size} samples.\")\n",
    "    \n",
    "    return bounds_overlap\n",
    "\n",
    "def validate_data(gdf, required_cols, name=\"GeoDataFrame\"):\n",
    "    \"\"\"Validate that the GeoDataFrame has all required columns, no missing geometries, and valid geometries.\"\"\"\n",
    "    if gdf.empty:\n",
    "        logging.error(f\"{name} is empty.\")\n",
    "        raise ValueError(f\"{name} is empty.\")\n",
    "    missing_cols = [col for col in required_cols if col not in gdf.columns]\n",
    "    if missing_cols:\n",
    "        logging.error(f\"Missing columns in {name}: {missing_cols}\")\n",
    "        raise KeyError(f\"Missing columns in {name}: {missing_cols}\")\n",
    "    if gdf.geometry.isna().any():\n",
    "        logging.error(f\"Missing geometries in {name}\")\n",
    "        raise ValueError(f\"Missing geometries in {name}\")\n",
    "    if not all(gdf.geometry.is_valid):\n",
    "        logging.error(f\"Invalid geometries in {name}\")\n",
    "        raise ValueError(f\"Invalid geometries in {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ae081",
   "metadata": {},
   "source": [
    "Cell 3: Walkability Computation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea0d0ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_road_type_accident_correlation(roads_gdf, neighborhoods_gdf, accidents_gdf):\n",
    "    logging.info(\"Computing correlation between road types and accident density...\")\n",
    "    \n",
    "    # Validate input data\n",
    "    validate_data(roads_gdf, ['class', 'geometry', 'length_m'], \"roads_gdf\")\n",
    "    validate_data(neighborhoods_gdf, ['LIE_NAME', 'geometry'], \"neighborhoods_gdf\")\n",
    "    validate_data(accidents_gdf, ['geometry'], \"accidents_gdf\")\n",
    "    \n",
    "    # Ensure correct CRS\n",
    "    target_crs = 'EPSG:3826'\n",
    "    source_crs = 'EPSG:4326'\n",
    "    for gdf, name in [(roads_gdf, \"roads\"), (neighborhoods_gdf, \"neighborhoods\"), (accidents_gdf, \"accidents\")]:\n",
    "        if gdf.crs is None:\n",
    "            logging.warning(f\"{name} has no CRS defined. Assuming {source_crs}.\")\n",
    "            gdf.set_crs(source_crs, inplace=True)\n",
    "        if gdf.crs != target_crs:\n",
    "            logging.info(f\"Reprojecting {name} from {gdf.crs} to {target_crs}\")\n",
    "            gdf.to_crs(target_crs, inplace=True)\n",
    "    \n",
    "    # Log CRS, bounds, and sample geometries for debugging\n",
    "    logging.info(f\"Roads CRS: {roads_gdf.crs}, Bounds: {roads_gdf.total_bounds}\")\n",
    "    logging.info(f\"Neighborhoods CRS: {neighborhoods_gdf.crs}, Bounds: {neighborhoods_gdf.total_bounds}\")\n",
    "    logging.info(f\"Accidents CRS: {accidents_gdf.crs}, Bounds: {accidents_gdf.total_bounds}\")\n",
    "    logging.info(f\"Roads geometry types: {roads_gdf.geometry.type.unique()}\")\n",
    "    logging.info(f\"Neighborhoods geometry types: {neighborhoods_gdf.geometry.type.unique()}\")\n",
    "    sample_roads = roads_gdf.head(5)['geometry'].apply(lambda x: str(x)[:100])\n",
    "    sample_neighborhoods = neighborhoods_gdf.head(5)['geometry'].apply(lambda x: str(x)[:100])\n",
    "    logging.info(f\"Sample road geometries:\\n{sample_roads}\")\n",
    "    logging.info(f\"Sample neighborhood geometries:\\n{sample_neighborhoods}\")\n",
    "    \n",
    "    # Visualize data for debugging\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    neighborhoods_gdf.plot(ax=ax, color='blue', alpha=0.5, label='Neighborhoods')\n",
    "    roads_gdf.plot(ax=ax, color='red', alpha=0.5, label='Roads')\n",
    "    plt.legend()\n",
    "    plt.title('Roads and Neighborhoods Overlay')\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'roads_neighborhoods_overlap.png'))\n",
    "    plt.close()\n",
    "    logging.info(f\"Overlay plot saved to {os.path.join(BASE_DIR, 'roads_neighborhoods_overlap.png')}\")\n",
    "    \n",
    "    # Make local copies for roads and accidents\n",
    "    roads_gdf_local = roads_gdf.copy()\n",
    "    accidents_gdf_local = accidents_gdf.copy()\n",
    "    \n",
    "    # Add unique identifier to accidents\n",
    "    accidents_gdf_local['accident_id'] = range(len(accidents_gdf_local))\n",
    "    \n",
    "    # Define width ranking\n",
    "    width_ranking = {\n",
    "        'motorway': 5, 'trunk': 5, 'primary': 4, 'secondary': 4, 'tertiary': 3,\n",
    "        'residential': 3, 'living_street': 3, 'service': 2, 'track': 2,\n",
    "        'path': 1, 'footway': 1, 'cycleway': 1, 'steps': 1, 'pedestrian': 1,\n",
    "        'unclassified': 0, 'bridleway': 0, 'unknown': 0\n",
    "    }\n",
    "    roads_gdf_local['width_rank'] = roads_gdf_local['class'].map(width_ranking).fillna(0).astype(int)\n",
    "    \n",
    "    # Buffer wider roads for accident assignment\n",
    "    roads_gdf_buffered = roads_gdf_local.copy()\n",
    "    roads_gdf_buffered['geometry'] = roads_gdf_buffered.apply(\n",
    "        lambda row: row['geometry'].buffer(5) if row['width_rank'] >= 4 else row['geometry'], axis=1\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Assigning accidents to nearest road...\")\n",
    "    accidents_gdf_local['geometry'] = accidents_gdf_local['geometry'].apply(fix_geometry)\n",
    "    accidents_gdf_local = accidents_gdf_local[accidents_gdf_local['geometry'].is_valid & ~accidents_gdf_local['geometry'].is_empty]\n",
    "    \n",
    "    if accidents_gdf_local.empty:\n",
    "        logging.warning(\"No valid accidents after geometry fixing.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Assign accidents to nearest road\n",
    "    nearest = gpd.sjoin_nearest(\n",
    "        accidents_gdf_local,\n",
    "        roads_gdf_buffered[['geometry', 'class', 'width_rank']],\n",
    "        how='left',\n",
    "        distance_col='distance'\n",
    "    )\n",
    "    nearest['weighted_distance'] = nearest['distance'] / (nearest['width_rank'].replace(0, 1) ** 2)\n",
    "    nearest = nearest.sort_values('weighted_distance').drop_duplicates(subset=['accident_id'], keep='first')\n",
    "    \n",
    "    matched_accidents = nearest[['accident_id', 'index_right']].copy()\n",
    "    matched_accidents.columns = ['accident_id', 'road_idx']\n",
    "    matched_accidents = matched_accidents.dropna(subset=['road_idx'])\n",
    "    matched_accidents['road_idx'] = matched_accidents['road_idx'].astype(int)\n",
    "    \n",
    "    logging.info(f\"Matched {len(matched_accidents)} accidents out of {len(accidents_gdf_local)}\")\n",
    "    \n",
    "    # Reassign accidents from footway/cycleway to wider roads if possible\n",
    "    footway_cycleway_accidents = matched_accidents[\n",
    "        matched_accidents['road_idx'].isin(\n",
    "            roads_gdf_local[roads_gdf_local['class'].isin(['footway', 'cycleway'])].index\n",
    "        )\n",
    "    ]\n",
    "    if not footway_cycleway_accidents.empty:\n",
    "        logging.info(f\"Reassigning {len(footway_cycleway_accidents)} accidents from footway/cycleway...\")\n",
    "        accidents_to_reassign = accidents_gdf_local[accidents_gdf_local['accident_id'].isin(footway_cycleway_accidents['accident_id'])].copy()\n",
    "        wider_roads = roads_gdf_buffered[roads_gdf_buffered['width_rank'] >= 4]\n",
    "        if not wider_roads.empty:\n",
    "            reassigned = gpd.sjoin_nearest(\n",
    "                accidents_to_reassign,\n",
    "                wider_roads[['geometry', 'class']],\n",
    "                how='left',\n",
    "                max_distance=10\n",
    "            )\n",
    "            reassigned_matches = reassigned[['accident_id', 'index_right']].copy()\n",
    "            reassigned_matches.columns = ['accident_id', 'road_idx']\n",
    "            reassigned_matches = reassigned_matches.dropna(subset=['road_idx'])\n",
    "            reassigned_matches['road_idx'] = reassigned_matches['road_idx'].astype(int)\n",
    "            matched_accidents = matched_accidents[~matched_accidents['accident_id'].isin(reassigned_matches['accident_id'])]\n",
    "            matched_accidents = pd.concat([matched_accidents, reassigned_matches], ignore_index=True)\n",
    "            logging.info(f\"Reassigned {len(reassigned_matches)} accidents to wider roads\")\n",
    "    \n",
    "    # Count accidents per road\n",
    "    accident_counts = matched_accidents.groupby('road_idx').size().reindex(roads_gdf_local.index, fill_value=0)\n",
    "    roads_gdf_local['accident_count'] = accident_counts\n",
    "    \n",
    "    logging.info(f\"Accidents by road type:\\n{roads_gdf_local.groupby('class')['accident_count'].sum()}\")\n",
    "    \n",
    "    # Filter roads by minimum length\n",
    "    roads_gdf_local = roads_gdf_local[roads_gdf_local['length_m'] >= MIN_ROAD_LENGTH]\n",
    "    \n",
    "    # Log data integrity before calculating accident density\n",
    "    logging.info(f\"length_m stats:\\n{roads_gdf_local['length_m'].describe()}\")\n",
    "    logging.info(f\"accident_count stats:\\n{roads_gdf_local['accident_count'].describe()}\")\n",
    "    \n",
    "    # Calculate accident density\n",
    "    roads_gdf_local['accident_density'] = roads_gdf_local['accident_count'] / (roads_gdf_local['length_m'] / 1000)\n",
    "    roads_gdf_local['accident_density'] = roads_gdf_local['accident_density'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # Adjust density by width rank\n",
    "    roads_gdf_local['accident_density'] = roads_gdf_local['accident_density'] * (roads_gdf_local['width_rank'].replace(0, 1) / 5)\n",
    "    \n",
    "    # Log accident density integrity\n",
    "    logging.info(f\"NaN in accident_density: {roads_gdf_local['accident_density'].isna().sum()}\")\n",
    "    logging.info(f\"accident_density stats:\\n{roads_gdf_local['accident_density'].describe()}\")\n",
    "    \n",
    "    logging.info(f\"Road type counts:\\n{roads_gdf_local['class'].value_counts()}\")\n",
    "    print(f\"Road type counts:\\n{roads_gdf_local['class'].value_counts()}\")\n",
    "    \n",
    "    # Summarize by road type\n",
    "    summary = roads_gdf_local.groupby('class').agg({\n",
    "        'length_m': 'sum',\n",
    "        'accident_count': 'sum',\n",
    "        'accident_density': 'mean',\n",
    "        'width_rank': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    summary = summary[summary['length_m'] > 1000]\n",
    "    summary = summary[summary['width_rank'] > 0]\n",
    "    \n",
    "    print(\"\\n--- Road Type Accident Density Summary ---\")\n",
    "    print(summary[['class', 'length_m', 'accident_count', 'accident_density', 'width_rank']].round(2))\n",
    "    \n",
    "    if len(summary) >= 2:\n",
    "        corr, p_value = spearmanr(summary['width_rank'], summary['accident_density'])\n",
    "        logging.info(f\"Spearman's correlation between road width rank and accident density: {corr:.3f} (p-value: {p_value:.3f})\")\n",
    "        print(f\"Spearman's correlation: {corr:.3f} (p-value: {p_value:.3f})\")\n",
    "    else:\n",
    "        logging.warning(\"Insufficient road types for correlation analysis.\")\n",
    "        print(\"Insufficient road types for correlation analysis.\")\n",
    "    \n",
    "    # Compute average road accident density per neighborhood\n",
    "    logging.info(\"Computing average road accident density per neighborhood...\")\n",
    "    logging.info(f\"Roads DataFrame shape before join: {roads_gdf_local.shape}\")\n",
    "    logging.info(f\"Neighborhoods DataFrame shape before join: {neighborhoods_gdf.shape}\")\n",
    "    \n",
    "    # Fix geometries before join\n",
    "    roads_gdf_with_idx = roads_gdf_local[['geometry', 'class', 'length_m', 'width_rank', 'accident_density']].copy()\n",
    "    roads_gdf_with_idx['geometry'] = roads_gdf_with_idx['geometry'].apply(fix_geometry)\n",
    "    neighborhoods_gdf_with_idx = neighborhoods_gdf[['geometry', 'LIE_NAME']].copy()\n",
    "    neighborhoods_gdf_with_idx['geometry'] = neighborhoods_gdf_with_idx['geometry'].apply(fix_geometry)\n",
    "    \n",
    "    # Buffer roads slightly to capture near-intersections\n",
    "    roads_gdf_with_idx['geometry'] = roads_gdf_with_idx['geometry'].buffer(1)  # 1-meter buffer\n",
    "    \n",
    "    # Perform spatial join\n",
    "    road_neighborhoods = gpd.sjoin(\n",
    "        roads_gdf_with_idx,\n",
    "        neighborhoods_gdf_with_idx,\n",
    "        how='left',\n",
    "        predicate='intersects'\n",
    "    )\n",
    "    logging.info(f\"Road-neighborhood join resulted in {len(road_neighborhoods)} matches with columns: {road_neighborhoods.columns.tolist()}\")\n",
    "    \n",
    "    # Log non-NaN counts\n",
    "    logging.info(f\"Non-NaN LIE_NAME count: {road_neighborhoods['LIE_NAME'].notna().sum()}\")\n",
    "    logging.info(f\"Non-NaN accident_density count: {road_neighborhoods['accident_density'].notna().sum()}\")\n",
    "    logging.info(f\"Unique LIE_NAME values: {road_neighborhoods['LIE_NAME'].nunique()}\")\n",
    "    \n",
    "    # Calculate average accident density\n",
    "    avg_accident_density = road_neighborhoods.groupby('LIE_NAME')['accident_density'].mean()\n",
    "    logging.info(f\"Number of neighborhoods with calculated avg_accident_density: {len(avg_accident_density)}\")\n",
    "    logging.info(f\"NaN in avg_accident_density: {avg_accident_density.isna().sum()}\")\n",
    "    \n",
    "    # Assign to neighborhoods with robust fallback\n",
    "    if avg_accident_density.empty or avg_accident_density.isna().all():\n",
    "        logging.error(\"No valid accident density averages. Assigning 0 to all neighborhoods.\")\n",
    "        neighborhoods_gdf['avg_road_accident_density'] = 0\n",
    "    else:\n",
    "        neighborhoods_gdf['avg_road_accident_density'] = neighborhoods_gdf['LIE_NAME'].map(avg_accident_density).fillna(0)\n",
    "    \n",
    "    assigned_count = sum(~neighborhoods_gdf['avg_road_accident_density'].isna())\n",
    "    logging.info(f\"Assigned avg_road_accident_density to {assigned_count} neighborhoods\")\n",
    "    logging.info(f\"Avg road accident density stats:\\n{neighborhoods_gdf['avg_road_accident_density'].describe()}\")\n",
    "    \n",
    "    # Generate plots\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    summary_sorted = summary.sort_values('width_rank', ascending=False)\n",
    "    sns.barplot(data=summary_sorted, x='class', y='accident_density', hue='width_rank', dodge=False)\n",
    "    plt.xlabel('Road Type')\n",
    "    plt.ylabel('Mean Accident Density (Accidents per km)')\n",
    "    plt.title('Mean Accident Density by Road Type')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Width Rank')\n",
    "    plt.tight_layout()\n",
    "    bar_path = os.path.join(BASE_DIR, 'road_type_accident_bar.png')\n",
    "    plt.savefig(bar_path)\n",
    "    plt.close()\n",
    "    logging.info(f\"Bar chart saved to {bar_path}\")\n",
    "    print(f\"Bar chart saved to {bar_path}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=roads_gdf_local[roads_gdf_local['class'].isin(summary['class'])], \n",
    "                x='class', y='accident_density', hue='width_rank', dodge=False)\n",
    "    plt.xlabel('Road Type')\n",
    "    plt.ylabel('Accident Density (Accidents per km)')\n",
    "    plt.title('Distribution of Accident Density by Road Type')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yscale('log')\n",
    "    plt.legend(title='Width Rank')\n",
    "    plt.tight_layout()\n",
    "    box_path = os.path.join(BASE_DIR, 'road_type_accident_box.png')\n",
    "    plt.savefig(box_path)\n",
    "    plt.close()\n",
    "    logging.info(f\"Box chart saved to {box_path}\")\n",
    "    print(f\"Box chart saved to {box_path}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=summary, x='width_rank', y='accident_density', \n",
    "                    size='length_m', sizes=(50, 500), hue='class', style='class', alpha=0.7)\n",
    "    z = np.polyfit(summary['width_rank'], summary['accident_density'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(summary['width_rank'], p(summary['width_rank']), \"r--\", alpha=0.5)\n",
    "    plt.xlabel('Road Width Rank (1=Path, 5=Motorway)')\n",
    "    plt.ylabel('Mean Accident Density (Accidents per km)')\n",
    "    plt.title('Road Type vs. Accident Density')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    scatter_path = os.path.join(BASE_DIR, 'road_type_accident_scatter.png')\n",
    "    plt.savefig(scatter_path)\n",
    "    plt.close()\n",
    "    logging.info(f\"Scatter plot saved to {scatter_path}\")\n",
    "    print(f\"Scatter plot saved to {scatter_path}\")\n",
    "    \n",
    "    top_types = summary.nlargest(3, 'accident_density')[['class', 'accident_density']]\n",
    "    logging.info(f\"Top 3 road types by accident density:\\n{top_types.round(2)}\")\n",
    "    print(f\"Top 3 road types by accident density:\\n{top_types.round(2)}\")\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07b95a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pedestrian_road_density(roads_gdf, neighborhoods_gdf):\n",
    "    # Filters roads classified as 'footway', 'pedestrian', or 'cycleway'.\n",
    "    # Performs a spatial join to sum road lengths per neighborhood.\n",
    "    # Calculates density as length (m) / area (km²).\n",
    "    \"\"\"Compute pedestrian road density (length of pedestrian roads per km²) for each neighborhood.\"\"\"\n",
    "    logging.info(\"Computing pedestrian road density per neighborhood...\")\n",
    "    \n",
    "    # Filter pedestrian roads (e.g., footway, pedestrian, cycleway)\n",
    "    pedestrian_classes = ['footway', 'pedestrian', 'cycleway']\n",
    "    pedestrian_roads = roads_gdf[roads_gdf['class'].isin(pedestrian_classes)].copy()\n",
    "    \n",
    "    # Fix geometries\n",
    "    pedestrian_roads['geometry'] = pedestrian_roads['geometry'].apply(fix_geometry)\n",
    "    neighborhoods_gdf_with_idx = neighborhoods_gdf[['geometry', 'LIE_NAME', 'area_km2']].copy()\n",
    "    neighborhoods_gdf_with_idx['geometry'] = neighborhoods_gdf_with_idx['geometry'].apply(fix_geometry)\n",
    "    \n",
    "    # Perform spatial join\n",
    "    pedestrian_road_neighborhoods = gpd.sjoin(\n",
    "        pedestrian_roads[['geometry', 'length_m']],\n",
    "        neighborhoods_gdf_with_idx,\n",
    "        how='left',\n",
    "        predicate='intersects'\n",
    "    )\n",
    "    \n",
    "    # Sum pedestrian road lengths per neighborhood\n",
    "    pedestrian_length = pedestrian_road_neighborhoods.groupby('LIE_NAME')['length_m'].sum()\n",
    "    \n",
    "    # Calculate density (length in meters per km²)\n",
    "    neighborhoods_gdf['pedestrian_road_density'] = neighborhoods_gdf['LIE_NAME'].map(pedestrian_length).fillna(0) / (neighborhoods_gdf['area_km2'].replace(0, 1e-6) * 1000)\n",
    "    \n",
    "    logging.info(f\"pedestrian_road_density stats:\\n{neighborhoods_gdf['pedestrian_road_density'].describe()}\")\n",
    "    return neighborhoods_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d0683",
   "metadata": {},
   "source": [
    "Cell 4 Main Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3d3ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    logging.info(\"Stage 1: Loading and preparing data...\")\n",
    "    \n",
    "    # Define file paths and their corresponding keys\n",
    "    data_files = {\n",
    "        'neighborhoods': LANDUSE_NDVI_PATH,\n",
    "        'buildings': OSM_BUILDINGS_PATH,\n",
    "        'roads': OSM_ROADS_PATH,\n",
    "        'trees': OSM_TREES_PATH,\n",
    "        'transit': OSM_TRANSIT_PATH,\n",
    "        'urban_masterplan': URBAN_MASTERPLAN_PATH,\n",
    "        'accidents': ACCIDENTS_PATH,\n",
    "        'population': POPULATION_PATH\n",
    "    }\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # Load data with progress bar\n",
    "    for key, path in tqdm(data_files.items(), desc=\"Loading files\"):\n",
    "        try:\n",
    "            if key == 'population':\n",
    "                with open(path, 'r') as f:\n",
    "                    data[key] = pd.DataFrame(json.load(f))\n",
    "                logging.info(f\"Columns in population_df after loading: {list(data[key].columns)}\")\n",
    "            elif path.endswith('.geoparquet'):\n",
    "                data[key] = gpd.read_parquet(path)\n",
    "            else:\n",
    "                data[key] = gpd.read_file(path)\n",
    "            logging.info(f\"Loaded {key} with shape {data[key].shape}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load {key} from {path}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Log columns of neighborhoods_gdf to debug missing 'ndvi' and 'area_km2'\n",
    "    neighborhoods_gdf = data['neighborhoods']\n",
    "    logging.info(f\"Columns in neighborhoods_gdf after loading: {list(neighborhoods_gdf.columns)}\")\n",
    "    \n",
    "    # Check for alternative NDVI column names and rename if found\n",
    "    possible_ndvi_columns = ['ndvi_mean', 'NDVI', 'ndvi_value']\n",
    "    for col in possible_ndvi_columns:\n",
    "        if col in neighborhoods_gdf.columns and 'ndvi' not in neighborhoods_gdf.columns:\n",
    "            logging.info(f\"Found alternative NDVI column '{col}'. Renaming to 'ndvi'.\")\n",
    "            neighborhoods_gdf['ndvi'] = neighborhoods_gdf[col]\n",
    "            break\n",
    "    \n",
    "    # Ensure all GeoDataFrames are in the same CRS\n",
    "    target_crs = 'EPSG:3826'\n",
    "    for key in ['neighborhoods', 'buildings', 'roads', 'trees', 'transit', 'urban_masterplan', 'accidents']:\n",
    "        if key in data and isinstance(data[key], gpd.GeoDataFrame):\n",
    "            if data[key].crs is None:\n",
    "                logging.warning(f\"No CRS defined for {key}. Assuming EPSG:4326.\")\n",
    "                data[key].set_crs('EPSG:4326', inplace=True)\n",
    "            if data[key].crs != target_crs:\n",
    "                data[key] = data[key].to_crs(target_crs)\n",
    "                logging.info(f\"Converted {key} to CRS {target_crs}\")\n",
    "    \n",
    "    # Fix geometries in all GeoDataFrames\n",
    "    for key in ['neighborhoods', 'buildings', 'roads', 'trees', 'transit', 'urban_masterplan', 'accidents']:\n",
    "        if key in data and isinstance(data[key], gpd.GeoDataFrame):\n",
    "            data[key]['geometry'] = data[key]['geometry'].apply(fix_geometry)\n",
    "            invalid_geoms = data[key][~data[key].geometry.is_valid]\n",
    "            if not invalid_geoms.empty:\n",
    "                logging.warning(f\"Found {len(invalid_geoms)} invalid geometries in {key} after fixing.\")\n",
    "                data[key] = data[key][data[key].geometry.is_valid]\n",
    "    \n",
    "    # Compute intersections for neighborhoods\n",
    "    logging.info(\"Computing intersections for neighborhoods...\")\n",
    "    roads_gdf = data['roads']\n",
    "    \n",
    "    # Log columns of roads_gdf to debug missing 'length_m'\n",
    "    logging.info(f\"Columns in roads_gdf after loading: {list(roads_gdf.columns)}\")\n",
    "    \n",
    "    # Extract endpoints from road segments\n",
    "    logging.info(\"Extracting endpoints from road segments...\")\n",
    "    endpoints = []\n",
    "    road_indices = []\n",
    "    for idx, row in tqdm(roads_gdf.iterrows(), total=len(roads_gdf), desc=\"Extracting endpoints\"):\n",
    "        geom = row['geometry']\n",
    "        if geom.geom_type == 'LineString':\n",
    "            coords = list(geom.coords)\n",
    "            start_point = Point(coords[0])\n",
    "            end_point = Point(coords[-1])\n",
    "            if start_point.is_valid and end_point.is_valid:\n",
    "                endpoints.extend([start_point, end_point])\n",
    "                road_indices.extend([idx, idx])\n",
    "        elif geom.geom_type == 'MultiLineString':\n",
    "            for line in geom.geoms:\n",
    "                coords = list(line.coords)\n",
    "                start_point = Point(coords[0])\n",
    "                end_point = Point(coords[-1])\n",
    "                if start_point.is_valid and end_point.is_valid:\n",
    "                    endpoints.extend([start_point, end_point])\n",
    "                    road_indices.extend([idx, idx])\n",
    "    \n",
    "    if not endpoints:\n",
    "        logging.warning(\"No valid endpoints extracted from road segments. Using fallback method for intersections.\")\n",
    "        road_neighborhoods = gpd.sjoin(roads_gdf[['geometry']], neighborhoods_gdf[['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "        intersection_counts = road_neighborhoods.groupby('index_right').size()\n",
    "        neighborhoods_gdf['intersection_count'] = intersection_counts.reindex(neighborhoods_gdf.index, fill_value=0)\n",
    "    else:\n",
    "        endpoints_gdf = gpd.GeoDataFrame({'geometry': endpoints, 'road_idx': road_indices}, crs=target_crs)\n",
    "        \n",
    "        # Create a spatial index for endpoints\n",
    "        endpoints_sindex = endpoints_gdf.sindex\n",
    "        \n",
    "        # Cluster endpoints to identify intersections (points shared by 3 or more roads)\n",
    "        logging.info(\"Building endpoint-to-road mapping...\")\n",
    "        endpoint_to_roads = {}\n",
    "        for idx, point in tqdm(endpoints_gdf.iterrows(), total=len(endpoints_gdf), desc=\"Building endpoint-to-road mapping\"):\n",
    "            point_geom = point['geometry']\n",
    "            road_idx = point['road_idx']\n",
    "            point_key = (round(point_geom.x, 6), round(point_geom.y, 6))  # Round to avoid floating-point precision issues\n",
    "            if point_key not in endpoint_to_roads:\n",
    "                endpoint_to_roads[point_key] = set()\n",
    "            endpoint_to_roads[point_key].add(road_idx)\n",
    "        \n",
    "        logging.info(\"Identifying intersections...\")\n",
    "        intersections = []\n",
    "        for point_key, road_ids in tqdm(endpoint_to_roads.items(), desc=\"Identifying intersections\"):\n",
    "            if len(road_ids) >= 3:  # Intersection if shared by 3 or more roads\n",
    "                intersections.append(Point(point_key))\n",
    "        \n",
    "        if not intersections:\n",
    "            logging.warning(\"No intersections found using endpoint clustering. Using fallback method.\")\n",
    "            road_neighborhoods = gpd.sjoin(roads_gdf[['geometry']], neighborhoods_gdf[['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "            intersection_counts = road_neighborhoods.groupby('index_right').size()\n",
    "            neighborhoods_gdf['intersection_count'] = intersection_counts.reindex(neighborhoods_gdf.index, fill_value=0)\n",
    "        else:\n",
    "            intersections_gdf = gpd.GeoDataFrame({'geometry': intersections}, crs=target_crs)\n",
    "            \n",
    "            # Count intersections per neighborhood\n",
    "            logging.info(\"Counting intersections per neighborhood...\")\n",
    "            intersections_joined = gpd.sjoin(intersections_gdf, neighborhoods_gdf[['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "            intersection_counts = intersections_joined.groupby('index_right').size()\n",
    "            neighborhoods_gdf['intersection_count'] = intersection_counts.reindex(neighborhoods_gdf.index, fill_value=0)\n",
    "    \n",
    "    # Compute or verify area_km2\n",
    "    if 'area_km2' not in neighborhoods_gdf.columns:\n",
    "        logging.warning(\"'area_km2' column missing in neighborhoods_gdf. Computing from geometry...\")\n",
    "        neighborhoods_gdf['area_m2'] = neighborhoods_gdf['geometry'].area\n",
    "        neighborhoods_gdf['area_km2'] = neighborhoods_gdf['area_m2'] / 1_000_000  # Convert m² to km²\n",
    "        logging.info(f\"Computed area_km2 stats:\\n{neighborhoods_gdf['area_km2'].describe()}\")\n",
    "    else:\n",
    "        logging.info(f\"area_km2 already present. Stats:\\n{neighborhoods_gdf['area_km2'].describe()}\")\n",
    "    \n",
    "    # Compute intersection density\n",
    "    neighborhoods_gdf['intersection_density'] = neighborhoods_gdf['intersection_count'] / neighborhoods_gdf['area_km2'].replace(0, 1e-6)\n",
    "    logging.info(f\"Intersection count stats:\\n{neighborhoods_gdf['intersection_count'].describe()}\")\n",
    "    logging.info(f\"Intersection density stats:\\n{neighborhoods_gdf['intersection_density'].describe()}\")\n",
    "    \n",
    "    # Cache the result\n",
    "    try:\n",
    "        neighborhoods_gdf.to_parquet(INTERSECTION_CACHE_PATH)\n",
    "        logging.info(f\"Saved neighborhoods with intersections to {INTERSECTION_CACHE_PATH}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save neighborhoods with intersections: {e}\")\n",
    "    \n",
    "    data['neighborhoods'] = neighborhoods_gdf\n",
    "    \n",
    "    # Compute tree count per neighborhood\n",
    "    logging.info(\"Computing tree count per neighborhood...\")\n",
    "    trees_gdf = data['trees']\n",
    "    trees_joined = gpd.sjoin(trees_gdf[['geometry']], data['neighborhoods'][['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "    tree_counts = trees_joined.groupby('index_right').size()\n",
    "    data['neighborhoods']['tree_count'] = tree_counts.reindex(data['neighborhoods'].index, fill_value=0)\n",
    "    \n",
    "    # Compute transit count per neighborhood\n",
    "    logging.info(\"Computing transit count per neighborhood...\")\n",
    "    transit_gdf = data['transit']\n",
    "    transit_joined = gpd.sjoin(transit_gdf[['geometry']], data['neighborhoods'][['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "    transit_counts = transit_joined.groupby('index_right').size()\n",
    "    data['neighborhoods']['transit_count'] = transit_counts.reindex(data['neighborhoods'].index, fill_value=0)\n",
    "    \n",
    "    # Compute accident count per neighborhood\n",
    "    logging.info(\"Computing accident count per neighborhood...\")\n",
    "    accidents_gdf = data['accidents']\n",
    "    accidents_buffered = accidents_gdf.copy()\n",
    "    accidents_buffered['geometry'] = accidents_buffered['geometry'].buffer(BUFFER_DISTANCE)\n",
    "    accidents_joined = gpd.sjoin(accidents_buffered[['geometry']], data['neighborhoods'][['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "    accident_counts = accidents_joined.groupby('index_right').size()\n",
    "    data['neighborhoods']['accident_count'] = accident_counts.reindex(data['neighborhoods'].index, fill_value=0)\n",
    "    \n",
    "    # Compute road density per neighborhood\n",
    "    logging.info(\"Computing road density per neighborhood...\")\n",
    "    roads_gdf = data['roads']\n",
    "    \n",
    "    # Log columns of roads_gdf to debug missing 'length_m'\n",
    "    logging.info(f\"Columns in roads_gdf before computing road density: {list(roads_gdf.columns)}\")\n",
    "    \n",
    "    # Compute length_m if missing\n",
    "    if 'length_m' not in roads_gdf.columns:\n",
    "        logging.warning(\"'length_m' column missing in roads_gdf. Computing from geometry...\")\n",
    "        roads_gdf['length_m'] = roads_gdf['geometry'].length  # Length in meters (since CRS is EPSG:3826)\n",
    "        logging.info(f\"Computed length_m stats:\\n{roads_gdf['length_m'].describe()}\")\n",
    "    \n",
    "    roads_joined = gpd.sjoin(roads_gdf[['geometry', 'length_m']], data['neighborhoods'][['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "    road_lengths = roads_joined.groupby('index_right')['length_m'].sum()\n",
    "    data['neighborhoods']['road_density'] = road_lengths.reindex(data['neighborhoods'].index, fill_value=0) / (data['neighborhoods']['area_km2'] * 1000)\n",
    "    logging.info(f\"Road density stats:\\n{data['neighborhoods']['road_density'].describe()}\")\n",
    "    \n",
    "    # Merge population data\n",
    "    logging.info(\"Merging population data...\")\n",
    "    population_df = data['population']\n",
    "    population_df['LIE_NAME'] = population_df['LIE_NAME'].astype(str).str.strip()\n",
    "    data['neighborhoods']['LIE_NAME'] = data['neighborhoods']['LIE_NAME'].astype(str).str.strip()\n",
    "    \n",
    "    # Check for possible column names for total_population and elderly_percentage\n",
    "    expected_cols = ['total_population', 'elderly_percentage']\n",
    "    population_cols = list(population_df.columns)\n",
    "    missing_cols = [col for col in expected_cols if col not in population_cols]\n",
    "    \n",
    "    if missing_cols:\n",
    "        logging.warning(f\"Expected columns {missing_cols} not found in population_df. Attempting to find alternatives...\")\n",
    "        total_pop_alt = None\n",
    "        elderly_alt = None\n",
    "        for col in population_cols:\n",
    "            col_lower = col.lower()\n",
    "            if 'population' in col_lower and total_pop_alt is None:\n",
    "                total_pop_alt = col\n",
    "                logging.info(f\"Found alternative for total_population: {col}\")\n",
    "            if 'elderly' in col_lower and elderly_alt is None:\n",
    "                elderly_alt = col\n",
    "                logging.info(f\"Found alternative for elderly_percentage: {col}\")\n",
    "        \n",
    "        # Rename columns if alternatives are found\n",
    "        if total_pop_alt:\n",
    "            population_df = population_df.rename(columns={total_pop_alt: 'total_population'})\n",
    "        else:\n",
    "            logging.warning(\"No alternative found for total_population. Setting to 0.\")\n",
    "            population_df['total_population'] = 0\n",
    "        if elderly_alt:\n",
    "            population_df = population_df.rename(columns={elderly_alt: 'elderly_percentage'})\n",
    "        else:\n",
    "            logging.warning(\"No alternative found for elderly_percentage. Setting to 0.\")\n",
    "            population_df['elderly_percentage'] = 0\n",
    "    \n",
    "    # Perform the merge\n",
    "    data['neighborhoods'] = data['neighborhoods'].merge(\n",
    "        population_df[['LIE_NAME', 'total_population', 'elderly_percentage']],\n",
    "        on='LIE_NAME',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Compute land use percentages\n",
    "    logging.info(\"Computing land use percentages for neighborhoods...\")\n",
    "    urban_masterplan_gdf = data['urban_masterplan']\n",
    "    print_percentage_calculation(data['neighborhoods'], urban_masterplan_gdf, sample_size=3)\n",
    "    \n",
    "    for idx, row in data['neighborhoods'].iterrows():\n",
    "        neighborhood_geom = row['geometry']\n",
    "        if not neighborhood_geom.is_valid:\n",
    "            continue\n",
    "        relevant_masterplan = urban_masterplan_gdf[urban_masterplan_gdf.intersects(neighborhood_geom)]\n",
    "        if relevant_masterplan.empty:\n",
    "            continue\n",
    "        \n",
    "        temp_gdf = gpd.GeoDataFrame({'geometry': [neighborhood_geom]}, crs='EPSG:3826')\n",
    "        intersected = gpd.overlay(temp_gdf, relevant_masterplan, how='intersection', keep_geom_type=False)\n",
    "        if intersected.empty:\n",
    "            continue\n",
    "        \n",
    "        intersected['geometry'] = intersected['geometry'].apply(fix_geometry)\n",
    "        intersected = intersected[intersected.geometry.is_valid & ~intersected.geometry.is_empty]\n",
    "        if intersected.empty:\n",
    "            continue\n",
    "        \n",
    "        intersected['priority'] = intersected['Category'].map(CATEGORY_PRIORITY)\n",
    "        intersected = intersected.sort_values(by='priority', ascending=False)\n",
    "        \n",
    "        total_area = intersected.geometry.union_all().area\n",
    "        remaining_geom = intersected.geometry.union_all()\n",
    "        category_areas = {}\n",
    "        for category in intersected['Category'].unique():\n",
    "            category_rows = intersected[intersected['Category'] == category]\n",
    "            category_geom = category_rows.geometry.union_all()\n",
    "            try:\n",
    "                category_area_geom = category_geom.intersection(remaining_geom)\n",
    "                category_area = category_area_geom.area\n",
    "                category_areas[category] = category_area\n",
    "                remaining_geom = remaining_geom.difference(category_area_geom)\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Topology error for category {category} in neighborhood {row['LIE_NAME']}: {e}\")\n",
    "                category_areas[category] = 0.0\n",
    "        \n",
    "        for category in CATEGORY_PRIORITY.keys():\n",
    "            area = category_areas.get(category, 0.0)\n",
    "            percentage = (area / total_area * 100) if total_area > 0 else 0.0\n",
    "            data['neighborhoods'].at[idx, f'land_use_{category.lower()}_percent'] = percentage\n",
    "    \n",
    "    # Fill NaN values in land use percentages\n",
    "    for category in CATEGORY_PRIORITY.keys():\n",
    "        col = f'land_use_{category.lower()}_percent'\n",
    "        data['neighborhoods'][col] = data['neighborhoods'][col].fillna(0.0)\n",
    "    \n",
    "    # Fill NaN values in other columns\n",
    "    for col in ['intersection_count', 'intersection_density', 'tree_count', 'transit_count', 'accident_count', 'road_density', 'total_population', 'elderly_percentage']:\n",
    "        data['neighborhoods'][col] = data['neighborhoods'][col].fillna(0)\n",
    "    \n",
    "    # Print data structure summary\n",
    "    print_data_structure(data)\n",
    "    \n",
    "    logging.info(\"Finished loading and preparing data.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48993e82",
   "metadata": {},
   "source": [
    "Cell 5 compute_intersection_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2d1971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intersection_counts(neighborhoods_gdf, roads_gdf):\n",
    "    logging.info(\"Computing intersection counts for neighborhoods...\")\n",
    "    \n",
    "    # Extract endpoints from road segments\n",
    "    def get_endpoints(line):\n",
    "        if line is None or line.is_empty:\n",
    "            return []\n",
    "        coords = list(line.coords)\n",
    "        return [Point(coords[0]), Point(coords[-1])]\n",
    "    \n",
    "    endpoints = []\n",
    "    for idx, row in tqdm(roads_gdf.iterrows(), total=len(roads_gdf), desc=\"Extracting endpoints\"):\n",
    "        points = get_endpoints(row['geometry'])\n",
    "        for point in points:\n",
    "            endpoints.append({'geometry': point, 'road_idx': idx})\n",
    "    \n",
    "    endpoints_gdf = gpd.GeoDataFrame(endpoints, crs='EPSG:3826')\n",
    "    \n",
    "    # Build a mapping of endpoints to road indices\n",
    "    endpoint_to_roads = {}\n",
    "    for idx, row in tqdm(endpoints_gdf.iterrows(), total=len(endpoints_gdf), desc=\"Building endpoint-to-road mapping\"):\n",
    "        point = row['geometry']\n",
    "        road_idx = row['road_idx']\n",
    "        point_tuple = (point.x, point.y)\n",
    "        if point_tuple not in endpoint_to_roads:\n",
    "            endpoint_to_roads[point_tuple] = set()\n",
    "        endpoint_to_roads[point_tuple].add(road_idx)\n",
    "    \n",
    "    # Identify intersections (endpoints shared by 3 or more roads)\n",
    "    intersections = []\n",
    "    for point_tuple, road_indices in tqdm(endpoint_to_roads.items(), desc=\"Identifying intersections\"):\n",
    "        if len(road_indices) >= 3:  # Intersection if 3 or more roads share the endpoint\n",
    "            intersections.append({'geometry': Point(point_tuple)})\n",
    "    \n",
    "    if not intersections:\n",
    "        logging.warning(\"No intersections found. Setting intersection counts to 0.\")\n",
    "        neighborhoods_gdf['intersection_count'] = 0\n",
    "        neighborhoods_gdf['intersection_density'] = 0.0\n",
    "        return neighborhoods_gdf\n",
    "    \n",
    "    intersections_gdf = gpd.GeoDataFrame(intersections, crs='EPSG:3826')\n",
    "    \n",
    "    # Spatial join to count intersections per neighborhood\n",
    "    intersection_counts = gpd.sjoin(\n",
    "        neighborhoods_gdf[['geometry', 'LIE_NAME']],\n",
    "        intersections_gdf,\n",
    "        how='left',\n",
    "        predicate='contains'\n",
    "    )\n",
    "    intersection_counts = intersection_counts.groupby('LIE_NAME').size().reindex(neighborhoods_gdf['LIE_NAME'], fill_value=0)\n",
    "    neighborhoods_gdf['intersection_count'] = intersection_counts\n",
    "    \n",
    "    # Compute intersection density (intersections per km²)\n",
    "    neighborhoods_gdf['intersection_density'] = neighborhoods_gdf['intersection_count'] / neighborhoods_gdf['area_km2']\n",
    "    neighborhoods_gdf['intersection_density'] = neighborhoods_gdf['intersection_density'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    logging.info(f\"Intersection count stats: {neighborhoods_gdf['intersection_count'].describe()}\")\n",
    "    logging.info(f\"Intersection density stats: {neighborhoods_gdf['intersection_density'].describe()}\")\n",
    "    \n",
    "    return neighborhoods_gdf\n",
    "\n",
    "def build_graph(data, force_recompute=False):\n",
    "    logging.info(\"Stage 2: Building the graph...\")\n",
    "    \n",
    "    # Compute data hash to check if graph needs recomputing\n",
    "    data_hash = compute_data_hash(data)\n",
    "    cached_hash = None\n",
    "    if os.path.exists(GRAPH_DATA_HASH_PATH):\n",
    "        with open(GRAPH_DATA_HASH_PATH, 'r') as f:\n",
    "            cached_hash = f.read().strip()\n",
    "    \n",
    "    if not force_recompute and cached_hash == data_hash and all(\n",
    "        os.path.exists(path) for path in [GRAPH_NODES_CACHE_PATH, GRAPH_EDGES_CACHE_PATH, GRAPH_NODE_ID_CACHE_PATH]\n",
    "    ):\n",
    "        logging.info(\"Data unchanged. Loading graph from cache...\")\n",
    "        nodes_df = cudf.read_parquet(GRAPH_NODES_CACHE_PATH)\n",
    "        edges_df = cudf.read_parquet(GRAPH_EDGES_CACHE_PATH)\n",
    "        with open(GRAPH_NODE_ID_CACHE_PATH, 'r') as f:\n",
    "            node_id_to_index = json.load(f)\n",
    "        G = cugraph.Graph()\n",
    "        G.from_cudf_edgelist(\n",
    "            edges_df,\n",
    "            source='src',\n",
    "            destination='dst',\n",
    "            edge_attr='weight'\n",
    "        )\n",
    "        G._nodes = nodes_df\n",
    "        logging.info(\"Graph loaded from cache.\")\n",
    "        return G\n",
    "    \n",
    "    neighborhoods_gdf = data['neighborhoods'].copy()\n",
    "    buildings_gdf = data['buildings'].copy()\n",
    "    roads_gdf = data['roads'].copy()\n",
    "    trees_gdf = data['trees'].copy()\n",
    "    transit_gdf = data['transit'].copy()\n",
    "    \n",
    "    # Create nodes for neighborhoods, buildings, roads, trees, and transit\n",
    "    nodes = []\n",
    "    node_id_to_index = {}\n",
    "    current_idx = 0\n",
    "    \n",
    "    # Neighborhood nodes\n",
    "    for idx, row in neighborhoods_gdf.iterrows():\n",
    "        node_id = f\"neighborhood_{row['LIE_NAME']}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'neighborhood',\n",
    "            'LIE_NAME': row['LIE_NAME'],\n",
    "            'geometry': row['geometry'].centroid,\n",
    "            'ndvi_mean': row['ndvi_mean'],\n",
    "            'total_population': row['total_population'],\n",
    "            'elderly_percentage': row['elderly_percentage'],\n",
    "            'area_km2': row['area_km2']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    # Building nodes\n",
    "    for idx, row in buildings_gdf.iterrows():\n",
    "        node_id = f\"building_{idx}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'building',\n",
    "            'LIE_NAME': None,\n",
    "            'geometry': row['geometry'].centroid,\n",
    "            'building_type': row['building'],\n",
    "            'area_m2': row['area_m2']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    # Road nodes\n",
    "    for idx, row in roads_gdf.iterrows():\n",
    "        node_id = f\"road_{idx}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'road',\n",
    "            'LIE_NAME': None,\n",
    "            'geometry': row['geometry'].centroid,\n",
    "            'class': row['class'],\n",
    "            'length_m': row['length_m']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    # Tree nodes\n",
    "    for idx, row in trees_gdf.iterrows():\n",
    "        node_id = f\"tree_{idx}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'tree',\n",
    "            'LIE_NAME': None,\n",
    "            'geometry': row['geometry']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    # Transit nodes\n",
    "    for idx, row in transit_gdf.iterrows():\n",
    "        node_id = f\"transit_{idx}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'transit',\n",
    "            'LIE_NAME': None,\n",
    "            'geometry': row['geometry'],\n",
    "            'class': row['class']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    nodes_df = pd.DataFrame(nodes)\n",
    "    nodes_gdf = gpd.GeoDataFrame(nodes_df, geometry='geometry', crs='EPSG:3826')\n",
    "    nodes_df = cudf.from_pandas(nodes_df.drop(columns=['geometry']))\n",
    "    \n",
    "    # Create edges based on spatial proximity\n",
    "    edges = []\n",
    "    nodes_gdf_sindex = nodes_gdf.sindex\n",
    "    \n",
    "    # Neighborhood-to-neighborhood edges (shared borders)\n",
    "    logging.info(\"Creating neighborhood-to-neighborhood edges...\")\n",
    "    for idx1, row1 in neighborhoods_gdf.iterrows():\n",
    "        geom1 = row1['geometry']\n",
    "        node_idx1 = node_id_to_index[f\"neighborhood_{row1['LIE_NAME']}\"]\n",
    "        possible_matches = list(nodes_gdf_sindex.query(geom1, predicate='intersects'))\n",
    "        for idx2 in possible_matches:\n",
    "            row2 = nodes_gdf.iloc[idx2]\n",
    "            if row2['type'] != 'neighborhood':\n",
    "                continue\n",
    "            if row1['LIE_NAME'] == row2['LIE_NAME']:\n",
    "                continue\n",
    "            geom2 = neighborhoods_gdf[neighborhoods_gdf['LIE_NAME'] == row2['LIE_NAME']]['geometry'].iloc[0]\n",
    "            if geom1.intersects(geom2):\n",
    "                node_idx2 = node_id_to_index[f\"neighborhood_{row2['LIE_NAME']}\"]\n",
    "                edges.append({\n",
    "                    'src': node_idx1,\n",
    "                    'dst': node_idx2,\n",
    "                    'weight': 1.0\n",
    "                })\n",
    "    \n",
    "    # Other edges (neighborhood to building, road, tree, transit)\n",
    "    logging.info(\"Creating edges between neighborhoods and other entities...\")\n",
    "    for idx, row in tqdm(nodes_gdf.iterrows(), total=len(nodes_gdf), desc=\"Creating edges\"):\n",
    "        if row['type'] == 'neighborhood':\n",
    "            continue\n",
    "        geom = row['geometry']\n",
    "        possible_matches = list(neighborhoods_gdf.sindex.query(geom, predicate='contains'))\n",
    "        for match_idx in possible_matches:\n",
    "            neighborhood = neighborhoods_gdf.iloc[match_idx]\n",
    "            if neighborhood['geometry'].contains(geom):\n",
    "                node_idx1 = node_id_to_index[row['node_id']]\n",
    "                node_idx2 = node_id_to_index[f\"neighborhood_{neighborhood['LIE_NAME']}\"]\n",
    "                weight = 1.0\n",
    "                if row['type'] == 'transit':\n",
    "                    weight = 2.0  # Higher weight for transit nodes\n",
    "                edges.append({\n",
    "                    'src': node_idx1,\n",
    "                    'dst': node_idx2,\n",
    "                    'weight': weight\n",
    "                })\n",
    "                edges.append({\n",
    "                    'src': node_idx2,\n",
    "                    'dst': node_idx1,\n",
    "                    'weight': weight\n",
    "                })\n",
    "    \n",
    "    edges_df = cudf.DataFrame(edges)\n",
    "    \n",
    "    # Build the graph\n",
    "    G = cugraph.Graph()\n",
    "    G.from_cudf_edgelist(\n",
    "        edges_df,\n",
    "        source='src',\n",
    "        destination='dst',\n",
    "        edge_attr='weight'\n",
    "    )\n",
    "    G._nodes = nodes_df\n",
    "    \n",
    "    # Cache the graph\n",
    "    nodes_df.to_parquet(GRAPH_NODES_CACHE_PATH)\n",
    "    edges_df.to_parquet(GRAPH_EDGES_CACHE_PATH)\n",
    "    with open(GRAPH_NODE_ID_CACHE_PATH, 'w') as f:\n",
    "        json.dump(node_id_to_index, f)\n",
    "    with open(GRAPH_DATA_HASH_PATH, 'w') as f:\n",
    "        f.write(data_hash)\n",
    "    \n",
    "    logging.info(\"Graph construction completed.\")\n",
    "    return G\n",
    "\n",
    "def prepare_gnn_data(G):\n",
    "    logging.info(\"Stage 3: Preparing data for GNN...\")\n",
    "    \n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    edges_df = G.edgelist.edgelist_df.to_pandas()\n",
    "    \n",
    "    # Create node features\n",
    "    feature_columns = [\n",
    "        'ndvi_mean', 'total_population', 'elderly_percentage', 'area_km2',\n",
    "        'area_m2', 'length_m'\n",
    "    ]\n",
    "    features = []\n",
    "    for idx, row in nodes_df.iterrows():\n",
    "        node_features = []\n",
    "        for col in feature_columns:\n",
    "            value = row.get(col, 0.0)\n",
    "            if pd.isna(value):\n",
    "                value = 0.0\n",
    "            node_features.append(value)\n",
    "        \n",
    "        # One-hot encode node type\n",
    "        node_type = row['type']\n",
    "        type_encoding = [0] * 5  # 5 types: neighborhood, building, road, tree, transit\n",
    "        type_mapping = {\n",
    "            'neighborhood': 0,\n",
    "            'building': 1,\n",
    "            'road': 2,\n",
    "            'tree': 3,\n",
    "            'transit': 4\n",
    "        }\n",
    "        type_idx = type_mapping.get(node_type, 0)\n",
    "        type_encoding[type_idx] = 1\n",
    "        node_features.extend(type_encoding)\n",
    "        \n",
    "        features.append(node_features)\n",
    "    \n",
    "    feature_matrix = np.array(features, dtype=np.float32)\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    numerical_features = feature_matrix[:, :len(feature_columns)]\n",
    "    means = numerical_features.mean(axis=0)\n",
    "    stds = numerical_features.std(axis=0)\n",
    "    stds[stds == 0] = 1  # Avoid division by zero\n",
    "    numerical_features = (numerical_features - means) / stds\n",
    "    feature_matrix[:, :len(feature_columns)] = numerical_features\n",
    "    \n",
    "    # Create edge indices for PyG\n",
    "    edge_index = torch.tensor(\n",
    "        np.array([edges_df['src'].values, edges_df['dst'].values]),\n",
    "        dtype=torch.long\n",
    "    )\n",
    "    edge_attr = torch.tensor(edges_df['weight'].values, dtype=torch.float)\n",
    "    \n",
    "    # Create target (walkability score) for neighborhood nodes\n",
    "    y = np.zeros(len(nodes_df), dtype=np.float32)\n",
    "    if 'walkability_score' in nodes_df.columns:\n",
    "        walkability_scores = nodes_df['walkability_score'].fillna(0).values\n",
    "        mask = nodes_df['type'] == 'neighborhood'\n",
    "        y[mask] = walkability_scores[mask]\n",
    "    else:\n",
    "        logging.warning(\"Walkability scores not found in nodes_df. Setting targets to 0.\")\n",
    "    \n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    \n",
    "    node_type_mapping = {\n",
    "        'neighborhood': 0,\n",
    "        'building': 1,\n",
    "        'road': 2,\n",
    "        'tree': 3,\n",
    "        'transit': 4\n",
    "    }\n",
    "    node_type = nodes_df['type'].map(node_type_mapping).fillna(-1).astype(int).values\n",
    "    node_type = torch.tensor(node_type, dtype=torch.long)\n",
    "    \n",
    "    data = Data(\n",
    "        x=torch.tensor(feature_matrix, dtype=torch.float),\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=y,\n",
    "        node_type=node_type\n",
    "    )\n",
    "    \n",
    "    logging.info(\"GNN data prepared.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b04d9d",
   "metadata": {},
   "source": [
    "Cell 6: Graph Construction (build_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4158e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_neighborhood_neighborhood_edges(args):\n",
    "    idx, row, neighborhoods_gdf, neighborhood_sindex = args\n",
    "    edges = []\n",
    "    geom = row['geometry']\n",
    "    possible_matches_index = list(neighborhood_sindex.intersection(geom.bounds))\n",
    "    for other_idx in possible_matches_index:\n",
    "        if other_idx != idx:\n",
    "            other_row = neighborhoods_gdf.iloc[other_idx]\n",
    "            other_geom = other_row['geometry']\n",
    "            try:\n",
    "                if geom.buffer(1e-3).intersects(other_geom.buffer(1e-3)) or geom.buffer(1e-3).touches(other_geom.buffer(1e-3)):\n",
    "                    src = f\"neighborhood_{idx}\"\n",
    "                    dst = f\"neighborhood_{other_idx}\"\n",
    "                    edges.append({'src': src, 'dst': dst})\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error checking intersection between neighborhood {idx} and {other_idx}: {e}\")\n",
    "    return edges\n",
    "\n",
    "def compute_neighborhood_building_edges(args):\n",
    "    idx, row, buildings_gdf, building_sindex = args\n",
    "    edges = []\n",
    "    geom = row['geometry']\n",
    "    possible_matches_index = list(building_sindex.intersection(geom.bounds))\n",
    "    for building_idx in possible_matches_index:\n",
    "        building_row = buildings_gdf.iloc[building_idx]\n",
    "        building_geom = building_row['geometry']\n",
    "        try:\n",
    "            if geom.buffer(1e-3).intersects(building_geom.buffer(1e-3)):\n",
    "                src = f\"neighborhood_{idx}\"\n",
    "                dst = f\"building_{building_idx}\"\n",
    "                edges.append({'src': src, 'dst': dst})\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error checking intersection between neighborhood {idx} and building {building_idx}: {e}\")\n",
    "    return edges\n",
    "\n",
    "def compute_neighborhood_road_edges(args):\n",
    "    idx, row, roads_gdf, road_sindex = args\n",
    "    edges = []\n",
    "    geom = row['geometry']\n",
    "    possible_matches_index = list(road_sindex.intersection(geom.bounds))\n",
    "    for road_idx in possible_matches_index:\n",
    "        road_row = roads_gdf.iloc[road_idx]\n",
    "        road_geom = road_row['geometry']\n",
    "        try:\n",
    "            if geom.buffer(1e-3).intersects(road_geom.buffer(1e-3)):\n",
    "                src = f\"neighborhood_{idx}\"\n",
    "                dst = f\"road_{road_idx}\"\n",
    "                edges.append({'src': src, 'dst': dst})\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error checking intersection between neighborhood {idx} and road {road_idx}: {e}\")\n",
    "    return edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cdc1557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(data, force_recompute=False):\n",
    "    import cudf\n",
    "    import cugraph\n",
    "    import logging\n",
    "    import json\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    logging.info(\"Stage 2: Building city graph...\")\n",
    "    \n",
    "    # Compute data hash to check if cached graph can be used\n",
    "    current_hash = compute_data_hash(data)\n",
    "    if not force_recompute and os.path.exists(GRAPH_NODES_CACHE_PATH) and os.path.exists(GRAPH_DATA_HASH_PATH):\n",
    "        with open(GRAPH_DATA_HASH_PATH, 'r') as f:\n",
    "            cached_hash = f.read()\n",
    "        if cached_hash == current_hash:\n",
    "            try:\n",
    "                nodes_df = cudf.read_parquet(GRAPH_NODES_CACHE_PATH)\n",
    "                edges_df = cudf.read_parquet(GRAPH_EDGES_CACHE_PATH)\n",
    "                with open(GRAPH_NODE_ID_CACHE_PATH, 'r') as f:\n",
    "                    node_id_to_vertex = json.load(f)\n",
    "                G = cugraph.Graph()\n",
    "                G._nodes = nodes_df\n",
    "                if not edges_df.empty:\n",
    "                    G.from_cudf_edgelist(edges_df, source='src', destination='dst')\n",
    "                logging.info(f\"Loaded cached graph: {len(nodes_df)} nodes, {len(edges_df)} edges\")\n",
    "                return G\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Failed to load cached graph: {e}. Recomputing graph...\")\n",
    "    \n",
    "    # Initialize node DataFrame\n",
    "    nodes = []\n",
    "    vertex_to_index = {}\n",
    "    node_id_to_vertex = {}\n",
    "    current_index = 0\n",
    "    \n",
    "    # Add neighborhood nodes\n",
    "    logging.info(\"Adding neighborhood nodes...\")\n",
    "    neighborhoods_gdf = data['neighborhoods']\n",
    "    for i, row in tqdm(neighborhoods_gdf.iterrows(), total=len(neighborhoods_gdf), desc=\"Neighborhood nodes\"):\n",
    "        node_id = f\"neighborhood_{i}\"\n",
    "        vertex_to_index[node_id] = current_index\n",
    "        node_id_to_vertex[str(i)] = node_id\n",
    "        nodes.append({\n",
    "            'index': current_index,\n",
    "            'type': 'neighborhood',\n",
    "            'node_id': node_id,\n",
    "            'LIE_NAME': row['LIE_NAME'],\n",
    "            'avg_road_accident_density': row.get('avg_road_accident_density', 0),\n",
    "            'pedestrian_road_density': row.get('pedestrian_road_density', 0),\n",
    "            'ndvi': row.get('ndvi', 0),\n",
    "            'tree_count': row.get('tree_count', 0),\n",
    "            'transit_count': row.get('transit_count', 0),\n",
    "            'accident_count': row.get('accident_count', 0),\n",
    "            'road_density': row.get('road_density', 0),\n",
    "            'intersection_density': row.get('intersection_density', 0),\n",
    "            'total_population': row.get('total_population', 0),\n",
    "            'elderly_percentage': row.get('elderly_percentage', 0),\n",
    "            'min_x': float(row.geometry.bounds[0]),\n",
    "            'min_y': float(row.geometry.bounds[1]),\n",
    "            'max_x': float(row.geometry.bounds[2]),\n",
    "            'max_y': float(row.geometry.bounds[3])\n",
    "        })\n",
    "        for cat in CATEGORY_PRIORITY.keys():\n",
    "            col = f'land_use_{cat.lower()}_percent'\n",
    "            nodes[-1][col] = row.get(col, 0)\n",
    "        current_index += 1\n",
    "    \n",
    "    # Add building nodes\n",
    "    logging.info(\"Adding building nodes...\")\n",
    "    buildings_gdf = data['buildings']\n",
    "    if 'area_m2' not in buildings_gdf.columns:\n",
    "        logging.warning(\"'area_m2' missing. Computing from geometry...\")\n",
    "        buildings_gdf['area_m2'] = buildings_gdf.geometry.area\n",
    "    else:\n",
    "        logging.info(\"Using existing 'area_m2' column.\")\n",
    "    \n",
    "    for i, row in tqdm(buildings_gdf.iterrows(), total=len(buildings_gdf), desc=\"Building nodes\"):\n",
    "        node_id = f\"building_{i}\"\n",
    "        vertex_to_index[node_id] = current_index\n",
    "        node_id_to_vertex[str(i)] = node_id\n",
    "        nodes.append({\n",
    "            'index': current_index,\n",
    "            'type': 'building',\n",
    "            'node_id': node_id,\n",
    "            'building': row.get('building', 'unknown'),\n",
    "            'area_m2': row.get('area_m2', 0),\n",
    "            'min_x': float(row.geometry.bounds[0]),\n",
    "            'min_y': float(row.geometry.bounds[1]),\n",
    "            'max_x': float(row.geometry.bounds[2]),\n",
    "            'max_y': float(row.geometry.bounds[3])\n",
    "        })\n",
    "        current_index += 1\n",
    "    \n",
    "    # Add road nodes\n",
    "    logging.info(\"Adding road nodes...\")\n",
    "    roads_gdf = data['roads']\n",
    "    for i, row in tqdm(roads_gdf.iterrows(), total=len(roads_gdf), desc=\"Road nodes\"):\n",
    "        node_id = f\"road_{i}\"\n",
    "        vertex_to_index[node_id] = current_index\n",
    "        node_id_to_vertex[str(i)] = node_id\n",
    "        nodes.append({\n",
    "            'index': current_index,\n",
    "            'type': 'road',\n",
    "            'node_id': node_id,\n",
    "            'class': row.get('class', 'unknown'),\n",
    "            'length_m': row.get('length_m', 0),\n",
    "            'min_x': float(row.geometry.bounds[0]),\n",
    "            'min_y': float(row.geometry.bounds[1]),\n",
    "            'max_x': float(row.geometry.bounds[2]),\n",
    "            'max_y': float(row.geometry.bounds[3])\n",
    "        })\n",
    "        current_index += 1\n",
    "    \n",
    "    nodes_df = cudf.DataFrame(nodes)\n",
    "    \n",
    "    # Convert GeoDataFrames to cudf for GPU processing\n",
    "    logging.info(\"Converting GeoDataFrames to cudf for GPU processing...\")\n",
    "    neighborhoods_cudf = cudf.DataFrame.from_pandas(neighborhoods_gdf.drop(columns=['geometry']))\n",
    "    buildings_cudf = cudf.DataFrame.from_pandas(buildings_gdf.drop(columns=['geometry']))\n",
    "    roads_cudf = cudf.DataFrame.from_pandas(roads_gdf.drop(columns=['geometry']))\n",
    "    \n",
    "    # Extract bounding box coordinates\n",
    "    logging.info(\"Extracting bounding box coordinates...\")\n",
    "    neighborhoods_cudf['min_x'] = cudf.Series([float(g.bounds[0]) for g in neighborhoods_gdf.geometry])\n",
    "    neighborhoods_cudf['min_y'] = cudf.Series([float(g.bounds[1]) for g in neighborhoods_gdf.geometry])\n",
    "    neighborhoods_cudf['max_x'] = cudf.Series([float(g.bounds[2]) for g in neighborhoods_gdf.geometry])\n",
    "    neighborhoods_cudf['max_y'] = cudf.Series([float(g.bounds[3]) for g in neighborhoods_gdf.geometry])\n",
    "    \n",
    "    buildings_cudf['min_x'] = cudf.Series([float(g.bounds[0]) for g in buildings_gdf.geometry])\n",
    "    buildings_cudf['min_y'] = cudf.Series([float(g.bounds[1]) for g in buildings_gdf.geometry])\n",
    "    buildings_cudf['max_x'] = cudf.Series([float(g.bounds[2]) for g in buildings_gdf.geometry])\n",
    "    buildings_cudf['max_y'] = cudf.Series([float(g.bounds[3]) for g in buildings_gdf.geometry])\n",
    "    \n",
    "    roads_cudf['min_x'] = cudf.Series([float(g.bounds[0]) for g in roads_gdf.geometry])\n",
    "    roads_cudf['min_y'] = cudf.Series([float(g.bounds[1]) for g in roads_gdf.geometry])\n",
    "    roads_cudf['max_x'] = cudf.Series([float(g.bounds[2]) for g in roads_gdf.geometry])\n",
    "    roads_cudf['max_y'] = cudf.Series([float(g.bounds[3]) for g in roads_gdf.geometry])\n",
    "    \n",
    "    logging.info(f\"neighborhoods_cudf['min_x'] dtype: {neighborhoods_cudf['min_x'].dtype}\")\n",
    "    logging.info(f\"buildings_cudf['min_x'] dtype: {buildings_cudf['min_x'].dtype}\")\n",
    "    logging.info(f\"roads_cudf['min_x'] dtype: {roads_cudf['min_x'].dtype}\")\n",
    "    \n",
    "    # Create edges using GPU-accelerated spatial joins\n",
    "    logging.info(\"Creating edges using GPU-accelerated spatial joins...\")\n",
    "    edges = []\n",
    "    edge_counts = {'neighborhood-neighborhood': 0, 'neighborhood-building': 0, 'neighborhood-road': 0}\n",
    "    \n",
    "    # Neighborhood-Neighborhood edges\n",
    "    logging.info(\"Computing neighborhood-neighborhood edges...\")\n",
    "    for i in tqdm(range(len(neighborhoods_gdf)), desc=\"Neighborhood-Neighborhood edges\"):\n",
    "        row = neighborhoods_cudf.iloc[[i]]\n",
    "        geom_min_x = float(row['min_x'].values[0])\n",
    "        geom_min_y = float(row['min_y'].values[0])\n",
    "        geom_max_x = float(row['max_x'].values[0])\n",
    "        geom_max_y = float(row['max_y'].values[0])\n",
    "        matches = neighborhoods_cudf[\n",
    "            ~((geom_max_x < neighborhoods_cudf['min_x']) |\n",
    "              (geom_min_x > neighborhoods_cudf['max_x']) |\n",
    "              (geom_max_y < neighborhoods_cudf['min_y']) |\n",
    "              (geom_min_y > neighborhoods_cudf['max_y']))\n",
    "        ]\n",
    "        for j in matches.index.values_host:\n",
    "            if j != i:\n",
    "                src_vertex = f\"neighborhood_{i}\"\n",
    "                dst_vertex = f\"neighborhood_{j}\"\n",
    "                src = vertex_to_index[src_vertex]\n",
    "                dst = vertex_to_index[dst_vertex]\n",
    "                edges.append({'src': src, 'dst': dst})\n",
    "                edge_counts['neighborhood-neighborhood'] += 1\n",
    "    \n",
    "    # Neighborhood-Building edges\n",
    "    logging.info(\"Computing neighborhood-building edges...\")\n",
    "    for i in tqdm(range(len(neighborhoods_gdf)), desc=\"Neighborhood-Building edges\"):\n",
    "        row = neighborhoods_cudf.iloc[[i]]\n",
    "        geom_min_x = float(row['min_x'].values[0])\n",
    "        geom_min_y = float(row['min_y'].values[0])\n",
    "        geom_max_x = float(row['max_x'].values[0])\n",
    "        geom_max_y = float(row['max_y'].values[0])\n",
    "        matches = buildings_cudf[\n",
    "            ~((geom_max_x < buildings_cudf['min_x']) |\n",
    "              (geom_min_x > buildings_cudf['max_x']) |\n",
    "              (geom_max_y < buildings_cudf['min_y']) |\n",
    "              (geom_min_y > buildings_cudf['max_y']))\n",
    "        ]\n",
    "        for j in matches.index.values_host:\n",
    "            src_vertex = f\"neighborhood_{i}\"\n",
    "            dst_vertex = f\"building_{j}\"\n",
    "            src = vertex_to_index[src_vertex]\n",
    "            dst = vertex_to_index[dst_vertex]\n",
    "            edges.append({'src': src, 'dst': dst})\n",
    "            edge_counts['neighborhood-building'] += 1\n",
    "    \n",
    "    # Neighborhood-Road edges\n",
    "    logging.info(\"Computing neighborhood-road edges...\")\n",
    "    for i in tqdm(range(len(neighborhoods_gdf)), desc=\"Neighborhood-Road edges\"):\n",
    "        row = neighborhoods_cudf.iloc[[i]]\n",
    "        geom_min_x = float(row['min_x'].values[0])\n",
    "        geom_min_y = float(row['min_y'].values[0])\n",
    "        geom_max_x = float(row['max_x'].values[0])\n",
    "        geom_max_y = float(row['max_y'].values[0])\n",
    "        matches = roads_cudf[\n",
    "            ~((geom_max_x < roads_cudf['min_x']) |\n",
    "              (geom_min_x > roads_cudf['max_x']) |\n",
    "              (geom_max_y < roads_cudf['min_y']) |\n",
    "              (geom_min_y > roads_cudf['max_y']))\n",
    "        ]\n",
    "        for j in matches.index.values_host:\n",
    "            src_vertex = f\"neighborhood_{i}\"\n",
    "            dst_vertex = f\"road_{j}\"\n",
    "            src = vertex_to_index[src_vertex]\n",
    "            dst = vertex_to_index[dst_vertex]\n",
    "            edges.append({'src': src, 'dst': dst})\n",
    "            edge_counts['neighborhood-road'] += 1\n",
    "    \n",
    "    logging.info(f\"Edge counts by type: {edge_counts}\")\n",
    "    \n",
    "    edges_df = cudf.DataFrame(edges)\n",
    "    logging.info(f\"Created {len(edges_df)} total edges\")\n",
    "    \n",
    "    # Validate edges\n",
    "    valid_indices = set(nodes_df['index'].to_pandas())\n",
    "    if edges_df.empty:\n",
    "        logging.warning(\"No edges created. Graph will have nodes but no edges.\")\n",
    "    else:\n",
    "        edges_df = edges_df[edges_df['src'].isin(valid_indices) & edges_df['dst'].isin(valid_indices)]\n",
    "        logging.info(f\"After validation, {len(edges_df)} edges remain\")\n",
    "        if not edges_df.empty:\n",
    "            logging.info(f\"Sample edges after validation:\\n{edges_df.head().to_pandas()}\")\n",
    "    \n",
    "    # Create graph\n",
    "    G = cugraph.Graph()\n",
    "    G._nodes = nodes_df\n",
    "    if not edges_df.empty:\n",
    "        G.from_cudf_edgelist(edges_df, source='src', destination='dst')\n",
    "    else:\n",
    "        logging.warning(\"No valid edges created. Graph will have nodes but no edges.\")\n",
    "    \n",
    "    # Save graph data to cache\n",
    "    logging.info(\"Saving graph data to cache...\")\n",
    "    try:\n",
    "        nodes_df.to_parquet(GRAPH_NODES_CACHE_PATH)\n",
    "        edges_df.to_parquet(GRAPH_EDGES_CACHE_PATH)\n",
    "        with open(GRAPH_DATA_HASH_PATH, 'w') as f:\n",
    "            f.write(current_hash)\n",
    "        with open(GRAPH_NODE_ID_CACHE_PATH, 'w') as f:\n",
    "            json.dump(node_id_to_vertex, f)\n",
    "        logging.info(\"Successfully saved graph data to cache.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save graph data to cache: {e}\")\n",
    "    \n",
    "    logging.info(f\"City graph constructed: {len(nodes_df)} nodes, {len(edges_df)} edges\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14e080",
   "metadata": {},
   "source": [
    "Cell 7: Rule-Based Walkability Scores (compute_walkability_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ef4aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_walkability_scores(G, data):\n",
    "    \"\"\"\n",
    "    Compute walkability scores for neighborhood nodes in the graph and assign them.\n",
    "    \n",
    "    Args:\n",
    "        G (cugraph.Graph): The city graph with nodes and edges.\n",
    "        data (dict): Dictionary containing roads and other datasets.\n",
    "    \n",
    "    Returns:\n",
    "        cugraph.Graph: Updated graph with walkability scores assigned to neighborhood nodes.\n",
    "    \"\"\"\n",
    "    logging.info(\"Computing walkability scores for neighborhoods...\")\n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    \n",
    "    walkability_components = compute_walkability_components_all(data['neighborhoods'], data)\n",
    "    \n",
    "    logging.info(f\"Number of neighborhood nodes in nodes_df: {len(nodes_df[nodes_df['type'] == 'neighborhood'])}\")\n",
    "    logging.info(f\"Number of entries in walkability_components: {len(walkability_components)}\")\n",
    "    logging.info(f\"Sample LIE_NAME in nodes_df: {nodes_df[nodes_df['type'] == 'neighborhood']['LIE_NAME'].head().tolist()}\")\n",
    "    logging.info(f\"Sample LIE_NAME in walkability_components: {walkability_components['LIE_NAME'].head().tolist()}\")\n",
    "    \n",
    "    nodes_df = nodes_df.merge(\n",
    "        walkability_components[['LIE_NAME', 'walkability_score', 'walkability_category']],\n",
    "        on='LIE_NAME',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    unmatched = nodes_df[(nodes_df['type'] == 'neighborhood') & (nodes_df['walkability_score'].isna())]\n",
    "    if len(unmatched) > 0:\n",
    "        logging.warning(f\"Found {len(unmatched)} neighborhood nodes without walkability scores. Filling with 0.\")\n",
    "        nodes_df.loc[nodes_df['type'] == 'neighborhood', 'walkability_score'] = nodes_df['walkability_score'].fillna(0)\n",
    "        nodes_df.loc[nodes_df['type'] == 'neighborhood', 'walkability_category'] = nodes_df['walkability_category'].fillna('low')\n",
    "    \n",
    "    G._nodes = cudf.from_pandas(nodes_df)\n",
    "    \n",
    "    logging.info(\"Finished computing walkability scores.\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989dbf84",
   "metadata": {},
   "source": [
    "Cell 8 prepare_gnn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a057108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_gnn_data(G):\n",
    "    import torch\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    logging.info(\"Preparing data for GNN training...\")\n",
    "    nodes_df = G._nodes\n",
    "    edges_df = G.edgelist.edgelist_df if G.edgelist else cudf.DataFrame()\n",
    "    \n",
    "    numerical_features = [\n",
    "        'ndvi', 'tree_count', 'transit_count', 'accident_count',\n",
    "        'road_density', 'intersection_density', 'total_population',\n",
    "        'elderly_percentage', 'area_m2', 'length_m', 'avg_road_accident_density',\n",
    "        'pedestrian_road_density'\n",
    "    ] + [f'land_use_{cat.lower()}_percent' for cat in CATEGORY_PRIORITY.keys()]\n",
    "    \n",
    "    numerical_features.append('land_use_diversity')\n",
    "    \n",
    "    building_types = nodes_df[nodes_df['type'] == 'building']['building'].to_pandas().unique()\n",
    "    road_classes = nodes_df[nodes_df['type'] == 'road']['class'].to_pandas().unique()\n",
    "    categorical_features = (\n",
    "        [f'building_{bt}' for bt in building_types if pd.notna(bt)] +\n",
    "        [f'road_class_{rc}' for rc in road_classes if pd.notna(rc)]\n",
    "    )\n",
    "    \n",
    "    all_features = numerical_features + categorical_features\n",
    "    \n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    node_types = []\n",
    "    \n",
    "    for node_type in tqdm(['neighborhood', 'building', 'road'], desc=\"Normalizing features by node type\"):\n",
    "        subset = nodes_df[nodes_df['type'] == node_type].to_pandas()\n",
    "        if subset.empty:\n",
    "            logging.warning(f\"No nodes of type {node_type} found. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        subset_features = pd.DataFrame(0.0, index=subset.index, columns=all_features)\n",
    "        \n",
    "        if node_type == 'neighborhood':\n",
    "            # Compute land_use_diversity\n",
    "            land_use_cols = [col for col in subset.columns if col.startswith('land_use_') and col.endswith('_percent')]\n",
    "            if land_use_cols:\n",
    "                subset['land_use_diversity'] = subset[land_use_cols].apply(\n",
    "                    lambda row: -np.sum([p * np.log(p + 1e-10) for p in row / 100.0 if p > 0]), axis=1\n",
    "                )\n",
    "            else:\n",
    "                subset['land_use_diversity'] = 0\n",
    "            \n",
    "            for col in numerical_features:\n",
    "                if col in subset.columns:\n",
    "                    subset_features[col] = subset[col].astype(float).fillna(0)\n",
    "                    logging.debug(f\"{node_type} - {col} pre-normalization std: {subset[col].std():.4f}\")\n",
    "                else:\n",
    "                    logging.warning(f\"Column {col} missing in neighborhood nodes. Setting to 0.\")\n",
    "                    subset_features[col] = 0\n",
    "        elif node_type == 'building':\n",
    "            if 'area_m2' in subset.columns:\n",
    "                subset_features['area_m2'] = subset['area_m2'].astype(float).fillna(0)\n",
    "                logging.debug(f\"{node_type} - area_m2 pre-normalization std: {subset['area_m2'].std():.4f}\")\n",
    "        else:  # road\n",
    "            if 'length_m' in subset.columns:\n",
    "                subset_features['length_m'] = subset['length_m'].astype(float).fillna(0)\n",
    "                logging.debug(f\"{node_type} - length_m pre-normalization std: {subset['length_m'].std():.4f}\")\n",
    "        \n",
    "        if node_type == 'building':\n",
    "            for bt in building_types:\n",
    "                if pd.notna(bt):\n",
    "                    subset_features[f'building_{bt}'] = (subset['building'] == bt).astype(float)\n",
    "        elif node_type == 'road':\n",
    "            for rc in road_classes:\n",
    "                if pd.notna(rc):\n",
    "                    subset_features[f'road_class_{rc}'] = (subset['class'] == rc).astype(float)\n",
    "        \n",
    "        # Z-score normalization for numerical features\n",
    "        for col in numerical_features:\n",
    "            if col in subset_features.columns and subset_features[col].std() > 0:\n",
    "                subset_features[col] = (\n",
    "                    (subset_features[col] - subset_features[col].mean()) / subset_features[col].std()\n",
    "                ).fillna(0)\n",
    "                logging.debug(f\"{node_type} - {col} post-normalization std: {subset_features[col].std():.4f}\")\n",
    "            else:\n",
    "                logging.debug(f\"Column {col} has zero variance or is missing for {node_type}. Setting to 0.\")\n",
    "        \n",
    "        logging.info(f\"Node type {node_type}: {len(subset)} nodes, feature shape: {subset_features.shape}\")\n",
    "        \n",
    "        features_list.append(subset_features.values)\n",
    "        \n",
    "        if node_type == 'neighborhood':\n",
    "            labels = subset['walkability_score'].astype(float).fillna(0).values\n",
    "            labels_list.append(labels[:, None])  # Shape [n, 1]\n",
    "        else:\n",
    "            labels_list.append(np.zeros((len(subset), 1)))\n",
    "        \n",
    "        node_types.extend([node_type] * len(subset))\n",
    "    \n",
    "    try:\n",
    "        features = np.vstack(features_list)\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Failed to stack features: {e}\")\n",
    "        raise\n",
    "    \n",
    "    labels = np.vstack(labels_list)\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "    if not edges_df.empty:\n",
    "        edge_index = torch.tensor(edges_df[['src', 'dst']].to_pandas().values.T, dtype=torch.long)\n",
    "        logging.info(f\"Edge index created with {edge_index.shape[1]} edges\")\n",
    "        max_index = nodes_df['index'].max()\n",
    "        if edge_index.max() > max_index or edge_index.min() < 0:\n",
    "            logging.warning(f\"Edge indices out of bounds: min={edge_index.min()}, max={edge_index.max()}, expected max={max_index}\")\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "    else:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        logging.warning(\"No edges found in graph.\")\n",
    "    \n",
    "    data = Data(\n",
    "        x=features_tensor,\n",
    "        edge_index=edge_index,\n",
    "        y=labels_tensor\n",
    "    )\n",
    "    \n",
    "    data.node_types = node_types\n",
    "    \n",
    "    logging.info(f\"Prepared GNN data: {features_tensor.shape[0]} nodes, {edge_index.shape[1]} edges\")\n",
    "    logging.info(f\"Feature matrix shape: {features_tensor.shape}\")\n",
    "    logging.info(f\"Label tensor shape: {labels_tensor.shape}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a9a6b5",
   "metadata": {},
   "source": [
    "Cell 9: WalkabilityGNN, train_gnn_model, predict_walkability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "635ecc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNWalkabilityPredictor(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=128, num_heads=4, dropout_rate=0.3):\n",
    "        super(GNNWalkabilityPredictor, self).__init__()\n",
    "        self.conv1 = GATConv(num_features, hidden_dim, heads=num_heads, concat=True)\n",
    "        self.bn1 = BatchNorm(hidden_dim * num_heads)\n",
    "        self.conv2 = GATConv(hidden_dim * num_heads, hidden_dim // 2, heads=1, concat=True)\n",
    "        self.bn2 = BatchNorm(hidden_dim // 2)\n",
    "        self.fc1 = torch.nn.Linear(hidden_dim // 2, hidden_dim // 4)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim // 4, 1)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        if edge_index.numel() > 0:\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = self.bn1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            x = self.bn2(x)\n",
    "            x = F.relu(x)\n",
    "        else:\n",
    "            logging.warning(\"No edges in the graph. Using linear layer for node features only.\")\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "def train_gnn_model(data_gnn, hidden_dim=128, num_heads=4, dropout_rate=0.3, lr=0.005, weight_decay=1e-4, epochs=500, patience=30):\n",
    "    logging.info(\"Stage 4: Training GNN model...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_gnn = data_gnn.to(device)\n",
    "    \n",
    "    neighborhood_mask = np.array([t == 'neighborhood' for t in data_gnn.node_types])\n",
    "    train_indices = np.where(neighborhood_mask)[0]\n",
    "    \n",
    "    if len(train_indices) == 0:\n",
    "        logging.error(\"No neighborhood nodes found for training.\")\n",
    "        raise ValueError(\"No neighborhood nodes found for training.\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    train_idx = np.random.choice(train_indices, size=int(0.8 * len(train_indices)), replace=False)\n",
    "    val_idx = np.setdiff1d(train_indices, train_idx)\n",
    "    \n",
    "    train_mask = torch.zeros(data_gnn.num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(data_gnn.num_nodes, dtype=torch.bool)\n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    data_gnn.train_mask = train_mask\n",
    "    data_gnn.val_mask = val_mask\n",
    "    \n",
    "    neighborhood_labels = data_gnn.y[neighborhood_mask].cpu().numpy()\n",
    "    logging.info(f\"Target (walkability_score) distribution for neighborhood nodes:\\n{pd.Series(neighborhood_labels.flatten()).describe()}\")\n",
    "    \n",
    "    model = GNNWalkabilityPredictor(num_features=data_gnn.x.shape[1], hidden_dim=hidden_dim, num_heads=num_heads, dropout_rate=dropout_rate).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = os.path.join(CHECKPOINT_DIR, 'best_gnn_model.pth')\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_maes = []\n",
    "    val_maes = []\n",
    "    train_r2s = []\n",
    "    val_r2s = []\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training epochs\"):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data_gnn)\n",
    "        loss = criterion(out[data_gnn.train_mask], data_gnn.y[data_gnn.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(data_gnn)\n",
    "            val_loss = criterion(val_out[data_gnn.val_mask], data_gnn.y[data_gnn.val_mask])\n",
    "            \n",
    "            train_pred = out[data_gnn.train_mask].detach().cpu().numpy()\n",
    "            train_true = data_gnn.y[data_gnn.train_mask].cpu().numpy()\n",
    "            val_pred = val_out[data_gnn.val_mask].detach().cpu().numpy()\n",
    "            val_true = data_gnn.y[data_gnn.val_mask].cpu().numpy()\n",
    "            \n",
    "            train_mae = mean_absolute_error(train_true, train_pred)\n",
    "            train_r2 = r2_score(train_true, train_pred)\n",
    "            val_mae = mean_absolute_error(val_true, val_pred)\n",
    "            val_r2 = r2_score(val_true, val_pred)\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            val_losses.append(val_loss.item())\n",
    "            train_maes.append(train_mae)\n",
    "            val_maes.append(val_mae)\n",
    "            train_r2s.append(train_r2)\n",
    "            val_r2s.append(val_r2)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            logging.info(f\"Epoch {epoch}, Train Loss: {loss.item():.4f}, Train MAE: {train_mae:.4f}, Train R2: {train_r2:.4f}, Val Loss: {val_loss.item():.4f}, Val MAE: {val_mae:.4f}, Val R2: {val_r2:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                logging.info(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    logging.info(\"Finished training GNN model.\")\n",
    "    return {\n",
    "        'model': model,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_maes': train_maes,\n",
    "        'val_maes': val_maes,\n",
    "        'train_r2s': train_r2s,\n",
    "        'val_r2s': val_r2s\n",
    "    }\n",
    "\n",
    "def predict_walkability(G, model):\n",
    "    logging.info(\"Predicting walkability scores using GNN model...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    data_gnn = prepare_gnn_data(G)\n",
    "    data_gnn = data_gnn.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(data_gnn)\n",
    "    \n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    neighborhood_mask = nodes_df['type'] == 'neighborhood'\n",
    "    nodes_df.loc[neighborhood_mask, 'walkability_gnn'] = predictions[neighborhood_mask].cpu().numpy().flatten()\n",
    "    \n",
    "    nodes_df['walkability_gnn'] = nodes_df['walkability_gnn'].clip(0, 1)\n",
    "    \n",
    "    # Compute walkability_category with dynamic thresholds for GNN predictions\n",
    "    low_threshold = nodes_df.loc[neighborhood_mask, 'walkability_gnn'].quantile(0.33)\n",
    "    high_threshold = nodes_df.loc[neighborhood_mask, 'walkability_gnn'].quantile(0.66)\n",
    "    logging.info(f\"GNN walkability category thresholds - low: {low_threshold:.4f}, high: {high_threshold:.4f}\")\n",
    "    \n",
    "    def categorize_gnn_score(score):\n",
    "        if score < low_threshold:\n",
    "            return 'low'\n",
    "        elif score < high_threshold:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'high'\n",
    "    \n",
    "    nodes_df.loc[neighborhood_mask, 'walkability_category'] = nodes_df.loc[neighborhood_mask, 'walkability_gnn'].apply(categorize_gnn_score)\n",
    "    \n",
    "    G._nodes = cudf.from_pandas(nodes_df)\n",
    "    \n",
    "    logging.info(\"Finished predicting walkability scores.\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5406821f",
   "metadata": {},
   "source": [
    "Cell 10: Interactive Map Generation (create_interactive_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d059c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_map(G, data):\n",
    "    \"\"\"Generate an interactive Kepler.gl map to visualize walkability scores and other geodata.\"\"\"\n",
    "    logging.info(\"Generating interactive Kepler.gl map...\")\n",
    "    \n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    neighborhoods_gdf = data['neighborhoods'].copy()\n",
    "\n",
    "    # Standardize LIE_NAME for merging\n",
    "    nodes_df['LIE_NAME'] = nodes_df['LIE_NAME'].astype(str).str.strip()\n",
    "    neighborhoods_gdf['LIE_NAME'] = neighborhoods_gdf['LIE_NAME'].astype(str).str.strip()\n",
    "\n",
    "    # Filter for neighborhood nodes and select necessary columns\n",
    "    neighborhood_nodes = nodes_df[nodes_df['type'] == 'neighborhood'][['LIE_NAME', 'walkability_score', 'walkability_gnn', 'walkability_category']]\n",
    "\n",
    "    # Merge data\n",
    "    map_data = neighborhoods_gdf[['LIE_NAME', 'geometry']].merge(\n",
    "        neighborhood_nodes,\n",
    "        on='LIE_NAME',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Drop duplicates in-place\n",
    "    map_data.drop_duplicates(subset=['LIE_NAME'], keep='first', inplace=True)\n",
    "\n",
    "    # Fill NaN values\n",
    "    map_data['walkability_score'] = map_data['walkability_score'].fillna(0)\n",
    "    map_data['walkability_gnn'] = map_data['walkability_gnn'].fillna(0)\n",
    "    map_data['walkability_category'] = map_data['walkability_category'].fillna('low')\n",
    "\n",
    "    # Convert to GeoDataFrame and transform CRS\n",
    "    map_data = gpd.GeoDataFrame(map_data, geometry='geometry', crs='EPSG:3826')\n",
    "    map_data['geometry'] = map_data['geometry'].to_crs('EPSG:4326')\n",
    "\n",
    "    # Prepare kepler_data\n",
    "    kepler_data = {\n",
    "        'neighborhoods': map_data[['LIE_NAME', 'walkability_score', 'walkability_gnn', 'walkability_category', 'geometry']].to_json()\n",
    "    }\n",
    "\n",
    "    # Prepare roads data\n",
    "    if 'roads' in data:\n",
    "        roads_gdf = data['roads'].copy()\n",
    "        if roads_gdf.crs != 'EPSG:4326':\n",
    "            roads_gdf = roads_gdf.to_crs('EPSG:4326')\n",
    "        road_columns = ['class', 'length_m', 'geometry']\n",
    "        available_columns = [col for col in road_columns if col in roads_gdf.columns]\n",
    "        if 'geometry' in available_columns:\n",
    "            kepler_data['roads'] = roads_gdf[available_columns].to_json()\n",
    "        else:\n",
    "            logging.warning(\"Roads GeoDataFrame missing 'geometry' column. Skipping roads layer.\")\n",
    "    else:\n",
    "        logging.warning(\"Roads data not found in data dictionary. Skipping roads layer.\")\n",
    "\n",
    "    # Prepare buildings data\n",
    "    if 'buildings' in data:\n",
    "        buildings_gdf = data['buildings'].copy()\n",
    "        if buildings_gdf.crs != 'EPSG:4326':\n",
    "            buildings_gdf = buildings_gdf.to_crs('EPSG:4326')\n",
    "        building_columns = ['building', 'area_m2', 'geometry']\n",
    "        available_columns = [col for col in building_columns if col in buildings_gdf.columns]\n",
    "        if 'geometry' in available_columns:\n",
    "            kepler_data['buildings'] = buildings_gdf[available_columns].to_json()\n",
    "        else:\n",
    "            logging.warning(\"Buildings GeoDataFrame missing 'geometry' column. Skipping buildings layer.\")\n",
    "    else:\n",
    "        logging.warning(\"Buildings data not found in data dictionary. Skipping buildings layer.\")\n",
    "\n",
    "    # Define neighborhoods layer\n",
    "    neighborhoods_layer = {\n",
    "        \"id\": \"neighborhoods\",\n",
    "        \"type\": \"geojson\",\n",
    "        \"config\": {\n",
    "            \"dataId\": \"neighborhoods\",\n",
    "            \"label\": \"Neighborhoods\",\n",
    "            \"color\": [18, 147, 154],\n",
    "            \"columns\": {\n",
    "                \"geojson\": \"geometry\"\n",
    "            },\n",
    "            \"isVisible\": True,\n",
    "            \"visConfig\": {\n",
    "                \"opacity\": 0.7,\n",
    "                \"strokeOpacity\": 0.9,\n",
    "                \"thickness\": 1,\n",
    "                \"strokeColor\": [255, 255, 255],\n",
    "                \"colorRange\": {\n",
    "                    \"name\": \"Global Warming\",\n",
    "                    \"type\": \"sequential\",\n",
    "                    \"colors\": [\n",
    "                        \"#5A1846\", \"#900C3F\", \"#C70039\",\n",
    "                        \"#E3611C\", \"#F1920E\", \"#FFC107\"\n",
    "                    ]\n",
    "                },\n",
    "                \"strokeColorRange\": {\n",
    "                    \"name\": \"Global Warming\",\n",
    "                    \"type\": \"sequential\",\n",
    "                    \"colors\": [\n",
    "                        \"#5A1846\", \"#900C3F\", \"#C70039\",\n",
    "                        \"#E3611C\", \"#F1920E\", \"#FFC107\"\n",
    "                    ]\n",
    "                },\n",
    "                \"colorField\": {\n",
    "                    \"name\": \"walkability_gnn\",\n",
    "                    \"type\": \"real\"\n",
    "                },\n",
    "                \"colorScale\": \"quantile\"\n",
    "            }\n",
    "        },\n",
    "        \"visualChannels\": {\n",
    "            \"colorField\": {\n",
    "                \"name\": \"walkability_gnn\",\n",
    "                \"type\": \"real\"\n",
    "            },\n",
    "            \"colorScale\": \"quantile\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Define roads layer if available\n",
    "    if 'roads' in kepler_data:\n",
    "        roads_layer = {\n",
    "            \"id\": \"roads\",\n",
    "            \"type\": \"geojson\",\n",
    "            \"config\": {\n",
    "                \"dataId\": \"roads\",\n",
    "                \"label\": \"Roads\",\n",
    "                \"color\": [255, 0, 0],\n",
    "                \"columns\": {\n",
    "                    \"geojson\": \"geometry\"\n",
    "                },\n",
    "                \"isVisible\": True,\n",
    "                \"visConfig\": {\n",
    "                    \"opacity\": 0.8,\n",
    "                    \"strokeOpacity\": 0.8,\n",
    "                    \"thickness\": 2,\n",
    "                    \"strokeColor\": [255, 0, 0],\n",
    "                    \"colorField\": {\n",
    "                        \"name\": \"class\",\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"colorScale\": \"ordinal\",\n",
    "                    \"colorRange\": {\n",
    "                        \"name\": \"ColorBrewer Paired-12\",\n",
    "                        \"type\": \"all\",\n",
    "                        \"category\": \"ColorBrewer\",\n",
    "                        \"colors\": [\"#a6cee3\", \"#1f78b4\", \"#b2df8a\", \"#33a02c\", \"#fb9a99\", \"#e31a1c\", \"#fdbf6f\", \"#ff7f00\", \"#cab2d6\", \"#6a3d9a\", \"#ffff99\", \"#b15928\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"visualChannels\": {\n",
    "                \"colorField\": {\n",
    "                    \"name\": \"class\",\n",
    "                    \"type\": \"string\"\n",
    "                },\n",
    "                \"colorScale\": \"ordinal\"\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        roads_layer = None\n",
    "\n",
    "    # Define buildings layer if available\n",
    "    if 'buildings' in kepler_data:\n",
    "        buildings_layer = {\n",
    "            \"id\": \"buildings\",\n",
    "            \"type\": \"geojson\",\n",
    "            \"config\": {\n",
    "                \"dataId\": \"buildings\",\n",
    "                \"label\": \"Buildings\",\n",
    "                \"color\": [0, 255, 0],\n",
    "                \"columns\": {\n",
    "                    \"geojson\": \"geometry\"\n",
    "                },\n",
    "                \"isVisible\": True,\n",
    "                \"visConfig\": {\n",
    "                    \"opacity\": 0.5,\n",
    "                    \"strokeOpacity\": 0.5,\n",
    "                    \"thickness\": 0.5,\n",
    "                    \"strokeColor\": [0, 0, 0],\n",
    "                    \"colorField\": {\n",
    "                        \"name\": \"building\",\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"colorScale\": \"ordinal\",\n",
    "                    \"colorRange\": {\n",
    "                        \"name\": \"ColorBrewer Set3-12\",\n",
    "                        \"type\": \"all\",\n",
    "                        \"category\": \"ColorBrewer\",\n",
    "                        \"colors\": [\"#8dd3c7\", \"#ffffb3\", \"#bebada\", \"#fb8072\", \"#80b1d3\", \"#fdb462\", \"#b3de69\", \"#fccde5\", \"#d9d9d9\", \"#bc80bd\", \"#ccebc5\", \"#ffed6f\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"visualChannels\": {\n",
    "                \"colorField\": {\n",
    "                    \"name\": \"building\",\n",
    "                    \"type\": \"string\"\n",
    "                },\n",
    "                \"colorScale\": \"ordinal\"\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        buildings_layer = None\n",
    "\n",
    "    # Create layers list\n",
    "    layers = [neighborhoods_layer]\n",
    "    if roads_layer:\n",
    "        layers.append(roads_layer)\n",
    "    if buildings_layer:\n",
    "        layers.append(buildings_layer)\n",
    "\n",
    "    # Define tooltips\n",
    "    tooltips = {\n",
    "        \"neighborhoods\": [\n",
    "            {\"name\": \"LIE_NAME\", \"format\": None},\n",
    "            {\"name\": \"walkability_score\", \"format\": \"{:.3f}\"},\n",
    "            {\"name\": \"walkability_gnn\", \"format\": \"{:.3f}\"},\n",
    "            {\"name\": \"walkability_category\", \"format\": None}\n",
    "        ]\n",
    "    }\n",
    "    if 'roads' in kepler_data:\n",
    "        tooltips['roads'] = [\n",
    "            {\"name\": \"class\", \"format\": None},\n",
    "            {\"name\": \"length_m\", \"format\": \"{:.2f}\"}\n",
    "        ]\n",
    "    if 'buildings' in kepler_data:\n",
    "        tooltips['buildings'] = [\n",
    "            {\"name\": \"building\", \"format\": None},\n",
    "            {\"name\": \"area_m2\", \"format\": \"{:.2f}\"}\n",
    "        ]\n",
    "\n",
    "    # Update config\n",
    "    config = {\n",
    "        \"version\": \"v1\",\n",
    "        \"config\": {\n",
    "            \"visState\": {\n",
    "                \"layers\": layers,\n",
    "                \"interactionConfig\": {\n",
    "                    \"tooltip\": {\n",
    "                        \"fieldsToShow\": tooltips,\n",
    "                        \"enabled\": True\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"mapState\": {\n",
    "                \"latitude\": 25.0330,\n",
    "                \"longitude\": 121.5654,\n",
    "                \"zoom\": 11\n",
    "            },\n",
    "            \"mapStyle\": {\n",
    "                \"styleType\": \"dark\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    map_1 = KeplerGl(height=800, data=kepler_data, config=config)\n",
    "    map_path = os.path.join(BASE_DIR, 'taipei_walkability_map.html')\n",
    "    map_1.save_to_html(file_name=map_path)\n",
    "    logging.info(f\"Interactive map generated and saved as {map_path}\")\n",
    "    print(f\"Map saved to {map_path}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0683d",
   "metadata": {},
   "source": [
    "Cell 11: Main Execution (main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f13b11d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 18:44:03,055 - INFO - Ensured subgraph directory exists: /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/subgraphs\n",
      "2025-04-27 18:44:03,056 - INFO - Stage 1: Loading and preparing data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting load_and_prepare_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:   0%|          | 0/8 [00:00<?, ?it/s]2025-04-27 18:44:03,224 - INFO - Loaded neighborhoods with shape (456, 57)\n",
      "Loading files:  12%|█▎        | 1/8 [00:00<00:01,  6.05it/s]2025-04-27 18:44:04,431 - INFO - Loaded buildings with shape (74306, 9)\n",
      "Loading files:  25%|██▌       | 2/8 [00:01<00:04,  1.29it/s]2025-04-27 18:44:04,613 - INFO - Loaded roads with shape (81444, 2)\n",
      "Loading files:  38%|███▊      | 3/8 [00:01<00:02,  1.98it/s]2025-04-27 18:44:04,650 - INFO - Loaded trees with shape (5019, 12)\n",
      "2025-04-27 18:44:04,726 - INFO - Loaded transit with shape (29892, 11)\n",
      "Loading files:  62%|██████▎   | 5/8 [00:01<00:00,  3.98it/s]2025-04-27 18:44:05,355 - INFO - Loaded urban_masterplan with shape (15521, 15)\n",
      "Loading files:  75%|███████▌  | 6/8 [00:02<00:00,  2.78it/s]2025-04-27 18:44:05,872 - INFO - Loaded accidents with shape (56133, 8)\n",
      "Loading files:  88%|████████▊ | 7/8 [00:02<00:00,  2.46it/s]2025-04-27 18:44:05,881 - INFO - Columns in population_df after loading: ['LIE_NAME', 'Total_Population', 'Elderly_Percentage']\n",
      "2025-04-27 18:44:05,882 - INFO - Loaded population with shape (456, 3)\n",
      "Loading files: 100%|██████████| 8/8 [00:02<00:00,  2.83it/s]\n",
      "2025-04-27 18:44:05,883 - INFO - Columns in neighborhoods_gdf after loading: ['LIE_NAME', 'SECT_NAME', '2024population', 'land_use_city_open_area_count', 'land_use_city_open_area_area_m2', 'land_use_city_open_area_percent', 'land_use_commercial_count', 'land_use_commercial_area_m2', 'land_use_commercial_percent', 'land_use_infrastructure_count', 'land_use_infrastructure_area_m2', 'land_use_infrastructure_percent', 'land_use_government_count', 'land_use_government_area_m2', 'land_use_government_percent', 'land_use_public_transportation_count', 'land_use_public_transportation_area_m2', 'land_use_public_transportation_percent', 'land_use_education_count', 'land_use_education_area_m2', 'land_use_education_percent', 'land_use_medical_count', 'land_use_medical_area_m2', 'land_use_medical_percent', 'land_use_amenity_count', 'land_use_amenity_area_m2', 'land_use_amenity_percent', 'land_use_road_count', 'land_use_road_area_m2', 'land_use_road_percent', 'land_use_pedestrian_count', 'land_use_pedestrian_area_m2', 'land_use_pedestrian_percent', 'land_use_natural_count', 'land_use_natural_area_m2', 'land_use_natural_percent', 'land_use_special_zone_count', 'land_use_special_zone_area_m2', 'land_use_special_zone_percent', 'land_use_river_count', 'land_use_river_area_m2', 'land_use_river_percent', 'land_use_military_count', 'land_use_military_area_m2', 'land_use_military_percent', 'land_use_residential_count', 'land_use_residential_area_m2', 'land_use_residential_percent', 'land_use_industrial_count', 'land_use_industrial_area_m2', 'land_use_industrial_percent', 'land_use_agriculture_count', 'land_use_agriculture_area_m2', 'land_use_agriculture_percent', 'ndvi_mean', 'ndvi_median', 'geometry']\n",
      "2025-04-27 18:44:05,883 - INFO - Found alternative NDVI column 'ndvi_mean'. Renaming to 'ndvi'.\n",
      "2025-04-27 18:44:06,048 - INFO - Converted buildings to CRS EPSG:3826\n",
      "2025-04-27 18:44:06,090 - INFO - Converted trees to CRS EPSG:3826\n",
      "2025-04-27 18:44:06,116 - INFO - Converted transit to CRS EPSG:3826\n",
      "2025-04-27 18:44:06,194 - INFO - Converted urban_masterplan to CRS EPSG:3826\n",
      "2025-04-27 18:44:06,221 - INFO - Converted accidents to CRS EPSG:3826\n",
      "2025-04-27 18:44:10,653 - INFO - Computing intersections for neighborhoods...\n",
      "2025-04-27 18:44:10,654 - INFO - Columns in roads_gdf after loading: ['class', 'geometry']\n",
      "2025-04-27 18:44:10,655 - INFO - Extracting endpoints from road segments...\n",
      "Extracting endpoints: 100%|██████████| 81444/81444 [00:04<00:00, 18377.31it/s]\n",
      "2025-04-27 18:44:15,232 - INFO - Building endpoint-to-road mapping...\n",
      "Building endpoint-to-road mapping: 100%|██████████| 162888/162888 [00:04<00:00, 36747.80it/s]\n",
      "2025-04-27 18:44:19,667 - INFO - Identifying intersections...\n",
      "Identifying intersections: 100%|██████████| 101237/101237 [00:00<00:00, 440510.16it/s]\n",
      "2025-04-27 18:44:19,912 - INFO - Counting intersections per neighborhood...\n",
      "2025-04-27 18:44:19,969 - WARNING - 'area_km2' column missing in neighborhoods_gdf. Computing from geometry...\n",
      "2025-04-27 18:44:19,972 - INFO - Computed area_km2 stats:\n",
      "count    456.000000\n",
      "mean       0.588925\n",
      "std        1.351428\n",
      "min        0.031744\n",
      "25%        0.134566\n",
      "50%        0.209650\n",
      "75%        0.425264\n",
      "max       16.324434\n",
      "Name: area_km2, dtype: float64\n",
      "2025-04-27 18:44:19,975 - INFO - Intersection count stats:\n",
      "count    456.000000\n",
      "mean      33.800439\n",
      "std       24.763978\n",
      "min        1.000000\n",
      "25%       16.000000\n",
      "50%       26.000000\n",
      "75%       46.000000\n",
      "max      185.000000\n",
      "Name: intersection_count, dtype: float64\n",
      "2025-04-27 18:44:19,977 - INFO - Intersection density stats:\n",
      "count    456.000000\n",
      "mean     126.190854\n",
      "std       83.149602\n",
      "min        2.960154\n",
      "25%       66.874411\n",
      "50%      111.292996\n",
      "75%      165.079134\n",
      "max      490.471026\n",
      "Name: intersection_density, dtype: float64\n",
      "2025-04-27 18:44:19,993 - INFO - Saved neighborhoods with intersections to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/neighborhoods_with_intersections.geoparquet\n",
      "2025-04-27 18:44:19,994 - INFO - Computing tree count per neighborhood...\n",
      "2025-04-27 18:44:20,030 - INFO - Computing transit count per neighborhood...\n",
      "2025-04-27 18:44:20,130 - INFO - Computing accident count per neighborhood...\n",
      "2025-04-27 18:44:20,903 - INFO - Computing road density per neighborhood...\n",
      "2025-04-27 18:44:20,904 - INFO - Columns in roads_gdf before computing road density: ['class', 'geometry']\n",
      "2025-04-27 18:44:20,904 - WARNING - 'length_m' column missing in roads_gdf. Computing from geometry...\n",
      "2025-04-27 18:44:20,913 - INFO - Computed length_m stats:\n",
      "count     81444.000000\n",
      "mean        145.622456\n",
      "std        2304.902398\n",
      "min           0.030284\n",
      "25%          28.160770\n",
      "50%          61.698697\n",
      "75%         130.534001\n",
      "max      426414.891763\n",
      "Name: length_m, dtype: float64\n",
      "2025-04-27 18:44:21,240 - INFO - Road density stats:\n",
      "count     456.000000\n",
      "mean      158.024830\n",
      "std       171.789340\n",
      "min         7.561174\n",
      "25%        55.500294\n",
      "50%        96.985664\n",
      "75%       214.942848\n",
      "max      1657.324822\n",
      "Name: road_density, dtype: float64\n",
      "2025-04-27 18:44:21,241 - INFO - Merging population data...\n",
      "2025-04-27 18:44:21,242 - WARNING - Expected columns ['total_population', 'elderly_percentage'] not found in population_df. Attempting to find alternatives...\n",
      "2025-04-27 18:44:21,243 - INFO - Found alternative for total_population: Total_Population\n",
      "2025-04-27 18:44:21,244 - INFO - Found alternative for elderly_percentage: Elderly_Percentage\n",
      "2025-04-27 18:44:21,248 - INFO - Computing land use percentages for neighborhoods...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Percentage Calculation Process ---\n",
      "\n",
      "Neighborhood: 板溪里 (Index: 373)\n",
      "Total unique master plan area: 63777.59 m²\n",
      "Area of City_Open_Area (priority 10): 478.13 m²\n",
      "Area of Education (priority 6): 8173.36 m²\n",
      "Area of Commercial (priority 4): 23017.42 m²\n",
      "Area of Residential (priority 3): 32108.69 m²\n",
      "\n",
      "Percentages:\n",
      "City_Open_Area: 0.75%\n",
      "Education: 12.82%\n",
      "Commercial: 36.09%\n",
      "Residential: 50.34%\n",
      "Sum of percentages: 100.00%\n",
      "\n",
      "Neighborhood: 芝山里 (Index: 39)\n",
      "Total unique master plan area: 1061285.95 m²\n",
      "Area of Education (priority 6): 80442.15 m²\n",
      "Area of Residential (priority 3): 196204.41 m²\n",
      "Area of Natural (priority 2): 775753.11 m²\n",
      "Area of River (priority 1): 2754.93 m²\n",
      "Area of Government (priority 1): 6131.35 m²\n",
      "\n",
      "Percentages:\n",
      "Education: 7.58%\n",
      "Residential: 18.49%\n",
      "Natural: 73.10%\n",
      "River: 0.26%\n",
      "Government: 0.58%\n",
      "Sum of percentages: 100.00%\n",
      "\n",
      "Neighborhood: 和平里 (Index: 340)\n",
      "Total unique master plan area: 98073.11 m²\n",
      "Area of City_Open_Area (priority 10): 4608.53 m²\n",
      "Area of Public_Transportation (priority 8): 5304.93 m²\n",
      "Area of Commercial (priority 4): 15463.83 m²\n",
      "Area of Residential (priority 3): 50273.32 m²\n",
      "Area of Special_Zone (priority 1): 22422.50 m²\n",
      "\n",
      "Percentages:\n",
      "City_Open_Area: 4.70%\n",
      "Public_Transportation: 5.41%\n",
      "Commercial: 15.77%\n",
      "Residential: 51.26%\n",
      "Special_Zone: 22.86%\n",
      "Sum of percentages: 100.00%\n",
      "--- End of Percentage Calculation Process ---\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 18:44:21,863 - WARNING - Topology error for category Residential in neighborhood 陽明里: TopologyException: found non-noded intersection between LINESTRING (305235 2.7814e+06, 305232 2.7814e+06) and LINESTRING (305233 2.7814e+06, 305238 2.7814e+06) at 305235.20948094811 2781398.7662400398\n",
      "2025-04-27 18:44:22,109 - WARNING - Topology error for category Natural in neighborhood 陽明里: TopologyException: found non-noded intersection between LINESTRING (305351 2.78185e+06, 305350 2.78185e+06) and LINESTRING (305350 2.78185e+06, 305350 2.78185e+06) at 305349.93489947024 2781846.2793679931\n",
      "2025-04-27 18:44:22,871 - WARNING - Topology error for category Agriculture in neighborhood 豐年里: TopologyException: found non-noded intersection between LINESTRING (299444 2.78096e+06, 299444 2.78096e+06) and LINESTRING (299444 2.78096e+06, 299444 2.78096e+06) at 299443.97226843517 2780959.3288864125\n",
      "2025-04-27 18:44:23,532 - WARNING - Topology error for category Pedestrian in neighborhood 關渡里: TopologyException: found non-noded intersection between LINESTRING (296871 2.77919e+06, 296870 2.77919e+06) and LINESTRING (296871 2.77919e+06, 296869 2.77919e+06) at 296869.99175471725 2779190.4644867191\n",
      "2025-04-27 18:44:23,659 - WARNING - Topology error for category Infrastructure in neighborhood 關渡里: TopologyException: found non-noded intersection between LINESTRING (296911 2.77884e+06, 296911 2.77884e+06) and LINESTRING (296911 2.77884e+06, 296911 2.77884e+06) at 296911.44015440479 2778839.5138171767\n",
      "2025-04-27 18:44:26,156 - WARNING - Topology error for category Government in neighborhood 福順里: TopologyException: found non-noded intersection between LINESTRING (301533 2.77502e+06, 301533 2.77502e+06) and LINESTRING (301533 2.77502e+06, 301533 2.77502e+06) at 301532.96899694926 2775020.80209503\n",
      "2025-04-27 18:44:27,437 - WARNING - Topology error for category River in neighborhood 五分里: TopologyException: found non-noded intersection between LINESTRING (312333 2.77328e+06, 312333 2.77328e+06) and LINESTRING (312332 2.77328e+06, 312333 2.77328e+06) at 312332.69765419257 2773277.367184672\n",
      "2025-04-27 18:44:28,228 - WARNING - Topology error for category Road in neighborhood 南港里: TopologyException: found non-noded intersection between LINESTRING (311644 2.77189e+06, 311657 2.7719e+06) and LINESTRING (311630 2.77189e+06, 311657 2.7719e+06) at 311657.42832653591 2771895.1006993004\n",
      "2025-04-27 18:44:28,913 - WARNING - Topology error for category Special_Zone in neighborhood 中南里: TopologyException: found non-noded intersection between LINESTRING (311955 2.77135e+06, 311955 2.77135e+06) and LINESTRING (311955 2.77135e+06, 311955 2.77135e+06) at 311955.22635489638 2771352.7457217239\n",
      "2025-04-27 18:44:28,936 - WARNING - Topology error for category Road in neighborhood 中南里: TopologyException: found non-noded intersection between LINESTRING (311345 2.77161e+06, 311344 2.77161e+06) and LINESTRING (311344 2.77161e+06, 311344 2.77161e+06) at 311344.20670072001 2771612.8331892812\n",
      "2025-04-27 18:44:28,948 - WARNING - Topology error for category Infrastructure in neighborhood 中南里: TopologyException: found non-noded intersection between LINESTRING (311955 2.77135e+06, 311955 2.77135e+06) and LINESTRING (311955 2.77135e+06, 311955 2.77135e+06) at 311955.22635489638 2771352.7457217239\n",
      "2025-04-27 18:44:28,969 - WARNING - Topology error for category Government in neighborhood 中南里: TopologyException: found non-noded intersection between LINESTRING (311038 2.77019e+06, 311035 2.77019e+06) and LINESTRING (311038 2.77019e+06, 311035 2.77019e+06) at 311037.75737428927 2770193.7686056904\n",
      "2025-04-27 18:44:28,979 - WARNING - Topology error for category River in neighborhood 中南里: TopologyException: found non-noded intersection between LINESTRING (311955 2.77135e+06, 311955 2.77135e+06) and LINESTRING (311955 2.77135e+06, 311955 2.77135e+06) at 311955.22635489638 2771352.7457217239\n",
      "2025-04-27 18:44:30,777 - WARNING - Topology error for category Infrastructure in neighborhood 舊莊里: TopologyException: found non-noded intersection between LINESTRING (312826 2.76904e+06, 312824 2.76904e+06) and LINESTRING (312825 2.76904e+06, 312824 2.76904e+06) at 312824.50008723215 2769040.8990552658\n",
      "2025-04-27 18:44:31,367 - WARNING - Topology error for category Commercial in neighborhood 安康里: TopologyException: found non-noded intersection between LINESTRING (307950 2.76963e+06, 307949 2.76962e+06) and LINESTRING (307949 2.76962e+06, 307949 2.76962e+06) at 307948.62715117587 2769624.3576414455\n",
      "2025-04-27 18:44:31,501 - WARNING - Topology error for category River in neighborhood 九如里: TopologyException: found non-noded intersection between LINESTRING (312138 2.77015e+06, 312138 2.77014e+06) and LINESTRING (312138 2.77015e+06, 312138 2.77014e+06) at 312138.34700008662 2770150.6440004231\n",
      "2025-04-27 18:44:33,853 - WARNING - Topology error for category Road in neighborhood 博嘉里: TopologyException: found non-noded intersection between LINESTRING (308001 2.76569e+06, 308001 2.76569e+06) and LINESTRING (308001 2.76569e+06, 308000 2.76568e+06) at 308000.95038661739 2765685.1649155417\n",
      "2025-04-27 18:44:34,210 - WARNING - Topology error for category Residential in neighborhood 萬盛里: TopologyException: found non-noded intersection between LINESTRING (304449 2.76649e+06, 304450 2.7665e+06) and LINESTRING (304449 2.76648e+06, 304450 2.7665e+06) at 304449.97013593506 2766495.7391462782\n",
      "2025-04-27 18:44:34,542 - WARNING - Topology error for category Commercial in neighborhood 萬興里: TopologyException: found non-noded intersection between LINESTRING (307982 2.76418e+06, 307986 2.76418e+06) and LINESTRING (307982 2.76418e+06, 307984 2.76418e+06) at 307982.32430909772 2764175.6883213609\n",
      "2025-04-27 18:44:34,752 - WARNING - Topology error for category Residential in neighborhood 萬興里: TopologyException: found non-noded intersection between LINESTRING (307982 2.76418e+06, 307986 2.76418e+06) and LINESTRING (307982 2.76418e+06, 307984 2.76418e+06) at 307982.32430909772 2764175.6883213609\n",
      "2025-04-27 18:44:34,804 - WARNING - Topology error for category Natural in neighborhood 萬興里: TopologyException: found non-noded intersection between LINESTRING (310276 2.76631e+06, 310277 2.76631e+06) and LINESTRING (310277 2.76631e+06, 310277 2.76631e+06) at 310277.46693728981 2766309.5977024199\n",
      "2025-04-27 18:44:34,914 - WARNING - Topology error for category River in neighborhood 萬興里: TopologyException: found non-noded intersection between LINESTRING (307907 2.76419e+06, 307910 2.76418e+06) and LINESTRING (307907 2.76419e+06, 307907 2.76419e+06) at 307906.68941059953 2764186.3632669998\n",
      "2025-04-27 18:44:34,953 - WARNING - Topology error for category Infrastructure in neighborhood 萬興里: TopologyException: found non-noded intersection between LINESTRING (307982 2.76418e+06, 307986 2.76418e+06) and LINESTRING (307982 2.76418e+06, 307984 2.76418e+06) at 307982.32430909772 2764175.6883213609\n",
      "2025-04-27 18:44:35,016 - WARNING - Topology error for category Road in neighborhood 萬興里: TopologyException: found non-noded intersection between LINESTRING (309012 2.76609e+06, 309011 2.76609e+06) and LINESTRING (309011 2.76609e+06, 309016 2.76609e+06) at 309011.33430958074 2766091.7154930364\n",
      "2025-04-27 18:44:36,257 - INFO - Finished loading and preparing data.\n",
      "2025-04-27 18:44:36,297 - INFO - Computing correlation between road types and accident density...\n",
      "2025-04-27 18:44:36,318 - INFO - Roads CRS: {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"ProjectedCRS\", \"name\": \"TWD97 / TM2 zone 121\", \"base_crs\": {\"name\": \"TWD97\", \"datum\": {\"type\": \"GeodeticReferenceFrame\", \"name\": \"Taiwan Datum 1997\", \"ellipsoid\": {\"name\": \"GRS 1980\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257222101}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"id\": {\"authority\": \"EPSG\", \"code\": 3824}}, \"conversion\": {\"name\": \"Taiwan 2-degree TM zone 121\", \"method\": {\"name\": \"Transverse Mercator\", \"id\": {\"authority\": \"EPSG\", \"code\": 9807}}, \"parameters\": [{\"name\": \"Latitude of natural origin\", \"value\": 0, \"unit\": \"degree\", \"id\": {\"authority\": \"EPSG\", \"code\": 8801}}, {\"name\": \"Longitude of natural origin\", \"value\": 121, \"unit\": \"degree\", \"id\": {\"authority\": \"EPSG\", \"code\": 8802}}, {\"name\": \"Scale factor at natural origin\", \"value\": 0.9999, \"unit\": \"unity\", \"id\": {\"authority\": \"EPSG\", \"code\": 8805}}, {\"name\": \"False easting\", \"value\": 250000, \"unit\": \"metre\", \"id\": {\"authority\": \"EPSG\", \"code\": 8806}}, {\"name\": \"False northing\", \"value\": 0, \"unit\": \"metre\", \"id\": {\"authority\": \"EPSG\", \"code\": 8807}}]}, \"coordinate_system\": {\"subtype\": \"Cartesian\", \"axis\": [{\"name\": \"Easting\", \"abbreviation\": \"X\", \"direction\": \"east\", \"unit\": \"metre\"}, {\"name\": \"Northing\", \"abbreviation\": \"Y\", \"direction\": \"north\", \"unit\": \"metre\"}]}, \"scope\": \"Engineering survey, topographic mapping.\", \"area\": \"Taiwan, Republic of China - between 120\\u00b0E and 122\\u00b0E, onshore and offshore - Taiwan Island.\", \"bbox\": {\"south_latitude\": 20.41, \"west_longitude\": 119.99, \"north_latitude\": 26.72, \"east_longitude\": 122.06}, \"id\": {\"authority\": \"EPSG\", \"code\": 3826}}, Bounds: [ -49201.34316331 2687086.8646322   340402.47812579 3107530.84473387]\n",
      "2025-04-27 18:44:36,319 - INFO - Neighborhoods CRS: EPSG:3826, Bounds: [ 296266.05303084 2761514.89561711  317197.26073793 2789176.16901603]\n",
      "2025-04-27 18:44:36,323 - INFO - Accidents CRS: EPSG:3826, Bounds: [ 295756.69149719 2761989.77126713  315058.49224312 2786732.8587656 ]\n",
      "2025-04-27 18:44:36,329 - INFO - Roads geometry types: ['LineString']\n",
      "2025-04-27 18:44:36,330 - INFO - Neighborhoods geometry types: ['Polygon']\n",
      "2025-04-27 18:44:36,332 - INFO - Sample road geometries:\n",
      "0    LINESTRING (324778.51079599274 2780945.2627613...\n",
      "1    LINESTRING (296169.2235130913 2759463.11395603...\n",
      "2    LINESTRING (296381.58548612776 2758713.2679554...\n",
      "3    LINESTRING (297130.61534516735 2759165.1029101...\n",
      "4    LINESTRING (296863.93588555494 2759022.6435459...\n",
      "Name: geometry, dtype: object\n",
      "2025-04-27 18:44:36,333 - INFO - Sample neighborhood geometries:\n",
      "0    POLYGON ((302666.54271737445 2785226.842290448...\n",
      "1    POLYGON ((307802.16998795647 2787372.759303745...\n",
      "2    POLYGON ((302320.04429191677 2784654.093771082...\n",
      "3    POLYGON ((308159.7721281759 2784411.3655833476...\n",
      "4    POLYGON ((302319.03440019337 2784368.948626034...\n",
      "Name: geometry, dtype: object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Structure Summary ---\n",
      "\n",
      "Dataset: neighborhoods\n",
      "Shape: (456, 68)\n",
      "Columns and Data Types:\n",
      "LIE_NAME                            object\n",
      "SECT_NAME                           object\n",
      "2024population                       int32\n",
      "land_use_city_open_area_count        int32\n",
      "land_use_city_open_area_area_m2    float64\n",
      "                                    ...   \n",
      "transit_count                        int64\n",
      "accident_count                       int64\n",
      "road_density                       float64\n",
      "total_population                     int64\n",
      "elderly_percentage                 float64\n",
      "Length: 68, dtype: object\n",
      "Missing values (total): 0\n",
      "Missing values per column:\n",
      "LIE_NAME                           0\n",
      "SECT_NAME                          0\n",
      "2024population                     0\n",
      "land_use_city_open_area_count      0\n",
      "land_use_city_open_area_area_m2    0\n",
      "                                  ..\n",
      "transit_count                      0\n",
      "accident_count                     0\n",
      "road_density                       0\n",
      "total_population                   0\n",
      "elderly_percentage                 0\n",
      "Length: 68, dtype: int64\n",
      "Unique LIE_NAME: 456\n",
      "Sample data (first 2 rows):\n",
      "  LIE_NAME SECT_NAME  2024population  land_use_city_open_area_count  \\\n",
      "0      湖田里       北投區             856                              0   \n",
      "1      菁山里       士林區            1509                              0   \n",
      "\n",
      "   land_use_city_open_area_area_m2  land_use_city_open_area_percent  \\\n",
      "0                              0.0                              0.0   \n",
      "1                              0.0                              0.0   \n",
      "\n",
      "   land_use_commercial_count  land_use_commercial_area_m2  \\\n",
      "0                          0                          0.0   \n",
      "1                          0                          0.0   \n",
      "\n",
      "   land_use_commercial_percent  land_use_infrastructure_count  ...  \\\n",
      "0                          0.0                              0  ...   \n",
      "1                          0.0                              4  ...   \n",
      "\n",
      "   intersection_count       area_m2   area_km2  intersection_density  \\\n",
      "0                 110  1.632443e+07  16.324434              6.738365   \n",
      "1                  94  1.171922e+07  11.719218              8.021013   \n",
      "\n",
      "   tree_count  transit_count  accident_count  road_density  total_population  \\\n",
      "0         106            400              70      7.561174               857   \n",
      "1          54            313              33      9.913151              1485   \n",
      "\n",
      "   elderly_percentage  \n",
      "0               28.24  \n",
      "1               25.72  \n",
      "\n",
      "[2 rows x 68 columns]\n",
      "\n",
      "Dataset: buildings\n",
      "Shape: (74306, 9)\n",
      "Columns and Data Types:\n",
      "full_id       object\n",
      "osm_id        object\n",
      "building      object\n",
      "屋齡            object\n",
      "建物高度          object\n",
      "地上層數          object\n",
      "構造種類          object\n",
      "使用分區          object\n",
      "geometry    geometry\n",
      "dtype: object\n",
      "Missing values (total): 49\n",
      "Missing values per column:\n",
      "full_id      0\n",
      "osm_id       0\n",
      "building    31\n",
      "屋齡           0\n",
      "建物高度         0\n",
      "地上層數        18\n",
      "構造種類         0\n",
      "使用分區         0\n",
      "geometry     0\n",
      "dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "    full_id   osm_id    building    屋齡  建物高度  地上層數     構造種類     使用分區  \\\n",
      "0  r2633015  2633015   dormitory  <NA>  <NA>  <NA>  Unknown  Unknown   \n",
      "1  r2633016  2633016  university  <NA>  <NA>  <NA>  Unknown  Unknown   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((304401.483 2780904.12, 304430.995 27...  \n",
      "1  POLYGON ((304357.549 2780790.65, 304352.56 278...  \n",
      "\n",
      "Dataset: roads\n",
      "Shape: (81444, 3)\n",
      "Columns and Data Types:\n",
      "class         object\n",
      "geometry    geometry\n",
      "length_m     float64\n",
      "dtype: object\n",
      "Missing values (total): 604\n",
      "Missing values per column:\n",
      "class       604\n",
      "geometry      0\n",
      "length_m      0\n",
      "dtype: int64\n",
      "Road class counts:\n",
      "class\n",
      "service          21575\n",
      "footway          19776\n",
      "residential      15429\n",
      "tertiary          5402\n",
      "steps             4301\n",
      "secondary         4059\n",
      "path              3857\n",
      "unclassified      1957\n",
      "primary           1292\n",
      "cycleway           878\n",
      "track              741\n",
      "trunk              609\n",
      "pedestrian         323\n",
      "motorway           316\n",
      "living_street      267\n",
      "unknown             56\n",
      "bridleway            2\n",
      "Name: count, dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "     class                                           geometry       length_m\n",
      "0     None  LINESTRING (324778.511 2780945.263, 324826.86 ...  426414.891763\n",
      "1  service  LINESTRING (296169.224 2759463.114, 296160.343...    1055.960489\n",
      "\n",
      "Dataset: trees\n",
      "Shape: (5019, 12)\n",
      "Columns and Data Types:\n",
      "id               object\n",
      "geometry       geometry\n",
      "version           int32\n",
      "sources          object\n",
      "subtype          object\n",
      "class            object\n",
      "surface          object\n",
      "names            object\n",
      "level           float64\n",
      "source_tags      object\n",
      "wikidata         object\n",
      "elevation       float64\n",
      "dtype: object\n",
      "Missing values (total): 24487\n",
      "Missing values per column:\n",
      "id                0\n",
      "geometry          0\n",
      "version           0\n",
      "sources           0\n",
      "subtype           0\n",
      "class             0\n",
      "surface        5013\n",
      "names          4553\n",
      "level          5015\n",
      "source_tags       0\n",
      "wikidata       4943\n",
      "elevation      4963\n",
      "dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "                                 id  \\\n",
      "0  08b4ba2399d31fff0003c5f62a41c335   \n",
      "1  08b4ba0ae3a22fff0003ca54f368c81d   \n",
      "\n",
      "                                            geometry  version  \\\n",
      "0  POLYGON ((161777.639 2544255.739, 161813.286 2...        0   \n",
      "1  POLYGON ((297620.639 2760911.484, 297592.092 2...        0   \n",
      "\n",
      "                                             sources subtype   class surface  \\\n",
      "0  [{'property': '', 'dataset': 'OpenStreetMap', ...    land  island    None   \n",
      "1  [{'property': '', 'dataset': 'OpenStreetMap', ...  forest    wood    None   \n",
      "\n",
      "                                               names  level  \\\n",
      "0  {'primary': '臺灣', 'common': [('af', 'Taiwan'),...    NaN   \n",
      "1                                               None    NaN   \n",
      "\n",
      "                               source_tags wikidata  elevation  \n",
      "0  [(place, island), (type, multipolygon)]   Q22502        NaN  \n",
      "1  [(natural, wood), (type, multipolygon)]     None        NaN  \n",
      "\n",
      "Dataset: transit\n",
      "Shape: (29892, 11)\n",
      "Columns and Data Types:\n",
      "id               object\n",
      "geometry       geometry\n",
      "version           int32\n",
      "sources          object\n",
      "subtype          object\n",
      "class            object\n",
      "surface          object\n",
      "names            object\n",
      "level           float64\n",
      "source_tags      object\n",
      "wikidata         object\n",
      "dtype: object\n",
      "Missing values (total): 101060\n",
      "Missing values per column:\n",
      "id                 0\n",
      "geometry           0\n",
      "version            0\n",
      "sources            0\n",
      "subtype            0\n",
      "class              0\n",
      "surface        28287\n",
      "names          17731\n",
      "level          25803\n",
      "source_tags        0\n",
      "wikidata       29239\n",
      "dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "                                 id  \\\n",
      "0  08b4ba0ac6758fff0001be0bbe7a4ada   \n",
      "1  08b4ba0ae332afff0001a76b5977464d   \n",
      "\n",
      "                                            geometry  version  \\\n",
      "0  LINESTRING (320296.896 2765488.056, 320042.372...        0   \n",
      "1                     POINT (296843.534 2759002.204)        0   \n",
      "\n",
      "                                             sources  subtype       class  \\\n",
      "0  [{'property': '', 'dataset': 'OpenStreetMap', ...    power  power_line   \n",
      "1  [{'property': '', 'dataset': 'OpenStreetMap', ...  barrier        gate   \n",
      "\n",
      "  surface names  level                         source_tags wikidata  \n",
      "0    None  None    NaN  [(power, line), (voltage, 345000)]     None  \n",
      "1    None  None    NaN                   [(barrier, gate)]     None  \n",
      "\n",
      "Dataset: urban_masterplan\n",
      "Shape: (15521, 15)\n",
      "Columns and Data Types:\n",
      "編號            object\n",
      "圖層            object\n",
      "顏色            object\n",
      "街廓編號          object\n",
      "分區代碼          object\n",
      "分區簡稱          object\n",
      "使用分區          object\n",
      "分區說明          object\n",
      "原屬分區          object\n",
      "變更前代碼         object\n",
      "變更前簡稱         object\n",
      "變更前分區         object\n",
      "Category      object\n",
      "Area         float64\n",
      "geometry    geometry\n",
      "dtype: object\n",
      "Missing values (total): 92705\n",
      "Missing values per column:\n",
      "編號              0\n",
      "圖層              5\n",
      "顏色              5\n",
      "街廓編號        15508\n",
      "分區代碼            5\n",
      "分區簡稱            5\n",
      "使用分區            0\n",
      "分區說明        15409\n",
      "原屬分區        15205\n",
      "變更前代碼       15521\n",
      "變更前簡稱       15521\n",
      "變更前分區       15521\n",
      "Category        0\n",
      "Area            0\n",
      "geometry        0\n",
      "dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "  編號  圖層  顏色  街廓編號 分區代碼 分區簡稱  使用分區  分區說明  原屬分區 變更前代碼 變更前簡稱 變更前分區  \\\n",
      "0  1  32  19  None  PEA    公  公園用地  None  None  None  None  None   \n",
      "1  2  32  19  None  PEA    公  公園用地  None  None  None  None  None   \n",
      "\n",
      "         Category      Area                                           geometry  \n",
      "0  City_Open_Area   823.190  MULTIPOLYGON (((303340.099 2771175.776, 303329...  \n",
      "1  City_Open_Area  4721.047  MULTIPOLYGON (((303230.925 2771108.301, 303088...  \n",
      "\n",
      "Dataset: accidents\n",
      "Shape: (56133, 8)\n",
      "Columns and Data Types:\n",
      "Month            object\n",
      "Day               int32\n",
      "Hours             int32\n",
      "Minute            int32\n",
      "Location         object\n",
      "Speed_limit       int32\n",
      "Roadtype          int32\n",
      "geometry       geometry\n",
      "dtype: object\n",
      "Missing values (total): 0\n",
      "Missing values per column:\n",
      "Month          0\n",
      "Day            0\n",
      "Hours          0\n",
      "Minute         0\n",
      "Location       0\n",
      "Speed_limit    0\n",
      "Roadtype       0\n",
      "geometry       0\n",
      "dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "     Month  Day  Hours  Minute              Location  Speed_limit  Roadtype  \\\n",
      "0  January    1      0      46  大安區仁愛路4段與忠孝東路4段216巷口           50         4   \n",
      "1  January    1      0      46  大安區仁愛路4段與忠孝東路4段216巷口           50         4   \n",
      "\n",
      "                        geometry  \n",
      "0  POINT (305807.42 2770096.759)  \n",
      "1  POINT (305807.42 2770096.759)  \n",
      "\n",
      "Dataset: population\n",
      "Shape: (456, 3)\n",
      "Columns and Data Types:\n",
      "LIE_NAME               object\n",
      "Total_Population        int64\n",
      "Elderly_Percentage    float64\n",
      "dtype: object\n",
      "Missing values (total): 0\n",
      "Missing values per column:\n",
      "LIE_NAME              0\n",
      "Total_Population      0\n",
      "Elderly_Percentage    0\n",
      "dtype: int64\n",
      "Unique LIE_NAME: 456\n",
      "Sample data (first 2 rows):\n",
      "  LIE_NAME  Total_Population  Elderly_Percentage\n",
      "0      南福里             12021               16.10\n",
      "1      奇岩里             11200               22.68\n",
      "--- End of Data Structure Summary ---\n",
      "\n",
      "Starting compute_road_type_accident_correlation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_118423/2344334293.py:35: UserWarning: Legend does not support handles for PatchCollection instances.\n",
      "See: https://matplotlib.org/stable/tutorials/intermediate/legend_guide.html#implementing-a-custom-legend-handler\n",
      "  plt.legend()\n",
      "2025-04-27 18:44:37,758 - INFO - Overlay plot saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/roads_neighborhoods_overlap.png\n",
      "2025-04-27 18:44:38,201 - INFO - Assigning accidents to nearest road...\n",
      "2025-04-27 18:44:42,736 - INFO - Matched 56133 accidents out of 56133\n",
      "2025-04-27 18:44:42,740 - INFO - Reassigning 4991 accidents from footway/cycleway...\n",
      "2025-04-27 18:44:42,835 - INFO - Reassigned 1429 accidents to wider roads\n",
      "2025-04-27 18:44:42,849 - INFO - Accidents by road type:\n",
      "class\n",
      "bridleway            0\n",
      "cycleway           247\n",
      "footway           3315\n",
      "living_street       79\n",
      "motorway           109\n",
      "path                86\n",
      "pedestrian          84\n",
      "primary           6535\n",
      "residential      10110\n",
      "secondary        16180\n",
      "service           5011\n",
      "steps               47\n",
      "tertiary          9135\n",
      "track                8\n",
      "trunk             2493\n",
      "unclassified      1665\n",
      "unknown             66\n",
      "Name: accident_count, dtype: int64\n",
      "2025-04-27 18:44:42,857 - INFO - length_m stats:\n",
      "count     75149.000000\n",
      "mean        157.303341\n",
      "std        2399.131756\n",
      "min          10.000152\n",
      "25%          35.129323\n",
      "50%          68.801448\n",
      "75%         139.983163\n",
      "max      426414.891763\n",
      "Name: length_m, dtype: float64\n",
      "2025-04-27 18:44:42,859 - INFO - accident_count stats:\n",
      "count    75149.000000\n",
      "mean         0.739172\n",
      "std          3.223498\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max        202.000000\n",
      "Name: accident_count, dtype: float64\n",
      "2025-04-27 18:44:42,861 - INFO - NaN in accident_density: 0\n",
      "2025-04-27 18:44:42,864 - INFO - accident_density stats:\n",
      "count    75149.000000\n",
      "mean         4.820610\n",
      "std         23.976435\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max       1153.699019\n",
      "Name: accident_density, dtype: float64\n",
      "2025-04-27 18:44:42,867 - INFO - Road type counts:\n",
      "class\n",
      "service          21204\n",
      "footway          16755\n",
      "residential      14861\n",
      "tertiary          5113\n",
      "secondary         3869\n",
      "path              3610\n",
      "steps             2968\n",
      "unclassified      1894\n",
      "primary           1209\n",
      "cycleway           825\n",
      "track              716\n",
      "trunk              593\n",
      "motorway           313\n",
      "pedestrian         297\n",
      "living_street      264\n",
      "unknown             54\n",
      "bridleway            2\n",
      "Name: count, dtype: int64\n",
      "2025-04-27 18:44:42,882 - INFO - Spearman's correlation between road width rank and accident density: 0.798 (p-value: 0.001)\n",
      "2025-04-27 18:44:42,883 - INFO - Computing average road accident density per neighborhood...\n",
      "2025-04-27 18:44:42,883 - INFO - Roads DataFrame shape before join: (75149, 6)\n",
      "2025-04-27 18:44:42,884 - INFO - Neighborhoods DataFrame shape before join: (456, 68)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Road type counts:\n",
      "class\n",
      "service          21204\n",
      "footway          16755\n",
      "residential      14861\n",
      "tertiary          5113\n",
      "secondary         3869\n",
      "path              3610\n",
      "steps             2968\n",
      "unclassified      1894\n",
      "primary           1209\n",
      "cycleway           825\n",
      "track              716\n",
      "trunk              593\n",
      "motorway           313\n",
      "pedestrian         297\n",
      "living_street      264\n",
      "unknown             54\n",
      "bridleway            2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Road Type Accident Density Summary ---\n",
      "            class    length_m  accident_count  accident_density  width_rank\n",
      "1        cycleway   263682.07             245              0.26           1\n",
      "2         footway  1767503.72            3235              0.73           1\n",
      "3   living_street    23929.78              77              1.58           3\n",
      "4        motorway   215317.82             109              2.74           5\n",
      "5            path   720602.60              84              0.09           1\n",
      "6      pedestrian    31830.26              82              0.47           1\n",
      "7         primary   212330.19            6433             44.39           4\n",
      "8     residential  1455020.72           10037              4.87           3\n",
      "9       secondary   534770.12           15993             36.32           4\n",
      "10        service  2502054.44            5001              0.68           2\n",
      "11          steps   161013.29              37              0.10           1\n",
      "12       tertiary   755660.86            9060             11.17           3\n",
      "13          track   159053.47               8              0.02           2\n",
      "14          trunk   265713.76            2469             11.69           5\n",
      "Spearman's correlation: 0.798 (p-value: 0.001)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 18:44:45,640 - INFO - Road-neighborhood join resulted in 86725 matches with columns: ['geometry', 'class', 'length_m', 'width_rank', 'accident_density', 'index_right', 'LIE_NAME']\n",
      "2025-04-27 18:44:45,642 - INFO - Non-NaN LIE_NAME count: 62277\n",
      "2025-04-27 18:44:45,643 - INFO - Non-NaN accident_density count: 86725\n",
      "2025-04-27 18:44:45,646 - INFO - Unique LIE_NAME values: 456\n",
      "2025-04-27 18:44:45,652 - INFO - Number of neighborhoods with calculated avg_accident_density: 456\n",
      "2025-04-27 18:44:45,653 - INFO - NaN in avg_accident_density: 0\n",
      "2025-04-27 18:44:45,655 - INFO - Assigned avg_road_accident_density to 456 neighborhoods\n",
      "2025-04-27 18:44:45,657 - INFO - Avg road accident density stats:\n",
      "count    456.000000\n",
      "mean       8.928906\n",
      "std        5.840511\n",
      "min        0.010199\n",
      "25%        4.955368\n",
      "50%        7.926718\n",
      "75%       12.076502\n",
      "max       33.147588\n",
      "Name: avg_road_accident_density, dtype: float64\n",
      "2025-04-27 18:44:45,819 - INFO - Bar chart saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_bar.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bar chart saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_bar.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 18:44:46,254 - INFO - Box chart saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_box.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box chart saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_box.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 18:44:46,509 - INFO - Scatter plot saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_scatter.png\n",
      "2025-04-27 18:44:46,512 - INFO - Top 3 road types by accident density:\n",
      "        class  accident_density\n",
      "7     primary             44.39\n",
      "9   secondary             36.32\n",
      "14      trunk             11.69\n",
      "2025-04-27 18:44:46,513 - INFO - Computing pedestrian road density per neighborhood...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scatter plot saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_scatter.png\n",
      "Top 3 road types by accident density:\n",
      "        class  accident_density\n",
      "7     primary             44.39\n",
      "9   secondary             36.32\n",
      "14      trunk             11.69\n",
      "Starting compute_pedestrian_road_density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 18:44:46,920 - INFO - pedestrian_road_density stats:\n",
      "count    456.000000\n",
      "mean      15.841759\n",
      "std       10.276229\n",
      "min        0.000000\n",
      "25%        8.202007\n",
      "50%       15.006599\n",
      "75%       21.558429\n",
      "max       57.267153\n",
      "Name: pedestrian_road_density, dtype: float64\n",
      "2025-04-27 18:44:46,924 - INFO - Stage 2: Building city graph...\n",
      "2025-04-27 18:44:46,925 - INFO - Dataset neighborhoods column types:\n",
      "LIE_NAME                            object\n",
      "SECT_NAME                           object\n",
      "2024population                       int32\n",
      "land_use_city_open_area_count        int32\n",
      "land_use_city_open_area_area_m2    float64\n",
      "                                    ...   \n",
      "road_density                       float64\n",
      "total_population                     int64\n",
      "elderly_percentage                 float64\n",
      "avg_road_accident_density          float64\n",
      "pedestrian_road_density            float64\n",
      "Length: 70, dtype: object\n",
      "2025-04-27 18:44:46,935 - INFO - Dataset buildings column types:\n",
      "full_id       object\n",
      "osm_id        object\n",
      "building      object\n",
      "屋齡            object\n",
      "建物高度          object\n",
      "地上層數          object\n",
      "構造種類          object\n",
      "使用分區          object\n",
      "geometry    geometry\n",
      "dtype: object\n",
      "2025-04-27 18:44:46,938 - INFO - Dataset roads column types:\n",
      "class         object\n",
      "geometry    geometry\n",
      "length_m     float64\n",
      "dtype: object\n",
      "2025-04-27 18:44:46,940 - INFO - Dataset trees column types:\n",
      "id               object\n",
      "geometry       geometry\n",
      "version           int32\n",
      "sources          object\n",
      "subtype          object\n",
      "class            object\n",
      "surface          object\n",
      "names            object\n",
      "level           float64\n",
      "source_tags      object\n",
      "wikidata         object\n",
      "elevation       float64\n",
      "dtype: object\n",
      "2025-04-27 18:44:46,944 - INFO - Dataset transit column types:\n",
      "id               object\n",
      "geometry       geometry\n",
      "version           int32\n",
      "sources          object\n",
      "subtype          object\n",
      "class            object\n",
      "surface          object\n",
      "names            object\n",
      "level           float64\n",
      "source_tags      object\n",
      "wikidata         object\n",
      "dtype: object\n",
      "2025-04-27 18:44:46,947 - INFO - Dataset urban_masterplan column types:\n",
      "編號            object\n",
      "圖層            object\n",
      "顏色            object\n",
      "街廓編號          object\n",
      "分區代碼          object\n",
      "分區簡稱          object\n",
      "使用分區          object\n",
      "分區說明          object\n",
      "原屬分區          object\n",
      "變更前代碼         object\n",
      "變更前簡稱         object\n",
      "變更前分區         object\n",
      "Category      object\n",
      "Area         float64\n",
      "geometry    geometry\n",
      "dtype: object\n",
      "2025-04-27 18:44:46,950 - INFO - Dataset accidents column types:\n",
      "Month            object\n",
      "Day               int32\n",
      "Hours             int32\n",
      "Minute            int32\n",
      "Location         object\n",
      "Speed_limit       int32\n",
      "Roadtype          int32\n",
      "geometry       geometry\n",
      "dtype: object\n",
      "2025-04-27 18:44:46,951 - INFO - Dataset population column types:\n",
      "LIE_NAME               object\n",
      "Total_Population        int64\n",
      "Elderly_Percentage    float64\n",
      "dtype: object\n",
      "2025-04-27 18:44:46,953 - INFO - Adding neighborhood nodes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting build_graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neighborhood nodes: 100%|██████████| 456/456 [00:00<00:00, 12142.81it/s]\n",
      "2025-04-27 18:44:46,994 - INFO - Adding building nodes...\n",
      "2025-04-27 18:44:46,994 - WARNING - 'area_m2' missing. Computing from geometry...\n",
      "Building nodes: 100%|██████████| 74306/74306 [00:03<00:00, 21517.15it/s]\n",
      "2025-04-27 18:44:50,453 - INFO - Adding road nodes...\n",
      "Road nodes: 100%|██████████| 81444/81444 [00:03<00:00, 21348.82it/s]\n",
      "2025-04-27 18:44:54,941 - INFO - Converting GeoDataFrames to cudf for GPU processing...\n",
      "2025-04-27 18:44:55,067 - INFO - Extracting bounding box coordinates...\n",
      "2025-04-27 18:44:57,026 - INFO - neighborhoods_cudf['min_x'] dtype: float64\n",
      "2025-04-27 18:44:57,027 - INFO - buildings_cudf['min_x'] dtype: float64\n",
      "2025-04-27 18:44:57,027 - INFO - roads_cudf['min_x'] dtype: float64\n",
      "2025-04-27 18:44:57,027 - INFO - Creating edges using GPU-accelerated spatial joins...\n",
      "2025-04-27 18:44:57,028 - INFO - Computing neighborhood-neighborhood edges...\n",
      "Neighborhood-Neighborhood edges:   0%|          | 0/456 [00:00<?, ?it/s]2025-04-27 18:44:57,063 - INFO - init\n",
      "Neighborhood-Neighborhood edges: 100%|██████████| 456/456 [00:05<00:00, 81.37it/s]\n",
      "2025-04-27 18:45:02,634 - INFO - Computing neighborhood-building edges...\n",
      "Neighborhood-Building edges: 100%|██████████| 456/456 [00:05<00:00, 78.50it/s]\n",
      "2025-04-27 18:45:08,445 - INFO - Computing neighborhood-road edges...\n",
      "Neighborhood-Road edges: 100%|██████████| 456/456 [00:04<00:00, 97.68it/s] \n",
      "2025-04-27 18:45:13,116 - INFO - Edge counts by type: {'neighborhood-neighborhood': 3292, 'neighborhood-building': 143476, 'neighborhood-road': 124141}\n",
      "2025-04-27 18:45:13,209 - INFO - Created 270909 total edges\n",
      "2025-04-27 18:45:13,256 - INFO - After validation, 270909 edges remain\n",
      "2025-04-27 18:45:13,259 - INFO - Sample edges after validation:\n",
      "   src  dst\n",
      "0    0    1\n",
      "1    0    2\n",
      "2    0    3\n",
      "3    0    4\n",
      "4    0    5\n",
      "2025-04-27 18:45:13,375 - INFO - Saving graph data to cache...\n",
      "2025-04-27 18:45:13,600 - INFO - Successfully saved graph data to cache.\n",
      "2025-04-27 18:45:13,601 - INFO - City graph constructed: 156206 nodes, 270909 edges\n",
      "2025-04-27 18:45:13,628 - INFO - Graph edge count: 270909\n",
      "2025-04-27 18:45:13,629 - INFO - Computing walkability scores for neighborhoods...\n",
      "2025-04-27 18:45:13,750 - INFO - land_use_score stats:\n",
      "count    456.000000\n",
      "mean       0.625839\n",
      "std        0.070916\n",
      "min        0.084592\n",
      "25%        0.606048\n",
      "50%        0.635505\n",
      "75%        0.664581\n",
      "max        0.773433\n",
      "Name: land_use_score, dtype: float64\n",
      "2025-04-27 18:45:13,752 - INFO - population_density stats:\n",
      "count       456.000000\n",
      "mean      27391.102721\n",
      "std       17989.464597\n",
      "min          52.497992\n",
      "25%       13417.369202\n",
      "50%       26826.507618\n",
      "75%       38590.694516\n",
      "max      106666.040616\n",
      "Name: population_density, dtype: float64\n",
      "2025-04-27 18:45:13,755 - INFO - transit_density stats:\n",
      "count     456.000000\n",
      "mean      138.214021\n",
      "std       128.415255\n",
      "min         0.000000\n",
      "25%        69.172097\n",
      "50%       112.735680\n",
      "75%       177.650984\n",
      "max      1954.897814\n",
      "Name: transit_density, dtype: float64\n",
      "2025-04-27 18:45:13,757 - INFO - ndvi stats:\n",
      "count    456.000000\n",
      "mean       0.288762\n",
      "std        0.193386\n",
      "min       -0.077327\n",
      "25%        0.154659\n",
      "50%        0.223579\n",
      "75%        0.365020\n",
      "max        0.818806\n",
      "Name: ndvi, dtype: float64\n",
      "2025-04-27 18:45:13,759 - INFO - green_space stats:\n",
      "count    456.000000\n",
      "mean       0.288762\n",
      "std        0.193386\n",
      "min       -0.077327\n",
      "25%        0.154659\n",
      "50%        0.223579\n",
      "75%        0.365020\n",
      "max        0.818806\n",
      "Name: green_space, dtype: float64\n",
      "2025-04-27 18:45:13,763 - INFO - accident_density stats:\n",
      "count     456.000000\n",
      "mean      808.327531\n",
      "std       609.766379\n",
      "min         0.000000\n",
      "25%       327.122362\n",
      "50%       713.002427\n",
      "75%      1141.046173\n",
      "max      3756.339295\n",
      "Name: accident_density, dtype: float64\n",
      "2025-04-27 18:45:13,765 - INFO - intersection_density stats:\n",
      "count    456.000000\n",
      "mean     126.190854\n",
      "std       83.149602\n",
      "min        2.960154\n",
      "25%       66.874411\n",
      "50%      111.292996\n",
      "75%      165.079134\n",
      "max      490.471026\n",
      "Name: intersection_density, dtype: float64\n",
      "2025-04-27 18:45:13,770 - INFO - Normalization statistics:\n",
      "2025-04-27 18:45:13,771 - INFO - land_use_score - median: 0.0000, IQR: 1.0000\n",
      "2025-04-27 18:45:13,772 - INFO - intersection_density - median: 111.2930, IQR: 98.2047\n",
      "2025-04-27 18:45:13,772 - INFO - population_density - median: 26826.5076, IQR: 25173.3253\n",
      "2025-04-27 18:45:13,773 - INFO - transit_density - median: 112.7357, IQR: 108.4789\n",
      "2025-04-27 18:45:13,773 - INFO - green_space - median: 0.2236, IQR: 0.2104\n",
      "2025-04-27 18:45:13,774 - INFO - accident_density - median: 713.0024, IQR: 813.9238\n",
      "2025-04-27 18:45:13,779 - INFO - walkability_score stats:\n",
      "count    456.000000\n",
      "mean       0.359348\n",
      "std        0.040287\n",
      "min        0.233758\n",
      "25%        0.334115\n",
      "50%        0.359866\n",
      "75%        0.383302\n",
      "max        0.593854\n",
      "Name: walkability_score, dtype: float64\n",
      "2025-04-27 18:45:13,780 - INFO - Walkability category thresholds - low: 0.3436, high: 0.3736\n",
      "2025-04-27 18:45:13,782 - INFO - Walkability category distribution:\n",
      "walkability_category\n",
      "high      155\n",
      "low       151\n",
      "medium    150\n",
      "Name: count, dtype: int64\n",
      "2025-04-27 18:45:13,789 - INFO - Number of neighborhood nodes in nodes_df: 456\n",
      "2025-04-27 18:45:13,790 - INFO - Number of entries in walkability_components: 456\n",
      "2025-04-27 18:45:13,796 - INFO - Sample LIE_NAME in nodes_df: ['湖田里', '菁山里', '大屯里', '平等里', '泉源里']\n",
      "2025-04-27 18:45:13,797 - INFO - Sample LIE_NAME in walkability_components: ['湖田里', '菁山里', '大屯里', '平等里', '泉源里']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting compute_walkability_scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 18:45:13,915 - INFO - Finished computing walkability scores.\n",
      "2025-04-27 18:45:13,918 - INFO - Preparing data for GNN training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prepare_gnn_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing features by node type:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-27 18:45:13,994 - INFO - Node type neighborhood: 456 nodes, feature shape: (456, 127)\n",
      "2025-04-27 18:45:14,265 - INFO - Node type building: 74306 nodes, feature shape: (74306, 127)\n",
      "Normalizing features by node type:  67%|██████▋   | 2/3 [00:00<00:00,  5.88it/s]2025-04-27 18:45:14,438 - INFO - Node type road: 81444 nodes, feature shape: (81444, 127)\n",
      "Normalizing features by node type: 100%|██████████| 3/3 [00:00<00:00,  5.78it/s]\n",
      "2025-04-27 18:45:14,529 - INFO - Edge index created with 270909 edges\n",
      "2025-04-27 18:45:14,540 - INFO - Prepared GNN data: 156206 nodes, 270909 edges\n",
      "2025-04-27 18:45:14,540 - INFO - Feature matrix shape: torch.Size([156206, 127])\n",
      "2025-04-27 18:45:14,541 - INFO - Label tensor shape: torch.Size([156206, 1])\n",
      "2025-04-27 18:45:14,544 - INFO - Stage 4: Training GNN model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting train_gnn_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 18:45:14,834 - INFO - Target (walkability_score) distribution for neighborhood nodes:\n",
      "count    456.000000\n",
      "mean       0.359348\n",
      "std        0.040287\n",
      "min        0.233758\n",
      "25%        0.334115\n",
      "50%        0.359866\n",
      "75%        0.383302\n",
      "max        0.593854\n",
      "dtype: float64\n",
      "Training epochs:   0%|          | 0/500 [00:00<?, ?it/s]2025-04-27 18:45:15,554 - INFO - Epoch 0, Train Loss: 0.0046, Train MAE: 0.0556, Train R2: -1.6796, Val Loss: 0.0069, Val MAE: 0.0766, Val R2: -4.4969\n",
      "Training epochs:   2%|▏         | 10/500 [00:02<01:56,  4.21it/s]2025-04-27 18:45:17,846 - INFO - Epoch 10, Train Loss: 0.0016, Train MAE: 0.0306, Train R2: 0.0589, Val Loss: 0.0025, Val MAE: 0.0422, Val R2: -0.9846\n",
      "Training epochs:   4%|▍         | 20/500 [00:04<01:43,  4.66it/s]2025-04-27 18:45:20,010 - INFO - Epoch 20, Train Loss: 0.0013, Train MAE: 0.0261, Train R2: 0.2636, Val Loss: 0.0011, Val MAE: 0.0257, Val R2: 0.0966\n",
      "Training epochs:   6%|▌         | 30/500 [00:07<01:41,  4.62it/s]2025-04-27 18:45:22,163 - INFO - Epoch 30, Train Loss: 0.0010, Train MAE: 0.0228, Train R2: 0.4355, Val Loss: 0.0010, Val MAE: 0.0242, Val R2: 0.2023\n",
      "Training epochs:   8%|▊         | 40/500 [00:09<01:38,  4.68it/s]2025-04-27 18:45:24,304 - INFO - Epoch 40, Train Loss: 0.0009, Train MAE: 0.0223, Train R2: 0.4735, Val Loss: 0.0009, Val MAE: 0.0234, Val R2: 0.2470\n",
      "Training epochs:  10%|█         | 50/500 [00:11<01:35,  4.72it/s]2025-04-27 18:45:26,437 - INFO - Epoch 50, Train Loss: 0.0008, Train MAE: 0.0210, Train R2: 0.5359, Val Loss: 0.0010, Val MAE: 0.0240, Val R2: 0.2209\n",
      "Training epochs:  12%|█▏        | 60/500 [00:13<01:33,  4.70it/s]2025-04-27 18:45:28,559 - INFO - Epoch 60, Train Loss: 0.0008, Train MAE: 0.0203, Train R2: 0.5546, Val Loss: 0.0008, Val MAE: 0.0221, Val R2: 0.3278\n",
      "Training epochs:  14%|█▍        | 70/500 [00:15<01:31,  4.70it/s]2025-04-27 18:45:30,708 - INFO - Epoch 70, Train Loss: 0.0007, Train MAE: 0.0190, Train R2: 0.6088, Val Loss: 0.0010, Val MAE: 0.0236, Val R2: 0.2287\n",
      "Training epochs:  16%|█▌        | 80/500 [00:17<01:28,  4.72it/s]2025-04-27 18:45:32,832 - INFO - Epoch 80, Train Loss: 0.0006, Train MAE: 0.0182, Train R2: 0.6225, Val Loss: 0.0010, Val MAE: 0.0233, Val R2: 0.2306\n",
      "Training epochs:  18%|█▊        | 90/500 [00:19<01:29,  4.60it/s]2025-04-27 18:45:35,003 - INFO - Epoch 90, Train Loss: 0.0006, Train MAE: 0.0181, Train R2: 0.6448, Val Loss: 0.0010, Val MAE: 0.0244, Val R2: 0.1652\n",
      "2025-04-27 18:45:35,004 - INFO - Early stopping at epoch 90\n",
      "Training epochs:  18%|█▊        | 90/500 [00:20<01:31,  4.47it/s]\n",
      "2025-04-27 18:45:35,012 - INFO - Finished training GNN model.\n",
      "2025-04-27 18:45:35,013 - INFO - Predicting walkability scores using GNN model...\n",
      "2025-04-27 18:45:35,014 - INFO - Preparing data for GNN training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting predict_walkability...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing features by node type:   0%|          | 0/3 [00:00<?, ?it/s]2025-04-27 18:45:35,106 - INFO - Node type neighborhood: 456 nodes, feature shape: (456, 127)\n",
      "2025-04-27 18:45:35,403 - INFO - Node type building: 74306 nodes, feature shape: (74306, 127)\n",
      "Normalizing features by node type:  67%|██████▋   | 2/3 [00:00<00:00,  5.26it/s]2025-04-27 18:45:35,580 - INFO - Node type road: 81444 nodes, feature shape: (81444, 127)\n",
      "Normalizing features by node type: 100%|██████████| 3/3 [00:00<00:00,  5.33it/s]\n",
      "2025-04-27 18:45:35,665 - INFO - Edge index created with 270909 edges\n",
      "2025-04-27 18:45:35,668 - INFO - Prepared GNN data: 156206 nodes, 270909 edges\n",
      "2025-04-27 18:45:35,669 - INFO - Feature matrix shape: torch.Size([156206, 127])\n",
      "2025-04-27 18:45:35,669 - INFO - Label tensor shape: torch.Size([156206, 1])\n",
      "2025-04-27 18:45:36,139 - INFO - GNN walkability category thresholds - low: 0.3505, high: 0.3690\n",
      "2025-04-27 18:45:36,265 - INFO - Finished predicting walkability scores.\n",
      "2025-04-27 18:45:36,269 - INFO - Generating interactive Kepler.gl map...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting create_interactive_map...\n",
      "User Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 18:45:42,863 - INFO - Interactive map generated and saved as /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/taipei_walkability_map.html\n",
      "2025-04-27 18:45:42,992 - INFO - Final validation - Walkability scores in neighborhood nodes:\n",
      "2025-04-27 18:45:42,994 - INFO - Walkability score distribution:\n",
      "count    456.000000\n",
      "mean       0.359348\n",
      "std        0.040287\n",
      "min        0.233758\n",
      "25%        0.334115\n",
      "50%        0.359866\n",
      "75%        0.383302\n",
      "max        0.593854\n",
      "Name: walkability_score, dtype: float64\n",
      "2025-04-27 18:45:42,995 - INFO - Walkability GNN distribution:\n",
      "count    456.000000\n",
      "mean       0.360371\n",
      "std        0.022180\n",
      "min        0.290742\n",
      "25%        0.346118\n",
      "50%        0.360622\n",
      "75%        0.374672\n",
      "max        0.420831\n",
      "Name: walkability_gnn, dtype: float64\n",
      "2025-04-27 18:45:42,995 - INFO - Walkability category distribution:\n",
      "walkability_category\n",
      "high      155\n",
      "low       151\n",
      "medium    150\n",
      "Name: count, dtype: int64\n",
      "2025-04-27 18:45:42,996 - INFO - Number of neighborhood nodes with non-zero walkability_score: 456/456\n",
      "2025-04-27 18:45:42,996 - INFO - Number of neighborhood nodes with non-zero walkability_gnn: 456/456\n",
      "2025-04-27 18:45:42,997 - WARNING - Walkability scores have low variation (std < 0.05). Components may need adjustment.\n",
      "2025-04-27 18:45:42,997 - WARNING - GNN predictions have low variation (std < 0.05). Check edge creation and model training.\n",
      "2025-04-27 18:45:42,999 - INFO - Correlation between walkability_score and walkability_gnn: 0.73 (p-value: 0.00)\n",
      "2025-04-27 18:45:43,000 - INFO - Processing complete. Timing summary:\n",
      "2025-04-27 18:45:43,000 - INFO - load_and_prepare_data: 33.24 seconds\n",
      "2025-04-27 18:45:43,000 - INFO - compute_road_type_accident_correlation: 10.22 seconds\n",
      "2025-04-27 18:45:43,001 - INFO - compute_pedestrian_road_density: 0.41 seconds\n",
      "2025-04-27 18:45:43,002 - INFO - build_graph: 26.70 seconds\n",
      "2025-04-27 18:45:43,002 - INFO - compute_walkability_scores: 0.29 seconds\n",
      "2025-04-27 18:45:43,003 - INFO - prepare_gnn_data: 0.63 seconds\n",
      "2025-04-27 18:45:43,003 - INFO - train_gnn_model: 20.47 seconds\n",
      "2025-04-27 18:45:43,004 - INFO - predict_walkability: 1.26 seconds\n",
      "2025-04-27 18:45:43,004 - INFO - create_interactive_map: 6.62 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/taipei_walkability_map.html!\n",
      "Map saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/taipei_walkability_map.html!\n",
      "Pipeline completed successfully.\n",
      "   src  dst\n",
      "0    0    1\n",
      "1    0    2\n",
      "2    0    3\n",
      "3    0    4\n",
      "4    0    5\n"
     ]
    }
   ],
   "source": [
    "def main(force_recompute_graph=False):\n",
    "      \"\"\"Main execution pipeline for the analysis.\"\"\"\n",
    "      logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "      os.makedirs(SUBGRAPH_DIR, exist_ok=True)\n",
    "      logging.info(f\"Ensured subgraph directory exists: {SUBGRAPH_DIR}\")\n",
    "\n",
    "      # Track timing for each step\n",
    "      timings = {}\n",
    "      \n",
    "      try:\n",
    "          # Step 1: Load and prepare data\n",
    "          start_time = time.time()\n",
    "          print(\"Starting load_and_prepare_data...\")\n",
    "          data = load_and_prepare_data()\n",
    "          timings['load_and_prepare_data'] = time.time() - start_time\n",
    "\n",
    "          # Step 2: Compute road type accident correlation\n",
    "          start_time = time.time()\n",
    "          print(\"Starting compute_road_type_accident_correlation...\")\n",
    "          road_accident_summary = compute_road_type_accident_correlation(\n",
    "              data['roads'], data['neighborhoods'], data['accidents']\n",
    "          )\n",
    "          timings['compute_road_type_accident_correlation'] = time.time() - start_time\n",
    "\n",
    "          # Step 2.5: Compute pedestrian road density\n",
    "          start_time = time.time()\n",
    "          print(\"Starting compute_pedestrian_road_density...\")\n",
    "          data['neighborhoods'] = compute_pedestrian_road_density(data['roads'], data['neighborhoods'])\n",
    "          timings['compute_pedestrian_road_density'] = time.time() - start_time\n",
    "\n",
    "          # Step 3: Build graph\n",
    "          start_time = time.time()\n",
    "          print(\"Starting build_graph...\")\n",
    "          G = build_graph(data, force_recompute=force_recompute_graph)\n",
    "          timings['build_graph'] = time.time() - start_time\n",
    "\n",
    "          # Validate edge counts\n",
    "          edge_count = G.edgelist.edgelist_df.shape[0] if G.edgelist else 0\n",
    "          logging.info(f\"Graph edge count: {edge_count}\")\n",
    "          if edge_count == 0:\n",
    "              logging.warning(\"Graph has no edges. GNN will not utilize graph structure.\")\n",
    "\n",
    "          # Step 4: Compute walkability scores\n",
    "          start_time = time.time()\n",
    "          print(\"Starting compute_walkability_scores...\")\n",
    "          G = compute_walkability_scores(G, data)\n",
    "          timings['compute_walkability_scores'] = time.time() - start_time\n",
    "\n",
    "          # Step 5: Prepare GNN data\n",
    "          start_time = time.time()\n",
    "          print(\"Starting prepare_gnn_data...\")\n",
    "          data_gnn = prepare_gnn_data(G)\n",
    "          timings['prepare_gnn_data'] = time.time() - start_time\n",
    "\n",
    "          # Step 6: Train GNN model\n",
    "          start_time = time.time()\n",
    "          print(\"Starting train_gnn_model...\")\n",
    "          results = train_gnn_model(data_gnn)\n",
    "          model = results['model']\n",
    "          timings['train_gnn_model'] = time.time() - start_time\n",
    "\n",
    "          # Step 7: Predict walkability\n",
    "          start_time = time.time()\n",
    "          print(\"Starting predict_walkability...\")\n",
    "          G = predict_walkability(G, model)\n",
    "          timings['predict_walkability'] = time.time() - start_time\n",
    "\n",
    "          # Step 8: Create interactive map\n",
    "          start_time = time.time()\n",
    "          print(\"Starting create_interactive_map...\")\n",
    "          create_interactive_map(G, data)\n",
    "          timings['create_interactive_map'] = time.time() - start_time\n",
    "\n",
    "          # Final validation: Check walkability scores\n",
    "          nodes_df = G._nodes.to_pandas()\n",
    "          neighborhood_nodes = nodes_df[nodes_df['type'] == 'neighborhood']\n",
    "          walkability_score_stats = neighborhood_nodes['walkability_score'].describe()\n",
    "          walkability_gnn_stats = neighborhood_nodes['walkability_gnn'].describe()\n",
    "          walkability_category_dist = neighborhood_nodes['walkability_category'].value_counts()\n",
    "          non_zero_walkability = (neighborhood_nodes['walkability_score'] > 0).sum()\n",
    "          non_zero_walkability_gnn = (neighborhood_nodes['walkability_gnn'] > 0).sum()\n",
    "          \n",
    "          logging.info(\"Final validation - Walkability scores in neighborhood nodes:\")\n",
    "          logging.info(f\"Walkability score distribution:\\n{walkability_score_stats}\")\n",
    "          logging.info(f\"Walkability GNN distribution:\\n{walkability_gnn_stats}\")\n",
    "          logging.info(f\"Walkability category distribution:\\n{walkability_category_dist}\")\n",
    "          logging.info(f\"Number of neighborhood nodes with non-zero walkability_score: {non_zero_walkability}/{len(neighborhood_nodes)}\")\n",
    "          logging.info(f\"Number of neighborhood nodes with non-zero walkability_gnn: {non_zero_walkability_gnn}/{len(neighborhood_nodes)}\")\n",
    "\n",
    "          # Check for low variation in walkability scores\n",
    "          if walkability_score_stats['std'] < 0.05:\n",
    "              logging.warning(\"Walkability scores have low variation (std < 0.05). Components may need adjustment.\")\n",
    "          if walkability_gnn_stats['std'] < 0.05:\n",
    "              logging.warning(\"GNN predictions have low variation (std < 0.05). Check edge creation and model training.\")\n",
    "\n",
    "          # Compute correlation between walkability_score and walkability_gnn\n",
    "          corr, p_value = pearsonr(neighborhood_nodes['walkability_score'], neighborhood_nodes['walkability_gnn'])\n",
    "          logging.info(f\"Correlation between walkability_score and walkability_gnn: {corr:.2f} (p-value: {p_value:.2f})\")\n",
    "          if corr < 0.5:\n",
    "              logging.warning(\"Low correlation between walkability_score and walkability_gnn. GNN predictions may not align well with rule-based scores.\")\n",
    "\n",
    "          # Log timing summary\n",
    "          logging.info(\"Processing complete. Timing summary:\")\n",
    "          for step, duration in timings.items():\n",
    "              logging.info(f\"{step}: {duration:.2f} seconds\")\n",
    "          \n",
    "          print(\"Pipeline completed successfully.\")\n",
    "          print(G.edgelist.edgelist_df.to_pandas().head())\n",
    "\n",
    "          return results\n",
    "\n",
    "      except Exception as e:\n",
    "          logging.error(f\"Pipeline failed with error: {str(e)}\")\n",
    "          raise\n",
    "\n",
    "def plot_training_history(results):\n",
    "    import matplotlib.pyplot as plt\n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(results['train_losses'], label='Train Loss')\n",
    "    plt.plot(results['val_losses'], label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.savefig('training_validation_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot MAEs\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(results['train_maes'], label='Train MAE')\n",
    "    plt.plot(results['val_maes'], label='Val MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation MAE')\n",
    "    plt.savefig('training_validation_mae.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot R2 scores\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(results['train_r2s'], label='Train R2')\n",
    "    plt.plot(results['val_r2s'], label='Val R2')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('R2 Score')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation R2 Score')\n",
    "    plt.savefig('training_validation_r2.png')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main(force_recompute_graph=True)\n",
    "    plot_training_history(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
