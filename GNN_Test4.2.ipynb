{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a986f754",
   "metadata": {},
   "source": [
    "Cell 0: CUDA Version check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc2c30cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.02.02 25.02.00\n"
     ]
    }
   ],
   "source": [
    "import cudf, cugraph\n",
    "print(cudf.__version__, cugraph.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac3ad7",
   "metadata": {},
   "source": [
    "Cell 1: Imports ,Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e388b0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNormalise Information to Standard \\nRead and Group to standard\\nMerge to standard\\ngraph\\nkepler merge\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import logging\n",
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Third-party imports\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import cugraph\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv, BatchNorm\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from shapely import make_valid\n",
    "from shapely.errors import GEOSException\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from keplergl import KeplerGl\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from torch_geometric.nn import GATConv, BatchNorm\n",
    "import torch.nn.functional as F\n",
    "import contextily as ctx\n",
    "# Plotting configuration\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Noto Sans CJK TC', 'Noto Serif CJK TC', 'Noto Sans Mono CJK TC', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Directory and file paths\n",
    "BASE_DIR = \"/home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data\"\n",
    "LANDUSE_NDVI_PATH = os.path.join(BASE_DIR, \"neighborhoods_with_ndvi_numerical_corrected.geojson\")\n",
    "OSM_BUILDINGS_PATH = os.path.join(BASE_DIR, \"Taipei_Buildings_fulldata.geojson\")\n",
    "OSM_ROADS_PATH = os.path.join(BASE_DIR, \"taipei_segments_cleaned_verified.geoparquet\")\n",
    "OSM_TREES_PATH = os.path.join(BASE_DIR, \"taipei_land.geoparquet\")\n",
    "OSM_TRANSIT_PATH = os.path.join(BASE_DIR, \"taipei_infrastructure.geoparquet\")\n",
    "URBAN_MASTERPLAN_PATH = os.path.join(BASE_DIR, \"Taipei_urban_masterplan.geojson\")\n",
    "ACCIDENTS_PATH = os.path.join(BASE_DIR, \"2023_accidents.geojson\")\n",
    "POPULATION_PATH = os.path.join(BASE_DIR, \"population_corrected.json\")\n",
    "SUBGRAPH_DIR = os.path.join(BASE_DIR, \"subgraphs\")\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"checkpoints\")\n",
    "INTERSECTION_CACHE_PATH = os.path.join(BASE_DIR, \"neighborhoods_with_intersections.geoparquet\")\n",
    "GRAPH_NODES_CACHE_PATH = os.path.join(BASE_DIR, \"graph_nodes.parquet\")\n",
    "GRAPH_EDGES_CACHE_PATH = os.path.join(BASE_DIR, \"graph_edges.parquet\")\n",
    "GRAPH_NODE_ID_CACHE_PATH = os.path.join(BASE_DIR, \"graph_node_id_to_index.json\")\n",
    "GRAPH_DATA_HASH_PATH = os.path.join(BASE_DIR, \"graph_data_hash.txt\")\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(SUBGRAPH_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Constants for spatial analysis\n",
    "BUFFER_DISTANCE = 10  # Meters, buffer distance for border sharing of accidents (tunable based on spatial resolution)\n",
    "MIN_ROAD_LENGTH = 10  # Meters, minimum road length to avoid inflated accident density (tunable based on dataset)\n",
    "\n",
    "# Land use category priorities for area assignment\n",
    "CATEGORY_PRIORITY = {\n",
    "    'City_Open_Area': 10,\n",
    "    'Pedestrian': 9,\n",
    "    'Public_Transportation': 8,\n",
    "    'Amenity': 7,\n",
    "    'Education': 6,\n",
    "    'Medical': 5,\n",
    "    'Commercial': 4,\n",
    "    'Residential': 3,\n",
    "    'Natural': 2,\n",
    "    'Road': 1,\n",
    "    'River': 1,\n",
    "    'Infrastructure': 1,\n",
    "    'Government': 1,\n",
    "    'Special_Zone': 1,\n",
    "    'Military': 1,\n",
    "    'Industrial': 1,\n",
    "    'Agriculture': 1\n",
    "}\n",
    "\n",
    "# Weights for land use diversity in walkability scoring\n",
    "land_use_weights = {\n",
    "    'city_open_area': 0.8,\n",
    "    'commercial': 0.7,\n",
    "    'infrastructure': 0.4,\n",
    "    'government': 0.5,\n",
    "    'public_transportation': 0.8,\n",
    "    'education': 0.7,\n",
    "    'medical': 0.6,\n",
    "    'amenity': 0.8,\n",
    "    'road': 0.3,\n",
    "    'pedestrian': 1.0,\n",
    "    'natural': 0.7,\n",
    "    'special_zone': 0.4,\n",
    "    'river': 0.7,\n",
    "    'military': 0.2,\n",
    "    'residential': 0.6,\n",
    "    'industrial': 0.3,\n",
    "    'agriculture': 0.4\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Normalise Information to Standard \n",
    "Read and Group to standard\n",
    "Merge to standard\n",
    "graph\n",
    "kepler merge\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4722f69a",
   "metadata": {},
   "source": [
    "Cell 2: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "713654c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import pearsonr\n",
    "import geopandas as gpd\n",
    "from shapely import make_valid\n",
    "\n",
    "def compute_land_use_percentages(neighborhoods_gdf, urban_masterplan, land_use_categories, category_priority):\n",
    "    \"\"\"Compute land use percentages for each neighborhood based on urban_masterplan intersections.\"\"\"\n",
    "    logging.info(\"Computing land use percentages for neighborhoods...\")\n",
    "    neighborhoods_gdf = neighborhoods_gdf.copy()\n",
    "    \n",
    "    # Validate urban_masterplan\n",
    "    if urban_masterplan.empty:\n",
    "        logging.error(\"urban_masterplan is empty. Using default percentages.\")\n",
    "        for cat in land_use_categories:\n",
    "            neighborhoods_gdf[f'land_use_{cat}_percent'] = 100.0 / len(land_use_categories)\n",
    "        return neighborhoods_gdf\n",
    "    \n",
    "    if 'Category' not in urban_masterplan.columns:\n",
    "        logging.error(\"urban_masterplan missing 'Category' column. Using default percentages.\")\n",
    "        for cat in land_use_categories:\n",
    "            neighborhoods_gdf[f'land_use_{cat}_percent'] = 100.0 / len(land_use_categories)\n",
    "        return neighborhoods_gdf\n",
    "    \n",
    "    # Ensure CRS alignment\n",
    "    if neighborhoods_gdf.crs != urban_masterplan.crs:\n",
    "        logging.info(f\"Aligning urban_masterplan CRS from {urban_masterplan.crs} to {neighborhoods_gdf.crs}\")\n",
    "        urban_masterplan = urban_masterplan.to_crs(neighborhoods_gdf.crs)\n",
    "    \n",
    "    # Fix geometries\n",
    "    neighborhoods_gdf['geometry'] = neighborhoods_gdf['geometry'].apply(make_valid)\n",
    "    urban_masterplan = urban_masterplan.copy()\n",
    "    urban_masterplan['geometry'] = urban_masterplan['geometry'].apply(make_valid)\n",
    "    \n",
    "    # Initialize columns\n",
    "    for cat in land_use_categories:\n",
    "        neighborhoods_gdf[f'land_use_{cat}_percent'] = 0.0\n",
    "    \n",
    "    for idx, row in neighborhoods_gdf.iterrows():\n",
    "        try:\n",
    "            neighborhood_geom = row['geometry']\n",
    "            if not neighborhood_geom.is_valid:\n",
    "                logging.warning(f\"Invalid geometry for neighborhood {row['LIE_NAME']}. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Find intersecting masterplan polygons\n",
    "            relevant_masterplan = urban_masterplan[urban_masterplan.intersects(neighborhood_geom)]\n",
    "            if relevant_masterplan.empty:\n",
    "                logging.debug(f\"No masterplan polygons intersect with neighborhood {row['LIE_NAME']}.\")\n",
    "                continue\n",
    "            \n",
    "            # Perform overlay to get intersection areas\n",
    "            temp_gdf = gpd.GeoDataFrame({'geometry': [neighborhood_geom]}, crs=neighborhoods_gdf.crs)\n",
    "            intersected = gpd.overlay(temp_gdf, relevant_masterplan, how='intersection', keep_geom_type=False)\n",
    "            if intersected.empty:\n",
    "                logging.debug(f\"No valid intersections for neighborhood {row['LIE_NAME']} after overlay.\")\n",
    "                continue\n",
    "            \n",
    "            intersected['geometry'] = intersected['geometry'].apply(make_valid)\n",
    "            intersected = intersected[intersected.geometry.is_valid & ~intersected.geometry.is_empty]\n",
    "            if intersected.empty:\n",
    "                logging.debug(f\"No valid geometries after fixing for neighborhood {row['LIE_NAME']}.\")\n",
    "                continue\n",
    "            \n",
    "            # Assign priorities and sort\n",
    "            intersected['priority'] = intersected['Category'].map(category_priority).fillna(0)\n",
    "            intersected = intersected.sort_values(by='priority', ascending=False)\n",
    "            \n",
    "            total_area = intersected.geometry.union_all().area\n",
    "            if total_area == 0:\n",
    "                logging.debug(f\"Zero total area for neighborhood {row['LIE_NAME']}.\")\n",
    "                continue\n",
    "            \n",
    "            # Compute areas for each category\n",
    "            remaining_geom = intersected.geometry.union_all()\n",
    "            category_areas = {}\n",
    "            for category in intersected['Category'].unique():\n",
    "                category_rows = intersected[intersected['Category'] == category]\n",
    "                category_geom = category_rows.geometry.union_all()\n",
    "                try:\n",
    "                    category_area_geom = category_geom.intersection(remaining_geom)\n",
    "                    category_areas[category] = category_area_geom.area\n",
    "                    remaining_geom = remaining_geom.difference(category_area_geom)\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error computing area for category {category} in {row['LIE_NAME']}: {e}\")\n",
    "                    category_areas[category] = 0.0\n",
    "            \n",
    "            # Assign percentages\n",
    "            for category, area in category_areas.items():\n",
    "                cat_key = category.lower().replace(' ', '_')\n",
    "                if cat_key in land_use_categories:\n",
    "                    percentage = (area / total_area * 100) if total_area > 0 else 0.0\n",
    "                    neighborhoods_gdf.at[idx, f'land_use_{cat_key}_percent'] = percentage\n",
    "                    logging.debug(f\"Neighborhood {row['LIE_NAME']}: {cat_key} = {percentage:.2f}%\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing neighborhood {row['LIE_NAME']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Check if any percentages were computed\n",
    "    if neighborhoods_gdf[land_use_cols].sum().sum() == 0:\n",
    "        logging.warning(\"No land use percentages computed. Assigning default equal distribution.\")\n",
    "        for cat in land_use_categories:\n",
    "            neighborhoods_gdf[f'land_use_{cat}_percent'] = 100.0 / len(land_use_categories)\n",
    "    \n",
    "    return neighborhoods_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e6b19b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_walkability_components_all(neighborhoods_gdf, data):\n",
    "    # Step 1: Validate Land Use Percentages\n",
    "    land_use_categories = list(data.get('land_use_weights', {}).keys())\n",
    "    land_use_cols = [f'land_use_{cat}_percent' for cat in land_use_categories]\n",
    "    available_cols = [col for col in land_use_cols if col in neighborhoods_gdf.columns]\n",
    "    \n",
    "    logging.info(f\"Available land use columns: {available_cols}\")\n",
    "    logging.info(f\"Missing land use columns: {[col for col in land_use_cols if col not in available_cols]}\")\n",
    "    \n",
    "    # Check variation in land use columns\n",
    "    for col in available_cols:\n",
    "        logging.info(f\"{col} stats:\\n{neighborhoods_gdf[col].describe()}\")\n",
    "    \n",
    "    def compute_land_use_score(row):\n",
    "        score = 0.0\n",
    "        valid_cats = 0\n",
    "        for cat in land_use_categories:\n",
    "            col = f'land_use_{cat}_percent'\n",
    "            if col in row and not pd.isna(row[col]) and row[col] > 0:\n",
    "                p = row[col] / 100.0\n",
    "                score += data['land_use_weights'][cat] * p\n",
    "                valid_cats += 1\n",
    "        # Fallback if no valid categories or score is zero\n",
    "        if valid_cats == 0 or score == 0:\n",
    "            non_zero_cats = sum(1 for cat in land_use_categories if f'land_use_{cat}_percent' in row and not pd.isna(row[f'land_use_{cat}_percent']) and row[f'land_use_{cat}_percent'] > 0)\n",
    "            if non_zero_cats > 0:\n",
    "                score = non_zero_cats / len(land_use_categories)\n",
    "                logging.debug(f\"Row {row.name}: Using non-zero categories: {non_zero_cats}, score: {score}\")\n",
    "            else:\n",
    "                # Fallback: Use cluster-based score\n",
    "                cluster = row.get('cluster', 0)\n",
    "                score = 0.5 + (cluster * 0.1)\n",
    "                logging.debug(f\"Row {row.name}: No valid land use data. Using cluster-based score: {score}\")\n",
    "        return score\n",
    "    \n",
    "    neighborhoods_gdf['land_use_score'] = neighborhoods_gdf.apply(compute_land_use_score, axis=1)\n",
    "    logging.info(f\"land_use_score stats:\\n{neighborhoods_gdf['land_use_score'].describe()}\")\n",
    "    logging.info(f\"Sample land_use_score values (first 5 rows):\\n{neighborhoods_gdf['land_use_score'].head()}\")\n",
    "    \n",
    "    neighborhoods_gdf['population_density'] = np.log1p(neighborhoods_gdf['total_population'] / neighborhoods_gdf['area_km2'].replace(0, 1e-6))\n",
    "    logging.info(f\"population_density (log) stats:\\n{neighborhoods_gdf['population_density'].describe()}\")\n",
    "    \n",
    "    neighborhoods_gdf['transit_density'] = np.log1p(neighborhoods_gdf['transit_count'] / neighborhoods_gdf['area_km2'].replace(0, 1e-6))\n",
    "    logging.info(f\"transit_density (log) stats:\\n{neighborhoods_gdf['transit_density'].describe()}\")\n",
    "    \n",
    "    if 'ndvi' in neighborhoods_gdf.columns:\n",
    "        if 'land_use_city_open_area_percent' in neighborhoods_gdf.columns and 'land_use_river_percent' in neighborhoods_gdf.columns:\n",
    "            city_open_corr, _ = pearsonr(neighborhoods_gdf['ndvi'], neighborhoods_gdf['land_use_city_open_area_percent'].fillna(0) / 100.0)\n",
    "            river_corr, _ = pearsonr(neighborhoods_gdf['ndvi'], neighborhoods_gdf['land_use_river_percent'].fillna(0) / 100.0)\n",
    "            logging.info(f\"Correlation between NDVI and City_Open_Area: {city_open_corr:.3f}\")\n",
    "            logging.info(f\"Correlation between NDVI and River: {river_corr:.3f}\")\n",
    "        else:\n",
    "            city_open_corr, river_corr = 0.5, 0.5\n",
    "            logging.warning(\"Land use columns for City_Open_Area or River missing. Using default correlation weights.\")\n",
    "\n",
    "        city_open_contribution = neighborhoods_gdf['land_use_city_open_area_percent'].fillna(0) / 100.0\n",
    "        river_accessibility = np.minimum(neighborhoods_gdf['land_use_pedestrian_percent'].fillna(0) / 100.0, 1.0)\n",
    "        river_contribution = (neighborhoods_gdf['land_use_river_percent'].fillna(0) / 100.0) * river_accessibility\n",
    "\n",
    "        w1, w2, w3 = 0.5, city_open_corr / (city_open_corr + river_corr + 1), river_corr / (city_open_corr + river_corr + 1)\n",
    "        total_w = w1 + w2 + w3\n",
    "        w1, w2, w3 = w1/total_w, w2/total_w, w3/total_w\n",
    "\n",
    "        combined_green = (w1 * neighborhoods_gdf['ndvi'] + w2 * city_open_contribution + w3 * river_contribution)\n",
    "        neighborhoods_gdf['green_space'] = 1 / (1 + np.exp(-5 * (combined_green - 0.5)))\n",
    "        logging.info(f\"green_space stats:\\n{neighborhoods_gdf['green_space'].describe()}\")\n",
    "    else:\n",
    "        logging.warning(\"'ndvi' missing. Using tree_density.\")\n",
    "        neighborhoods_gdf['tree_density'] = neighborhoods_gdf['tree_count'] / neighborhoods_gdf['area_km2'].replace(0, 1e-6)\n",
    "        combined_green = (0.5 * np.log1p(neighborhoods_gdf['tree_density']) + \n",
    "                         0.25 * (neighborhoods_gdf['land_use_city_open_area_percent'].fillna(0) / 100.0) + \n",
    "                         0.25 * (neighborhoods_gdf['land_use_river_percent'].fillna(0) / 100.0))\n",
    "        neighborhoods_gdf['green_space'] = 1 / (1 + np.exp(-5 * (combined_green - 0.5)))\n",
    "    \n",
    "    neighborhoods_gdf['accident_density'] = np.log1p(neighborhoods_gdf['accident_count'] / neighborhoods_gdf['area_km2'].replace(0, 1e-6))\n",
    "    neighborhoods_gdf['intersection_density'] = np.log1p(neighborhoods_gdf['intersection_density'].fillna(0).clip(upper=neighborhoods_gdf['intersection_density'].quantile(0.95)))\n",
    "\n",
    "    pedestrian_roads = data['roads'][data['roads']['class'].isin(['footway', 'pedestrian', 'cycleway'])]\n",
    "    pedestrian_roads_in_neighborhoods = gpd.sjoin(pedestrian_roads, neighborhoods_gdf[['geometry', 'LIE_NAME', 'area_km2']], how='left', predicate='intersects')\n",
    "    pedestrian_length_per_neighborhood = pedestrian_roads_in_neighborhoods.groupby('LIE_NAME')['length_m'].sum()\n",
    "    neighborhoods_gdf['sidewalk_coverage'] = (neighborhoods_gdf['LIE_NAME'].map(pedestrian_length_per_neighborhood).fillna(0) / \n",
    "                                             neighborhoods_gdf['area_km2'].replace(0, 1e-6))\n",
    "    neighborhoods_gdf['sidewalk_coverage'] = np.log1p(neighborhoods_gdf['sidewalk_coverage'])\n",
    "\n",
    "    amenities_gdf = data['urban_masterplan'][data['urban_masterplan']['Category'].isin(['Commercial', 'Amenity', 'Education', 'Medical'])]\n",
    "    neighborhoods_gdf_buffered = neighborhoods_gdf.copy()\n",
    "    neighborhoods_gdf_buffered['geometry'] = neighborhoods_gdf_buffered['geometry'].buffer(500)\n",
    "    amenities_in_neighborhoods = gpd.sjoin(amenities_gdf, neighborhoods_gdf_buffered[['geometry', 'LIE_NAME', 'area_km2']], how='left', predicate='intersects')\n",
    "    amenities_count = amenities_in_neighborhoods.groupby('LIE_NAME').size().reset_index(name='amenities_count')\n",
    "    neighborhoods_gdf = neighborhoods_gdf.merge(amenities_count, on='LIE_NAME', how='left')\n",
    "    neighborhoods_gdf['amenities_count'] = neighborhoods_gdf['amenities_count'].fillna(0)\n",
    "    neighborhoods_gdf['amenities_density'] = np.log1p(neighborhoods_gdf['amenities_count'] / neighborhoods_gdf['area_km2'].replace(0, 1e-6))\n",
    "\n",
    "    # Step 3: Define Components\n",
    "    base_components = {\n",
    "        'land_use_score': 'higher_better',\n",
    "        'intersection_density': 'higher_better',\n",
    "        'population_density': 'higher_better',\n",
    "        'transit_density': 'higher_better',\n",
    "        'green_space': 'higher_better',\n",
    "        'accident_density': 'lower_better',\n",
    "        'sidewalk_coverage': 'higher_better',\n",
    "        'amenities_density': 'higher_better'\n",
    "    }\n",
    "\n",
    "    # Step 4: Cluster Neighborhoods\n",
    "    clustering_features = ['population_density', 'transit_density', 'green_space']\n",
    "    clustering_data = neighborhoods_gdf[clustering_features].fillna(0)\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "    neighborhoods_gdf['cluster'] = kmeans.fit_predict(clustering_data)\n",
    "    logging.info(f\"Neighborhood clusters:\\n{neighborhoods_gdf['cluster'].value_counts()}\")\n",
    "\n",
    "    # Step 5: Dynamic Weighting with Increased Variation\n",
    "    base_weights = {\n",
    "        'land_use_score': 0.20, 'intersection_density': 0.05, 'population_density': 0.20,\n",
    "        'transit_density': 0.20, 'green_space': 0.15, 'accident_density': 0.10,\n",
    "        'sidewalk_coverage': 0.05, 'amenities_density': 0.05,\n",
    "        'pop_transit_interaction': 0.05, 'safety_green_interaction': 0.05\n",
    "    }\n",
    "    \n",
    "    def adjust_weights(cluster):\n",
    "        weights = base_weights.copy()\n",
    "        if cluster == 0:  # Urban\n",
    "            weights['transit_density'] += 0.20\n",
    "            weights['amenities_density'] += 0.20\n",
    "            weights['green_space'] -= 0.20\n",
    "        elif cluster == 1:  # Suburban\n",
    "            weights['sidewalk_coverage'] += 0.20\n",
    "            weights['population_density'] -= 0.20\n",
    "        else:  # Rural\n",
    "            weights['green_space'] += 0.20\n",
    "            weights['accident_density'] += 0.20\n",
    "            weights['transit_density'] -= 0.20\n",
    "        total = sum(weights.values())\n",
    "        return {k: v/total for k, v in weights.items()}\n",
    "\n",
    "    # Step 6: Normalize Components\n",
    "    means = {}\n",
    "    stds = {}\n",
    "    for comp in base_components:\n",
    "        means[comp] = neighborhoods_gdf[comp].mean()\n",
    "        stds[comp] = neighborhoods_gdf[comp].std() if neighborhoods_gdf[comp].std() != 0 else 1.0\n",
    "        if stds[comp] == 0:\n",
    "            logging.warning(f\"Component {comp} has no variation. Adding small random noise.\")\n",
    "            neighborhoods_gdf[f'{comp}_norm'] = np.random.normal(0, 0.01, len(neighborhoods_gdf))\n",
    "        else:\n",
    "            neighborhoods_gdf[f'{comp}_norm'] = (neighborhoods_gdf[comp] - means[comp]) / stds[comp]\n",
    "        logging.info(f\"{comp}_norm stats:\\n{neighborhoods_gdf[f'{comp}_norm'].describe()}\")\n",
    "\n",
    "    # Step 7: Interaction Terms\n",
    "    neighborhoods_gdf['pop_transit_interaction'] = neighborhoods_gdf['population_density_norm'] * neighborhoods_gdf['transit_density_norm']\n",
    "    neighborhoods_gdf['safety_green_interaction'] = neighborhoods_gdf['accident_density_norm'] * neighborhoods_gdf['green_space_norm']\n",
    "    interaction_components = {\n",
    "        'pop_transit_interaction': 'higher_better',\n",
    "        'safety_green_interaction': 'lower_better'\n",
    "    }\n",
    "    for comp in interaction_components:\n",
    "        means[comp] = neighborhoods_gdf[comp].mean()\n",
    "        stds[comp] = neighborhoods_gdf[comp].std() if neighborhoods_gdf[comp].std() != 0 else 1.0\n",
    "        if stds[comp] == 0:\n",
    "            logging.warning(f\"Interaction component {comp} has no variation. Adding small random noise.\")\n",
    "            neighborhoods_gdf[f'{comp}_norm'] = np.random.normal(0, 0.01, len(neighborhoods_gdf))\n",
    "        else:\n",
    "            neighborhoods_gdf[f'{comp}_norm'] = (neighborhoods_gdf[comp] - means[comp]) / stds[comp]\n",
    "        logging.info(f\"{comp}_norm stats:\\n{neighborhoods_gdf[f'{comp}_norm'].describe()}\")\n",
    "\n",
    "    # Step 8: Combine Components\n",
    "    components = {**base_components, **interaction_components}\n",
    "\n",
    "    # Step 9: Compute Walkability Score with Debugging\n",
    "    scores = []\n",
    "    for idx, row in neighborhoods_gdf.iterrows():\n",
    "        cluster = row['cluster']\n",
    "        weights = adjust_weights(cluster)\n",
    "        score = 0.0\n",
    "        for comp in components:\n",
    "            norm_col = f'{comp}_norm'\n",
    "            if norm_col in neighborhoods_gdf.columns:\n",
    "                score += weights.get(comp, 0.05) * row[norm_col]\n",
    "            else:\n",
    "                logging.error(f\"Column {norm_col} missing.\")\n",
    "        scores.append(score)\n",
    "        if idx < 5:  # Log first 5 rows\n",
    "            logging.info(f\"Row {idx}: cluster={cluster}, weights={weights}, score={score}\")\n",
    "    \n",
    "    neighborhoods_gdf['walkability_score'] = scores\n",
    "    logging.info(f\"Walkability scores before scaling:\\n{neighborhoods_gdf['walkability_score'].describe()}\")\n",
    "\n",
    "    # Step 10: Scale to [0, 1]\n",
    "    min_score = neighborhoods_gdf['walkability_score'].min()\n",
    "    max_score = neighborhoods_gdf['walkability_score'].max()\n",
    "    logging.info(f\"Min score: {min_score}, Max score: {max_score}\")\n",
    "    if max_score != min_score:\n",
    "        neighborhoods_gdf['walkability_score'] = (neighborhoods_gdf['walkability_score'] - min_score) / (max_score - min_score)\n",
    "    else:\n",
    "        logging.warning(\"All scores identical. Setting to 0.5.\")\n",
    "        neighborhoods_gdf['walkability_score'] = 0.5\n",
    "    neighborhoods_gdf['walkability_score'] = neighborhoods_gdf['walkability_score'].clip(0, 1)\n",
    "    logging.info(f\"Final walkability_score stats:\\n{neighborhoods_gdf['walkability_score'].describe()}\")\n",
    "\n",
    "    # Step 11: Categorize\n",
    "    low_threshold = neighborhoods_gdf['walkability_score'].quantile(0.33)\n",
    "    high_threshold = neighborhoods_gdf['walkability_score'].quantile(0.66)\n",
    "    neighborhoods_gdf['walkability_category'] = neighborhoods_gdf['walkability_score'].apply(\n",
    "        lambda x: 'low' if x < low_threshold else 'medium' if x < high_threshold else 'high'\n",
    "    )\n",
    "    logging.info(f\"Walkability category distribution:\\n{neighborhoods_gdf['walkability_category'].value_counts()}\")\n",
    "\n",
    "    return_cols = ['LIE_NAME', 'walkability_score', 'walkability_category', 'cluster', 'green_space'] + [f'{comp}_norm' for comp in components]\n",
    "    return neighborhoods_gdf[return_cols], components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b05a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_geometry(geom, buffer_size=1e-5):\n",
    "    \"\"\"Fix invalid geometries with logging for debugging.\"\"\"\n",
    "    if geom is None or geom.is_empty:\n",
    "        logging.debug(\"Geometry is None or empty, returning a default Point(0,0).\")\n",
    "        return Point(0, 0)\n",
    "    try:\n",
    "        geom = make_valid(geom)\n",
    "        if not geom.is_valid:\n",
    "            logging.debug(f\"Geometry invalid after make_valid, applying buffer(0): {geom.bounds}\")\n",
    "            geom = geom.buffer(0)\n",
    "            if not geom.is_valid:\n",
    "                logging.debug(f\"Geometry still invalid, applying buffer with size {buffer_size}: {geom.bounds}\")\n",
    "                geom = geom.buffer(buffer_size)\n",
    "                geom = make_valid(geom)\n",
    "        if not geom.is_valid:\n",
    "            logging.warning(f\"Geometry remains invalid after all attempts: {geom.bounds}. Returning default Point(0,0).\")\n",
    "            return Point(0, 0)\n",
    "        return geom\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fixing geometry: {e}. Returning default Point(0,0).\")\n",
    "        return Point(0, 0)\n",
    "\n",
    "def print_data_structure(data_dict):\n",
    "    \"\"\"Print a detailed summary of the data structure for each dataset.\"\"\"\n",
    "    print(\"\\n--- Data Structure Summary ---\")\n",
    "    for key, df in data_dict.items():\n",
    "        if isinstance(df, (gpd.GeoDataFrame, pd.DataFrame, cudf.DataFrame)):\n",
    "            df = df.to_pandas() if isinstance(df, cudf.DataFrame) else df\n",
    "            print(f\"\\nDataset: {key}\")\n",
    "            print(f\"Shape: {df.shape}\")\n",
    "            print(f\"Columns and Data Types:\\n{df.dtypes}\")\n",
    "            print(f\"Missing values (total): {df.isnull().sum().sum()}\")\n",
    "            print(f\"Missing values per column:\\n{df.isnull().sum()}\")\n",
    "            if 'LIE_NAME' in df.columns:\n",
    "                print(f\"Unique LIE_NAME: {df['LIE_NAME'].nunique()}\")\n",
    "            if 'class' in df.columns and key == 'roads':\n",
    "                print(f\"Road class counts:\\n{df['class'].value_counts()}\")\n",
    "            print(f\"Sample data (first 2 rows):\\n{df.head(2)}\")\n",
    "    print(\"--- End of Data Structure Summary ---\\n\")\n",
    "\n",
    "def print_percentage_calculation(neighborhoods_gdf, urban_masterplan_gdf, sample_size=3):\n",
    "    \"\"\"Print the land use percentage calculation process for a sample of neighborhoods.\"\"\"\n",
    "    print(\"\\n--- Percentage Calculation Process ---\")\n",
    "    sample_neighborhoods = neighborhoods_gdf.sample(min(sample_size, len(neighborhoods_gdf)), random_state=42)\n",
    "    \n",
    "    for idx, row in sample_neighborhoods.iterrows():\n",
    "        lie_name = row['LIE_NAME']\n",
    "        print(f\"\\nNeighborhood: {lie_name} (Index: {idx})\")\n",
    "        \n",
    "        neighborhood_geom = fix_geometry(row['geometry'])\n",
    "        if not neighborhood_geom.is_valid:\n",
    "            print(f\"Neighborhood geometry is invalid after fixing: {lie_name}\")\n",
    "            continue\n",
    "        \n",
    "        relevant_masterplan = urban_masterplan_gdf[urban_masterplan_gdf.intersects(neighborhood_geom)]\n",
    "        if relevant_masterplan.empty:\n",
    "            print(\"No master plan polygons intersect with this neighborhood.\")\n",
    "            continue\n",
    "        \n",
    "        temp_gdf = gpd.GeoDataFrame({'geometry': [neighborhood_geom]}, crs='EPSG:3826')\n",
    "        intersected = gpd.overlay(temp_gdf, relevant_masterplan, how='intersection', keep_geom_type=False)\n",
    "        if intersected.empty:\n",
    "            print(\"No valid intersections after overlay.\")\n",
    "            continue\n",
    "        \n",
    "        intersected['geometry'] = intersected['geometry'].apply(fix_geometry)\n",
    "        intersected = intersected[intersected.geometry.is_valid & ~intersected.geometry.is_empty]\n",
    "        if intersected.empty:\n",
    "            print(\"No valid geometries after fixing intersected polygons.\")\n",
    "            continue\n",
    "        \n",
    "        intersected['priority'] = intersected['Category'].map(CATEGORY_PRIORITY)\n",
    "        intersected = intersected.sort_values(by='priority', ascending=False)\n",
    "        \n",
    "        total_area_geom = intersected.geometry.union_all()\n",
    "        total_area = total_area_geom.area\n",
    "        print(f\"Total unique master plan area: {total_area:.2f} m²\")\n",
    "        \n",
    "        remaining_geom = total_area_geom\n",
    "        category_areas = {}\n",
    "        for category in intersected['Category'].unique():\n",
    "            category_rows = intersected[intersected['Category'] == category]\n",
    "            category_geom = category_rows.geometry.union_all()\n",
    "            try:\n",
    "                category_area_geom = category_geom.intersection(remaining_geom)\n",
    "                category_area = category_area_geom.area\n",
    "                category_areas[category] = category_area\n",
    "                print(f\"Area of {category} (priority {CATEGORY_PRIORITY.get(category, 0)}): {category_area:.2f} m²\")\n",
    "                remaining_geom = remaining_geom.difference(category_area_geom)\n",
    "            except GEOSException as e:\n",
    "                print(f\"Topology error for category {category}: {e}\")\n",
    "                category_areas[category] = 0.0\n",
    "        \n",
    "        print(\"\\nPercentages:\")\n",
    "        total_percentage = 0.0\n",
    "        for category, area in category_areas.items():\n",
    "            percentage = (area / total_area * 100) if total_area > 0 else 0.0\n",
    "            total_percentage += percentage\n",
    "            print(f\"{category}: {percentage:.2f}%\")\n",
    "        print(f\"Sum of percentages: {total_percentage:.2f}%\")\n",
    "    print(\"--- End of Percentage Calculation Process ---\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96fd5338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_data_hash(data_dict):\n",
    "    \"\"\"Compute a hash of the data for caching purposes.\"\"\"\n",
    "    hasher = hashlib.sha256()\n",
    "    for key, df in data_dict.items():\n",
    "        if isinstance(df, (gpd.GeoDataFrame, pd.DataFrame, cudf.DataFrame)):\n",
    "            df = df.to_pandas() if isinstance(df, cudf.DataFrame) else df\n",
    "            hasher.update(str(df.shape).encode('utf-8'))\n",
    "            hasher.update(str(sorted(df.columns)).encode('utf-8'))\n",
    "            \n",
    "            logging.info(f\"Dataset {key} column types:\\n{df.dtypes}\")\n",
    "            \n",
    "            sample_df = df.head(5).copy()\n",
    "            if 'geometry' in sample_df.columns:\n",
    "                sample_df = sample_df.drop(columns=['geometry'])\n",
    "            for col in sample_df.columns:\n",
    "                sample_df[col] = sample_df[col].apply(\n",
    "                    lambda x: x.tolist() if isinstance(x, np.ndarray) else\n",
    "                              float(x) if isinstance(x, (np.floating, np.integer)) else x\n",
    "                )\n",
    "            try:\n",
    "                sample = sample_df.to_json()\n",
    "                hasher.update(sample.encode('utf-8'))\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to serialize sample for dataset {key}: {e}\")\n",
    "                sample = str(sample_df.to_dict())\n",
    "                hasher.update(sample.encode('utf-8'))\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def check_spatial_overlap(gdf1, gdf2, label1=\"gdf1\", label2=\"gdf2\"):\n",
    "    \"\"\"Check for spatial overlap between two GeoDataFrames and log the results.\"\"\"\n",
    "    logging.info(f\"Checking spatial overlap between {label1} and {label2}...\")\n",
    "    gdf1 = gdf1.copy()\n",
    "    gdf2 = gdf2.copy()\n",
    "    \n",
    "    if gdf1.crs != gdf2.crs:\n",
    "        logging.warning(f\"CRS mismatch between {label1} ({gdf1.crs}) and {label2} ({gdf2.crs}). Aligning to {gdf1.crs}...\")\n",
    "        gdf2 = gdf2.to_crs(gdf1.crs)\n",
    "    \n",
    "    gdf1['geometry'] = gdf1['geometry'].apply(fix_geometry)\n",
    "    gdf2['geometry'] = gdf2['geometry'].apply(fix_geometry)\n",
    "    \n",
    "    gdf1_bounds = gdf1.total_bounds\n",
    "    gdf2_bounds = gdf2.total_bounds\n",
    "    logging.info(f\"{label1} bounds: {gdf1_bounds}\")\n",
    "    logging.info(f\"{label2} bounds: {gdf2_bounds}\")\n",
    "    \n",
    "    bounds_overlap = not (gdf1_bounds[2] < gdf2_bounds[0] or\n",
    "                         gdf1_bounds[0] > gdf2_bounds[2] or\n",
    "                         gdf1_bounds[3] < gdf2_bounds[1] or\n",
    "                         gdf1_bounds[1] > gdf2_bounds[3])\n",
    "    logging.info(f\"Bounding boxes overlap: {bounds_overlap}\")\n",
    "    \n",
    "    sample_size = min(10, len(gdf1), len(gdf2))\n",
    "    if sample_size > 0:\n",
    "        sample_gdf1 = gdf1.sample(sample_size, random_state=42)\n",
    "        intersects = gpd.sjoin(sample_gdf1, gdf2, how='inner', predicate='intersects')\n",
    "        logging.info(f\"Sample intersection check: {len(intersects)} intersections found out of {sample_size} samples.\")\n",
    "    \n",
    "    return bounds_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59ac4e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(gdf, required_cols, name=\"GeoDataFrame\"):\n",
    "    \"\"\"Validate that the GeoDataFrame has all required columns, no missing geometries, and valid geometries.\"\"\"\n",
    "    if gdf.empty:\n",
    "        logging.error(f\"{name} is empty.\")\n",
    "        raise ValueError(f\"{name} is empty.\")\n",
    "    missing_cols = [col for col in required_cols if col not in gdf.columns]\n",
    "    if missing_cols:\n",
    "        logging.error(f\"Missing columns in {name}: {missing_cols}\")\n",
    "        raise KeyError(f\"Missing columns in {name}: {missing_cols}\")\n",
    "    if gdf.geometry.isna().any():\n",
    "        logging.error(f\"Missing geometries in {name}\")\n",
    "        raise ValueError(f\"Missing geometries in {name}\")\n",
    "    if not all(gdf.geometry.is_valid):\n",
    "        logging.error(f\"Invalid geometries in {name}\")\n",
    "        raise ValueError(f\"Invalid geometries in {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ae081",
   "metadata": {},
   "source": [
    "Cell 3: Walkability Computation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3af5d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Walkability Computation Functions (Updated to Return neighborhoods_gdf)\n",
    "def compute_road_type_accident_correlation(roads_gdf, neighborhoods_gdf, accidents_gdf):\n",
    "    logging.info(\"Computing correlation between road types and accident density...\")\n",
    "    \n",
    "    # Validate input data\n",
    "    validate_data(roads_gdf, ['class', 'geometry', 'length_m'], \"roads_gdf\")\n",
    "    validate_data(neighborhoods_gdf, ['LIE_NAME', 'geometry'], \"neighborhoods_gdf\")\n",
    "    validate_data(accidents_gdf, ['geometry'], \"accidents_gdf\")\n",
    "    \n",
    "    # Ensure correct CRS\n",
    "    target_crs = 'EPSG:3826'\n",
    "    source_crs = 'EPSG:4326'\n",
    "    for gdf, name in [(roads_gdf, \"roads\"), (neighborhoods_gdf, \"neighborhoods\"), (accidents_gdf, \"accidents\")]:\n",
    "        if gdf.crs is None:\n",
    "            logging.warning(f\"{name} has no CRS defined. Assuming {source_crs}.\")\n",
    "            gdf.set_crs(source_crs, inplace=True)\n",
    "        if gdf.crs != target_crs:\n",
    "            logging.info(f\"Reprojecting {name} from {gdf.crs} to {target_crs}\")\n",
    "            gdf.to_crs(target_crs, inplace=True)\n",
    "    \n",
    "    # Log CRS, bounds, and sample geometries for debugging\n",
    "    logging.info(f\"Roads CRS: {roads_gdf.crs}, Bounds: {roads_gdf.total_bounds}\")\n",
    "    logging.info(f\"Neighborhoods CRS: {neighborhoods_gdf.crs}, Bounds: {neighborhoods_gdf.total_bounds}\")\n",
    "    logging.info(f\"Accidents CRS: {accidents_gdf.crs}, Bounds: {accidents_gdf.total_bounds}\")\n",
    "    logging.info(f\"Roads geometry types: {roads_gdf.geometry.type.unique()}\")\n",
    "    logging.info(f\"Neighborhoods geometry types: {neighborhoods_gdf.geometry.type.unique()}\")\n",
    "    sample_roads = roads_gdf.head(5)['geometry'].apply(lambda x: str(x)[:100])\n",
    "    sample_neighborhoods = neighborhoods_gdf.head(5)['geometry'].apply(lambda x: str(x)[:100])\n",
    "    logging.info(f\"Sample road geometries:\\n{sample_roads}\")\n",
    "    logging.info(f\"Sample neighborhood geometries:\\n{sample_neighborhoods}\")\n",
    "    \n",
    "    # Visualize data for debugging\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    neighborhoods_gdf.plot(ax=ax, color='blue', alpha=0.5, label='Neighborhoods')\n",
    "    roads_gdf.plot(ax=ax, color='red', alpha=0.5, label='Roads')\n",
    "    plt.legend()\n",
    "    plt.title('Roads and Neighborhoods Overlay')\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'roads_neighborhoods_overlap.png'))\n",
    "    plt.close()\n",
    "    logging.info(f\"Overlay plot saved to {os.path.join(BASE_DIR, 'roads_neighborhoods_overlap.png')}\")\n",
    "    \n",
    "    # Make local copies for roads and accidents\n",
    "    roads_gdf_local = roads_gdf.copy()\n",
    "    accidents_gdf_local = accidents_gdf.copy()\n",
    "    \n",
    "    # Add unique identifier to accidents\n",
    "    accidents_gdf_local['accident_id'] = range(len(accidents_gdf_local))\n",
    "    \n",
    "    # Define width ranking\n",
    "    width_ranking = {\n",
    "        'motorway': 5, 'trunk': 5, 'primary': 4, 'secondary': 4, 'tertiary': 3,\n",
    "        'residential': 3, 'living_street': 3, 'service': 2, 'track': 2,\n",
    "        'path': 1, 'footway': 1, 'cycleway': 1, 'steps': 1, 'pedestrian': 1,\n",
    "        'unclassified': 0, 'bridleway': 0, 'unknown': 0\n",
    "    }\n",
    "    roads_gdf_local['width_rank'] = roads_gdf_local['class'].map(width_ranking).fillna(0).astype(int)\n",
    "    \n",
    "    # Buffer wider roads for accident assignment\n",
    "    roads_gdf_buffered = roads_gdf_local.copy()\n",
    "    roads_gdf_buffered['geometry'] = roads_gdf_buffered.apply(\n",
    "        lambda row: row['geometry'].buffer(5) if row['width_rank'] >= 4 else row['geometry'], axis=1\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Assigning accidents to nearest road...\")\n",
    "    accidents_gdf_local['geometry'] = accidents_gdf_local['geometry'].apply(fix_geometry)\n",
    "    accidents_gdf_local = accidents_gdf_local[accidents_gdf_local['geometry'].is_valid & ~accidents_gdf_local['geometry'].is_empty]\n",
    "    \n",
    "    if accidents_gdf_local.empty:\n",
    "        logging.warning(\"No valid accidents after geometry fixing.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Assign accidents to nearest road\n",
    "    nearest = gpd.sjoin_nearest(\n",
    "        accidents_gdf_local,\n",
    "        roads_gdf_buffered[['geometry', 'class', 'width_rank']],\n",
    "        how='left',\n",
    "        distance_col='distance'\n",
    "    )\n",
    "    nearest['weighted_distance'] = nearest['distance'] / (nearest['width_rank'].replace(0, 1) ** 2)\n",
    "    nearest = nearest.sort_values('weighted_distance').drop_duplicates(subset=['accident_id'], keep='first')\n",
    "    \n",
    "    matched_accidents = nearest[['accident_id', 'index_right']].copy()\n",
    "    matched_accidents.columns = ['accident_id', 'road_idx']\n",
    "    matched_accidents = matched_accidents.dropna(subset=['road_idx'])\n",
    "    matched_accidents['road_idx'] = matched_accidents['road_idx'].astype(int)\n",
    "    \n",
    "    logging.info(f\"Matched {len(matched_accidents)} accidents out of {len(accidents_gdf_local)}\")\n",
    "    \n",
    "    # Reassign accidents from footway/cycleway to wider roads if possible\n",
    "    footway_cycleway_accidents = matched_accidents[\n",
    "        matched_accidents['road_idx'].isin(\n",
    "            roads_gdf_local[roads_gdf_local['class'].isin(['footway', 'cycleway'])].index\n",
    "        )\n",
    "    ]\n",
    "    if not footway_cycleway_accidents.empty:\n",
    "        logging.info(f\"Reassigning {len(footway_cycleway_accidents)} accidents from footway/cycleway...\")\n",
    "        accidents_to_reassign = accidents_gdf_local[accidents_gdf_local['accident_id'].isin(footway_cycleway_accidents['accident_id'])].copy()\n",
    "        wider_roads = roads_gdf_buffered[roads_gdf_buffered['width_rank'] >= 4]\n",
    "        if not wider_roads.empty:\n",
    "            reassigned = gpd.sjoin_nearest(\n",
    "                accidents_to_reassign,\n",
    "                wider_roads[['geometry', 'class']],\n",
    "                how='left',\n",
    "                max_distance=10\n",
    "            )\n",
    "            reassigned_matches = reassigned[['accident_id', 'index_right']].copy()\n",
    "            reassigned_matches.columns = ['accident_id', 'road_idx']\n",
    "            reassigned_matches = reassigned_matches.dropna(subset=['road_idx'])\n",
    "            reassigned_matches['road_idx'] = reassigned_matches['road_idx'].astype(int)\n",
    "            matched_accidents = matched_accidents[~matched_accidents['accident_id'].isin(reassigned_matches['accident_id'])]\n",
    "            matched_accidents = pd.concat([matched_accidents, reassigned_matches], ignore_index=True)\n",
    "            logging.info(f\"Reassigned {len(reassigned_matches)} accidents to wider roads\")\n",
    "    \n",
    "    # Count accidents per road\n",
    "    accident_counts = matched_accidents.groupby('road_idx').size().reindex(roads_gdf_local.index, fill_value=0)\n",
    "    roads_gdf_local['accident_count'] = accident_counts\n",
    "    \n",
    "    logging.info(f\"Accidents by road type:\\n{roads_gdf_local.groupby('class')['accident_count'].sum()}\")\n",
    "    \n",
    "    # Filter roads by minimum length\n",
    "    roads_gdf_local = roads_gdf_local[roads_gdf_local['length_m'] >= MIN_ROAD_LENGTH]\n",
    "    \n",
    "    # Log data integrity before calculating accident density\n",
    "    logging.info(f\"length_m stats:\\n{roads_gdf_local['length_m'].describe()}\")\n",
    "    logging.info(f\"accident_count stats:\\n{roads_gdf_local['accident_count'].describe()}\")\n",
    "    \n",
    "    # Calculate accident density\n",
    "    roads_gdf_local['accident_density'] = roads_gdf_local['accident_count'] / (roads_gdf_local['length_m'] / 1000)\n",
    "    roads_gdf_local['accident_density'] = roads_gdf_local['accident_density'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # Store raw accident density before adjustments\n",
    "    roads_gdf_local['accident_density_raw'] = roads_gdf_local['accident_density']\n",
    "    \n",
    "    # Adjust density by width rank\n",
    "    roads_gdf_local['accident_density'] = roads_gdf_local['accident_density'] * (roads_gdf_local['width_rank'].replace(0, 1) / 5)\n",
    "    \n",
    "    # Apply smoothing and logarithmic transformation to reduce sparsity and skewness\n",
    "    roads_gdf_local['accident_density'] = roads_gdf_local['accident_density'] + 0.1  # Add small constant to reduce zeros\n",
    "    roads_gdf_local['accident_density'] = np.log1p(roads_gdf_local['accident_density'])\n",
    "    \n",
    "    # Log accident density integrity\n",
    "    logging.info(f\"NaN in accident_density: {roads_gdf_local['accident_density'].isna().sum()}\")\n",
    "    logging.info(f\"accident_density stats:\\n{roads_gdf_local['accident_density'].describe()}\")\n",
    "    \n",
    "    # Compute road type summary\n",
    "    road_types = roads_gdf_local['class'].unique()\n",
    "    road_types = [rt for rt in road_types if rt != 'bridleway']\n",
    "    summary_data = []\n",
    "    for rt in road_types:\n",
    "        rt_data = roads_gdf_local[roads_gdf_local['class'] == rt]\n",
    "        total_length = rt_data['length_m'].sum()\n",
    "        total_accidents = rt_data['accident_count'].sum()\n",
    "        mean_density = rt_data['accident_density'].mean()\n",
    "        width_rank = rt_data['width_rank'].iloc[0] if not rt_data.empty else 0\n",
    "        summary_data.append({\n",
    "            'class': rt,\n",
    "            'length_m': total_length,\n",
    "            'accident_count': total_accidents,\n",
    "            'accident_density': mean_density,\n",
    "            'width_rank': width_rank\n",
    "        })\n",
    "    \n",
    "    summary = pd.DataFrame(summary_data)\n",
    "    summary = summary[summary['length_m'] > 0]\n",
    "    \n",
    "    # Generate plots\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    summary_sorted = summary.sort_values('width_rank', ascending=False)\n",
    "    sns.barplot(data=summary_sorted, x='class', y='accident_density', hue='width_rank', dodge=False)\n",
    "    plt.xlabel('Road Type')\n",
    "    plt.ylabel('Mean Accident Density (log scale)')\n",
    "    plt.title('Mean Accident Density by Road Type')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Width Rank')\n",
    "    plt.tight_layout()\n",
    "    bar_path = os.path.join(BASE_DIR, 'road_type_accident_bar.png')\n",
    "    plt.savefig(bar_path)\n",
    "    plt.close()\n",
    "    logging.info(f\"Bar chart saved to {bar_path}\")\n",
    "    print(f\"Bar chart saved to {bar_path}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=roads_gdf_local[roads_gdf_local['class'].isin(summary['class'])], \n",
    "                x='class', y='accident_density', hue='width_rank', dodge=False)\n",
    "    plt.xlabel('Road Type')\n",
    "    plt.ylabel('Accident Density (log scale)')\n",
    "    plt.title('Distribution of Accident Density by Road Type')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Width Rank')\n",
    "    plt.tight_layout()\n",
    "    box_path = os.path.join(BASE_DIR, 'road_type_accident_box.png')\n",
    "    plt.savefig(box_path)\n",
    "    plt.close()\n",
    "    logging.info(f\"Box chart saved to {box_path}\")\n",
    "    print(f\"Box chart saved to {box_path}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=summary, x='width_rank', y='accident_density', \n",
    "                    size='length_m', sizes=(50, 500), hue='class', style='class', alpha=0.7)\n",
    "    z = np.polyfit(summary['width_rank'], summary['accident_density'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(summary['width_rank'], p(summary['width_rank']), \"r--\", alpha=0.5)\n",
    "    plt.xlabel('Road Width Rank (1=Path, 5=Motorway)')\n",
    "    plt.ylabel('Mean Accident Density (log scale)')\n",
    "    plt.title('Road Type vs. Accident Density')\n",
    "    plt.grid(True)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    scatter_path = os.path.join(BASE_DIR, 'road_type_accident_scatter.png')\n",
    "    plt.savefig(scatter_path)\n",
    "    plt.close()\n",
    "    logging.info(f\"Scatter plot saved to {scatter_path}\")\n",
    "    print(f\"Scatter plot saved to {scatter_path}\")\n",
    "    \n",
    "    top_types = summary.nlargest(3, 'accident_density')[['class', 'accident_density']]\n",
    "    logging.info(f\"Top 3 road types by accident density:\\n{top_types.round(2)}\")\n",
    "    print(f\"Top 3 road types by accident density:\\n{top_types.round(2)}\")\n",
    "    \n",
    "    logging.info(\"Computing average road accident density per neighborhood...\")\n",
    "    logging.info(f\"Roads DataFrame shape before join: {roads_gdf_local.shape}\")\n",
    "    logging.info(f\"Neighborhoods DataFrame shape before join: {neighborhoods_gdf.shape}\")\n",
    "    \n",
    "    roads_with_neighborhood = gpd.sjoin(\n",
    "        roads_gdf_local[['geometry', 'class', 'length_m', 'width_rank', 'accident_density']],\n",
    "        neighborhoods_gdf[['geometry', 'LIE_NAME']],\n",
    "        how='left',\n",
    "        predicate='intersects'\n",
    "    )\n",
    "    logging.info(f\"Road-neighborhood join resulted in {len(roads_with_neighborhood)} matches with columns: {list(roads_with_neighborhood.columns)}\")\n",
    "    logging.info(f\"Non-NaN LIE_NAME count: {roads_with_neighborhood['LIE_NAME'].notna().sum()}\")\n",
    "    logging.info(f\"Non-NaN accident_density count: {roads_with_neighborhood['accident_density'].notna().sum()}\")\n",
    "    logging.info(f\"Unique LIE_NAME values: {roads_with_neighborhood['LIE_NAME'].nunique()}\")\n",
    "    \n",
    "    avg_accident_density = roads_with_neighborhood.groupby('LIE_NAME')['accident_density'].mean().reset_index()\n",
    "    avg_accident_density.columns = ['LIE_NAME', 'avg_road_accident_density']\n",
    "    logging.info(f\"Number of neighborhoods with calculated avg_accident_density: {len(avg_accident_density)}\")\n",
    "    logging.info(f\"NaN in avg_accident_density: {avg_accident_density['avg_road_accident_density'].isna().sum()}\")\n",
    "    \n",
    "    neighborhoods_gdf = neighborhoods_gdf.merge(avg_accident_density, on='LIE_NAME', how='left')\n",
    "    neighborhoods_gdf['avg_road_accident_density'] = neighborhoods_gdf['avg_road_accident_density'].fillna(0)\n",
    "    logging.info(f\"Assigned avg_road_accident_density to {len(neighborhoods_gdf)} neighborhoods\")\n",
    "    logging.info(f\"Avg road accident density stats:\\n{neighborhoods_gdf['avg_road_accident_density'].describe()}\")\n",
    "    \n",
    "    return summary, roads_gdf_local[['accident_density_raw', 'accident_density']], neighborhoods_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07b95a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pedestrian_road_density(roads_gdf, neighborhoods_gdf):\n",
    "    # Filters roads classified as 'footway', 'pedestrian', or 'cycleway'.\n",
    "    # Performs a spatial join to sum road lengths per neighborhood.\n",
    "    # Calculates density as length (m) / area (km²).\n",
    "    \"\"\"Compute pedestrian road density (length of pedestrian roads per km²) for each neighborhood.\"\"\"\n",
    "    logging.info(\"Computing pedestrian road density per neighborhood...\")\n",
    "    \n",
    "    # Filter pedestrian roads (e.g., footway, pedestrian, cycleway)\n",
    "    pedestrian_classes = ['footway', 'pedestrian', 'cycleway']\n",
    "    pedestrian_roads = roads_gdf[roads_gdf['class'].isin(pedestrian_classes)].copy()\n",
    "    \n",
    "    # Fix geometries\n",
    "    pedestrian_roads['geometry'] = pedestrian_roads['geometry'].apply(fix_geometry)\n",
    "    neighborhoods_gdf_with_idx = neighborhoods_gdf[['geometry', 'LIE_NAME', 'area_km2']].copy()\n",
    "    neighborhoods_gdf_with_idx['geometry'] = neighborhoods_gdf_with_idx['geometry'].apply(fix_geometry)\n",
    "    \n",
    "    # Perform spatial join\n",
    "    pedestrian_road_neighborhoods = gpd.sjoin(\n",
    "        pedestrian_roads[['geometry', 'length_m']],\n",
    "        neighborhoods_gdf_with_idx,\n",
    "        how='left',\n",
    "        predicate='intersects'\n",
    "    )\n",
    "    \n",
    "    # Sum pedestrian road lengths per neighborhood\n",
    "    pedestrian_length = pedestrian_road_neighborhoods.groupby('LIE_NAME')['length_m'].sum()\n",
    "    \n",
    "    # Calculate density (length in meters per km²)\n",
    "    neighborhoods_gdf['pedestrian_road_density'] = neighborhoods_gdf['LIE_NAME'].map(pedestrian_length).fillna(0) / (neighborhoods_gdf['area_km2'].replace(0, 1e-6) * 1000)\n",
    "    \n",
    "    logging.info(f\"pedestrian_road_density stats:\\n{neighborhoods_gdf['pedestrian_road_density'].describe()}\")\n",
    "    return neighborhoods_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d0683",
   "metadata": {},
   "source": [
    "Cell 4 Main Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3d3ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    logging.info(\"Stage 1: Loading and preparing data...\")\n",
    "    \n",
    "    # Define file paths and their corresponding keys\n",
    "    data_files = {\n",
    "        'neighborhoods': LANDUSE_NDVI_PATH,\n",
    "        'buildings': OSM_BUILDINGS_PATH,\n",
    "        'roads': OSM_ROADS_PATH,\n",
    "        'trees': OSM_TREES_PATH,\n",
    "        'transit': OSM_TRANSIT_PATH,\n",
    "        'urban_masterplan': URBAN_MASTERPLAN_PATH,\n",
    "        'accidents': ACCIDENTS_PATH,\n",
    "        'population': POPULATION_PATH\n",
    "    }\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # Load data with progress bar\n",
    "    for key, path in tqdm(data_files.items(), desc=\"Loading files\"):\n",
    "        try:\n",
    "            if key == 'population':\n",
    "                with open(path, 'r') as f:\n",
    "                    data[key] = pd.DataFrame(json.load(f))\n",
    "                logging.info(f\"Columns in population_df after loading: {list(data[key].columns)}\")\n",
    "            elif path.endswith('.geoparquet'):\n",
    "                data[key] = gpd.read_parquet(path)\n",
    "            else:\n",
    "                data[key] = gpd.read_file(path)\n",
    "            logging.info(f\"Loaded {key} with shape {data[key].shape}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load {key} from {path}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    # Log columns of neighborhoods_gdf to debug missing 'ndvi' and 'area_km2'\n",
    "    neighborhoods_gdf = data['neighborhoods']\n",
    "    logging.info(f\"Columns in neighborhoods_gdf after loading: {list(neighborhoods_gdf.columns)}\")\n",
    "    \n",
    "    # Check for alternative NDVI column names and rename if found\n",
    "    possible_ndvi_columns = ['ndvi_mean', 'NDVI', 'ndvi_value']\n",
    "    for col in possible_ndvi_columns:\n",
    "        if col in neighborhoods_gdf.columns and 'ndvi' not in neighborhoods_gdf.columns:\n",
    "            logging.info(f\"Found alternative NDVI column '{col}'. Renaming to 'ndvi'.\")\n",
    "            neighborhoods_gdf['ndvi'] = neighborhoods_gdf[col]\n",
    "            break\n",
    "    \n",
    "    # Ensure all GeoDataFrames are in the same CRS\n",
    "    target_crs = 'EPSG:3826'\n",
    "    for key in ['neighborhoods', 'buildings', 'roads', 'trees', 'transit', 'urban_masterplan', 'accidents']:\n",
    "        if key in data and isinstance(data[key], gpd.GeoDataFrame):\n",
    "            if data[key].crs is None:\n",
    "                logging.warning(f\"No CRS defined for {key}. Assuming EPSG:4326.\")\n",
    "                data[key].set_crs('EPSG:4326', inplace=True)\n",
    "            if data[key].crs != target_crs:\n",
    "                data[key] = data[key].to_crs(target_crs)\n",
    "                logging.info(f\"Converted {key} to CRS {target_crs}\")\n",
    "    \n",
    "    # Fix geometries in all GeoDataFrames\n",
    "    for key in ['neighborhoods', 'buildings', 'roads', 'trees', 'transit', 'urban_masterplan', 'accidents']:\n",
    "        if key in data and isinstance(data[key], gpd.GeoDataFrame):\n",
    "            data[key]['geometry'] = data[key]['geometry'].apply(fix_geometry)\n",
    "            invalid_geoms = data[key][~data[key].geometry.is_valid]\n",
    "            if not invalid_geoms.empty:\n",
    "                logging.warning(f\"Found {len(invalid_geoms)} invalid geometries in {key} after fixing.\")\n",
    "                data[key] = data[key][data[key].geometry.is_valid]\n",
    "    \n",
    "    # Compute intersections for neighborhoods\n",
    "    logging.info(\"Computing intersections for neighborhoods...\")\n",
    "    roads_gdf = data['roads']\n",
    "    \n",
    "    # Log columns of roads_gdf to debug missing 'length_m'\n",
    "    logging.info(f\"Columns in roads_gdf after loading: {list(roads_gdf.columns)}\")\n",
    "    \n",
    "    # Extract endpoints from road segments\n",
    "    logging.info(\"Extracting endpoints from road segments...\")\n",
    "    endpoints = []\n",
    "    road_indices = []\n",
    "    for idx, row in tqdm(roads_gdf.iterrows(), total=len(roads_gdf), desc=\"Extracting endpoints\"):\n",
    "        geom = row['geometry']\n",
    "        if geom.geom_type == 'LineString':\n",
    "            coords = list(geom.coords)\n",
    "            start_point = Point(coords[0])\n",
    "            end_point = Point(coords[-1])\n",
    "            if start_point.is_valid and end_point.is_valid:\n",
    "                endpoints.extend([start_point, end_point])\n",
    "                road_indices.extend([idx, idx])\n",
    "        elif geom.geom_type == 'MultiLineString':\n",
    "            for line in geom.geoms:\n",
    "                coords = list(line.coords)\n",
    "                start_point = Point(coords[0])\n",
    "                end_point = Point(coords[-1])\n",
    "                if start_point.is_valid and end_point.is_valid:\n",
    "                    endpoints.extend([start_point, end_point])\n",
    "                    road_indices.extend([idx, idx])\n",
    "    \n",
    "    if not endpoints:\n",
    "        logging.warning(\"No valid endpoints extracted from road segments. Using fallback method for intersections.\")\n",
    "        road_neighborhoods = gpd.sjoin(roads_gdf[['geometry']], neighborhoods_gdf[['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "        intersection_counts = road_neighborhoods.groupby('index_right').size()\n",
    "        neighborhoods_gdf['intersection_count'] = intersection_counts.reindex(neighborhoods_gdf.index, fill_value=0)\n",
    "    else:\n",
    "        endpoints_gdf = gpd.GeoDataFrame({'geometry': endpoints, 'road_idx': road_indices}, crs=target_crs)\n",
    "        \n",
    "        # Create a spatial index for endpoints\n",
    "        endpoints_sindex = endpoints_gdf.sindex\n",
    "        \n",
    "        # Cluster endpoints to identify intersections (points shared by 3 or more roads)\n",
    "        logging.info(\"Building endpoint-to-road mapping...\")\n",
    "        endpoint_to_roads = {}\n",
    "        for idx, point in tqdm(endpoints_gdf.iterrows(), total=len(endpoints_gdf), desc=\"Building endpoint-to-road mapping\"):\n",
    "            point_geom = point['geometry']\n",
    "            road_idx = point['road_idx']\n",
    "            point_key = (round(point_geom.x, 6), round(point_geom.y, 6))  # Round to avoid floating-point precision issues\n",
    "            if point_key not in endpoint_to_roads:\n",
    "                endpoint_to_roads[point_key] = set()\n",
    "            endpoint_to_roads[point_key].add(road_idx)\n",
    "        \n",
    "        logging.info(\"Identifying intersections...\")\n",
    "        intersections = []\n",
    "        for point_key, road_ids in tqdm(endpoint_to_roads.items(), desc=\"Identifying intersections\"):\n",
    "            if len(road_ids) >= 3:  # Intersection if shared by 3 or more roads\n",
    "                intersections.append(Point(point_key))\n",
    "        \n",
    "        if not intersections:\n",
    "            logging.warning(\"No intersections found using endpoint clustering. Using fallback method.\")\n",
    "            road_neighborhoods = gpd.sjoin(roads_gdf[['geometry']], neighborhoods_gdf[['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "            intersection_counts = road_neighborhoods.groupby('index_right').size()\n",
    "            neighborhoods_gdf['intersection_count'] = intersection_counts.reindex(neighborhoods_gdf.index, fill_value=0)\n",
    "        else:\n",
    "            intersections_gdf = gpd.GeoDataFrame({'geometry': intersections}, crs=target_crs)\n",
    "            \n",
    "            # Count intersections per neighborhood\n",
    "            logging.info(\"Counting intersections per neighborhood...\")\n",
    "            intersections_joined = gpd.sjoin(intersections_gdf, neighborhoods_gdf[['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "            intersection_counts = intersections_joined.groupby('index_right').size()\n",
    "            neighborhoods_gdf['intersection_count'] = intersection_counts.reindex(neighborhoods_gdf.index, fill_value=0)\n",
    "    \n",
    "    # Compute or verify area_km2\n",
    "    if 'area_km2' not in neighborhoods_gdf.columns:\n",
    "        logging.warning(\"'area_km2' column missing in neighborhoods_gdf. Computing from geometry...\")\n",
    "        neighborhoods_gdf['area_m2'] = neighborhoods_gdf['geometry'].area\n",
    "        neighborhoods_gdf['area_km2'] = neighborhoods_gdf['area_m2'] / 1_000_000  # Convert m² to km²\n",
    "        logging.info(f\"Computed area_km2 stats:\\n{neighborhoods_gdf['area_km2'].describe()}\")\n",
    "    else:\n",
    "        logging.info(f\"area_km2 already present. Stats:\\n{neighborhoods_gdf['area_km2'].describe()}\")\n",
    "    \n",
    "    # Compute intersection density\n",
    "    neighborhoods_gdf['intersection_density'] = neighborhoods_gdf['intersection_count'] / neighborhoods_gdf['area_km2'].replace(0, 1e-6)\n",
    "    logging.info(f\"Intersection count stats:\\n{neighborhoods_gdf['intersection_count'].describe()}\")\n",
    "    logging.info(f\"Intersection density stats:\\n{neighborhoods_gdf['intersection_density'].describe()}\")\n",
    "    \n",
    "    # Cache the result\n",
    "    try:\n",
    "        neighborhoods_gdf.to_parquet(INTERSECTION_CACHE_PATH)\n",
    "        logging.info(f\"Saved neighborhoods with intersections to {INTERSECTION_CACHE_PATH}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save neighborhoods with intersections: {e}\")\n",
    "    \n",
    "    data['neighborhoods'] = neighborhoods_gdf\n",
    "    \n",
    "    # Compute tree count per neighborhood\n",
    "    logging.info(\"Computing tree count per neighborhood...\")\n",
    "    trees_gdf = data['trees']\n",
    "    trees_joined = gpd.sjoin(trees_gdf[['geometry']], data['neighborhoods'][['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "    tree_counts = trees_joined.groupby('index_right').size()\n",
    "    data['neighborhoods']['tree_count'] = tree_counts.reindex(data['neighborhoods'].index, fill_value=0)\n",
    "    \n",
    "    # Compute transit count per neighborhood\n",
    "    logging.info(\"Computing transit count per neighborhood...\")\n",
    "    transit_gdf = data['transit']\n",
    "    transit_joined = gpd.sjoin(transit_gdf[['geometry']], data['neighborhoods'][['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "    transit_counts = transit_joined.groupby('index_right').size()\n",
    "    data['neighborhoods']['transit_count'] = transit_counts.reindex(data['neighborhoods'].index, fill_value=0)\n",
    "    \n",
    "    # Compute accident count per neighborhood\n",
    "    logging.info(\"Computing accident count per neighborhood...\")\n",
    "    accidents_gdf = data['accidents']\n",
    "    accidents_buffered = accidents_gdf.copy()\n",
    "    accidents_buffered['geometry'] = accidents_buffered['geometry'].buffer(BUFFER_DISTANCE)\n",
    "    accidents_joined = gpd.sjoin(accidents_buffered[['geometry']], data['neighborhoods'][['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "    accident_counts = accidents_joined.groupby('index_right').size()\n",
    "    data['neighborhoods']['accident_count'] = accident_counts.reindex(data['neighborhoods'].index, fill_value=0)\n",
    "    \n",
    "    # Compute road density per neighborhood\n",
    "    logging.info(\"Computing road density per neighborhood...\")\n",
    "    roads_gdf = data['roads']\n",
    "    \n",
    "    # Log columns of roads_gdf to debug missing 'length_m'\n",
    "    logging.info(f\"Columns in roads_gdf before computing road density: {list(roads_gdf.columns)}\")\n",
    "    \n",
    "    # Compute length_m if missing\n",
    "    if 'length_m' not in roads_gdf.columns:\n",
    "        logging.warning(\"'length_m' column missing in roads_gdf. Computing from geometry...\")\n",
    "        roads_gdf['length_m'] = roads_gdf['geometry'].length  # Length in meters (since CRS is EPSG:3826)\n",
    "        logging.info(f\"Computed length_m stats:\\n{roads_gdf['length_m'].describe()}\")\n",
    "    \n",
    "    roads_joined = gpd.sjoin(roads_gdf[['geometry', 'length_m']], data['neighborhoods'][['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "    road_lengths = roads_joined.groupby('index_right')['length_m'].sum()\n",
    "    data['neighborhoods']['road_density'] = road_lengths.reindex(data['neighborhoods'].index, fill_value=0) / (data['neighborhoods']['area_km2'] * 1000)\n",
    "    logging.info(f\"Road density stats:\\n{data['neighborhoods']['road_density'].describe()}\")\n",
    "    \n",
    "    # Merge population data\n",
    "    logging.info(\"Merging population data...\")\n",
    "    population_df = data['population']\n",
    "    population_df['LIE_NAME'] = population_df['LIE_NAME'].astype(str).str.strip()\n",
    "    data['neighborhoods']['LIE_NAME'] = data['neighborhoods']['LIE_NAME'].astype(str).str.strip()\n",
    "    \n",
    "    # Check for possible column names for total_population and elderly_percentage\n",
    "    expected_cols = ['total_population', 'elderly_percentage']\n",
    "    population_cols = list(population_df.columns)\n",
    "    missing_cols = [col for col in expected_cols if col not in population_cols]\n",
    "    \n",
    "    if missing_cols:\n",
    "        logging.warning(f\"Expected columns {missing_cols} not found in population_df. Attempting to find alternatives...\")\n",
    "        total_pop_alt = None\n",
    "        elderly_alt = None\n",
    "        for col in population_cols:\n",
    "            col_lower = col.lower()\n",
    "            if 'population' in col_lower and total_pop_alt is None:\n",
    "                total_pop_alt = col\n",
    "                logging.info(f\"Found alternative for total_population: {col}\")\n",
    "            if 'elderly' in col_lower and elderly_alt is None:\n",
    "                elderly_alt = col\n",
    "                logging.info(f\"Found alternative for elderly_percentage: {col}\")\n",
    "        \n",
    "        # Rename columns if alternatives are found\n",
    "        if total_pop_alt:\n",
    "            population_df = population_df.rename(columns={total_pop_alt: 'total_population'})\n",
    "        else:\n",
    "            logging.warning(\"No alternative found for total_population. Setting to 0.\")\n",
    "            population_df['total_population'] = 0\n",
    "        if elderly_alt:\n",
    "            population_df = population_df.rename(columns={elderly_alt: 'elderly_percentage'})\n",
    "        else:\n",
    "            logging.warning(\"No alternative found for elderly_percentage. Setting to 0.\")\n",
    "            population_df['elderly_percentage'] = 0\n",
    "    \n",
    "    # Perform the merge\n",
    "    data['neighborhoods'] = data['neighborhoods'].merge(\n",
    "        population_df[['LIE_NAME', 'total_population', 'elderly_percentage']],\n",
    "        on='LIE_NAME',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Compute land use percentages\n",
    "    logging.info(\"Computing land use percentages for neighborhoods...\")\n",
    "    urban_masterplan_gdf = data['urban_masterplan']\n",
    "    print_percentage_calculation(data['neighborhoods'], urban_masterplan_gdf, sample_size=3)\n",
    "    \n",
    "    for idx, row in data['neighborhoods'].iterrows():\n",
    "        neighborhood_geom = row['geometry']\n",
    "        if not neighborhood_geom.is_valid:\n",
    "            continue\n",
    "        relevant_masterplan = urban_masterplan_gdf[urban_masterplan_gdf.intersects(neighborhood_geom)]\n",
    "        if relevant_masterplan.empty:\n",
    "            continue\n",
    "        \n",
    "        temp_gdf = gpd.GeoDataFrame({'geometry': [neighborhood_geom]}, crs='EPSG:3826')\n",
    "        intersected = gpd.overlay(temp_gdf, relevant_masterplan, how='intersection', keep_geom_type=False)\n",
    "        if intersected.empty:\n",
    "            continue\n",
    "        \n",
    "        intersected['geometry'] = intersected['geometry'].apply(fix_geometry)\n",
    "        intersected = intersected[intersected.geometry.is_valid & ~intersected.geometry.is_empty]\n",
    "        if intersected.empty:\n",
    "            continue\n",
    "        \n",
    "        intersected['priority'] = intersected['Category'].map(CATEGORY_PRIORITY)\n",
    "        intersected = intersected.sort_values(by='priority', ascending=False)\n",
    "        \n",
    "        total_area = intersected.geometry.union_all().area\n",
    "        remaining_geom = intersected.geometry.union_all()\n",
    "        category_areas = {}\n",
    "        for category in intersected['Category'].unique():\n",
    "            category_rows = intersected[intersected['Category'] == category]\n",
    "            category_geom = category_rows.geometry.union_all()\n",
    "            try:\n",
    "                category_area_geom = category_geom.intersection(remaining_geom)\n",
    "                category_area = category_area_geom.area\n",
    "                category_areas[category] = category_area\n",
    "                remaining_geom = remaining_geom.difference(category_area_geom)\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Topology error for category {category} in neighborhood {row['LIE_NAME']}: {e}\")\n",
    "                category_areas[category] = 0.0\n",
    "        \n",
    "        for category in CATEGORY_PRIORITY.keys():\n",
    "            area = category_areas.get(category, 0.0)\n",
    "            percentage = (area / total_area * 100) if total_area > 0 else 0.0\n",
    "            data['neighborhoods'].at[idx, f'land_use_{category.lower()}_percent'] = percentage\n",
    "    \n",
    "    # Fill NaN values in land use percentages\n",
    "    for category in CATEGORY_PRIORITY.keys():\n",
    "        col = f'land_use_{category.lower()}_percent'\n",
    "        data['neighborhoods'][col] = data['neighborhoods'][col].fillna(0.0)\n",
    "    \n",
    "    # Fill NaN values in other columns\n",
    "    for col in ['intersection_count', 'intersection_density', 'tree_count', 'transit_count', 'accident_count', 'road_density', 'total_population', 'elderly_percentage']:\n",
    "        data['neighborhoods'][col] = data['neighborhoods'][col].fillna(0)\n",
    "    \n",
    "    # Print data structure summary\n",
    "    print_data_structure(data)\n",
    "    \n",
    "    logging.info(\"Finished loading and preparing data.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48993e82",
   "metadata": {},
   "source": [
    "Cell 5 compute_intersection_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2d1971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intersection_counts(neighborhoods_gdf, roads_gdf):\n",
    "    logging.info(\"Computing intersection counts for neighborhoods...\")\n",
    "    \n",
    "    # Extract endpoints from road segments\n",
    "    def get_endpoints(line):\n",
    "        if line is None or line.is_empty:\n",
    "            return []\n",
    "        coords = list(line.coords)\n",
    "        return [Point(coords[0]), Point(coords[-1])]\n",
    "    \n",
    "    endpoints = []\n",
    "    for idx, row in tqdm(roads_gdf.iterrows(), total=len(roads_gdf), desc=\"Extracting endpoints\"):\n",
    "        points = get_endpoints(row['geometry'])\n",
    "        for point in points:\n",
    "            endpoints.append({'geometry': point, 'road_idx': idx})\n",
    "    \n",
    "    endpoints_gdf = gpd.GeoDataFrame(endpoints, crs='EPSG:3826')\n",
    "    \n",
    "    # Build a mapping of endpoints to road indices\n",
    "    endpoint_to_roads = {}\n",
    "    for idx, row in tqdm(endpoints_gdf.iterrows(), total=len(endpoints_gdf), desc=\"Building endpoint-to-road mapping\"):\n",
    "        point = row['geometry']\n",
    "        road_idx = row['road_idx']\n",
    "        point_tuple = (point.x, point.y)\n",
    "        if point_tuple not in endpoint_to_roads:\n",
    "            endpoint_to_roads[point_tuple] = set()\n",
    "        endpoint_to_roads[point_tuple].add(road_idx)\n",
    "    \n",
    "    # Identify intersections (endpoints shared by 3 or more roads)\n",
    "    intersections = []\n",
    "    for point_tuple, road_indices in tqdm(endpoint_to_roads.items(), desc=\"Identifying intersections\"):\n",
    "        if len(road_indices) >= 3:  # Intersection if 3 or more roads share the endpoint\n",
    "            intersections.append({'geometry': Point(point_tuple)})\n",
    "    \n",
    "    if not intersections:\n",
    "        logging.warning(\"No intersections found. Setting intersection counts to 0.\")\n",
    "        neighborhoods_gdf['intersection_count'] = 0\n",
    "        neighborhoods_gdf['intersection_density'] = 0.0\n",
    "        return neighborhoods_gdf\n",
    "    \n",
    "    intersections_gdf = gpd.GeoDataFrame(intersections, crs='EPSG:3826')\n",
    "    \n",
    "    # Spatial join to count intersections per neighborhood\n",
    "    intersection_counts = gpd.sjoin(\n",
    "        neighborhoods_gdf[['geometry', 'LIE_NAME']],\n",
    "        intersections_gdf,\n",
    "        how='left',\n",
    "        predicate='contains'\n",
    "    )\n",
    "    intersection_counts = intersection_counts.groupby('LIE_NAME').size().reindex(neighborhoods_gdf['LIE_NAME'], fill_value=0)\n",
    "    neighborhoods_gdf['intersection_count'] = intersection_counts\n",
    "    \n",
    "    # Compute intersection density (intersections per km²)\n",
    "    neighborhoods_gdf['intersection_density'] = neighborhoods_gdf['intersection_count'] / neighborhoods_gdf['area_km2']\n",
    "    neighborhoods_gdf['intersection_density'] = neighborhoods_gdf['intersection_density'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    logging.info(f\"Intersection count stats: {neighborhoods_gdf['intersection_count'].describe()}\")\n",
    "    logging.info(f\"Intersection density stats: {neighborhoods_gdf['intersection_density'].describe()}\")\n",
    "    \n",
    "    return neighborhoods_gdf\n",
    "\n",
    "def build_graph(data, force_recompute=False):\n",
    "    logging.info(\"Stage 2: Building the graph...\")\n",
    "    \n",
    "    # Compute data hash to check if graph needs recomputing\n",
    "    data_hash = compute_data_hash(data)\n",
    "    cached_hash = None\n",
    "    if os.path.exists(GRAPH_DATA_HASH_PATH):\n",
    "        with open(GRAPH_DATA_HASH_PATH, 'r') as f:\n",
    "            cached_hash = f.read().strip()\n",
    "    \n",
    "    if not force_recompute and cached_hash == data_hash and all(\n",
    "        os.path.exists(path) for path in [GRAPH_NODES_CACHE_PATH, GRAPH_EDGES_CACHE_PATH, GRAPH_NODE_ID_CACHE_PATH]\n",
    "    ):\n",
    "        logging.info(\"Data unchanged. Loading graph from cache...\")\n",
    "        nodes_df = cudf.read_parquet(GRAPH_NODES_CACHE_PATH)\n",
    "        edges_df = cudf.read_parquet(GRAPH_EDGES_CACHE_PATH)\n",
    "        with open(GRAPH_NODE_ID_CACHE_PATH, 'r') as f:\n",
    "            node_id_to_index = json.load(f)\n",
    "        G = cugraph.Graph()\n",
    "        G.from_cudf_edgelist(\n",
    "            edges_df,\n",
    "            source='src',\n",
    "            destination='dst',\n",
    "            edge_attr='weight'\n",
    "        )\n",
    "        G._nodes = nodes_df\n",
    "        logging.info(\"Graph loaded from cache.\")\n",
    "        return G\n",
    "    \n",
    "    neighborhoods_gdf = data['neighborhoods'].copy()\n",
    "    buildings_gdf = data['buildings'].copy()\n",
    "    roads_gdf = data['roads'].copy()\n",
    "    trees_gdf = data['trees'].copy()\n",
    "    transit_gdf = data['transit'].copy()\n",
    "    \n",
    "    # Create nodes for neighborhoods, buildings, roads, trees, and transit\n",
    "    nodes = []\n",
    "    node_id_to_index = {}\n",
    "    current_idx = 0\n",
    "    \n",
    "    # Neighborhood nodes\n",
    "    for idx, row in neighborhoods_gdf.iterrows():\n",
    "        node_id = f\"neighborhood_{row['LIE_NAME']}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'neighborhood',\n",
    "            'LIE_NAME': row['LIE_NAME'],\n",
    "            'geometry': row['geometry'].centroid,\n",
    "            'ndvi_mean': row['ndvi_mean'],\n",
    "            'total_population': row['total_population'],\n",
    "            'elderly_percentage': row['elderly_percentage'],\n",
    "            'area_km2': row['area_km2']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    # Building nodes\n",
    "    for idx, row in buildings_gdf.iterrows():\n",
    "        node_id = f\"building_{idx}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'building',\n",
    "            'LIE_NAME': None,\n",
    "            'geometry': row['geometry'].centroid,\n",
    "            'building_type': row['building'],\n",
    "            'area_m2': row['area_m2']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    # Road nodes\n",
    "    for idx, row in roads_gdf.iterrows():\n",
    "        node_id = f\"road_{idx}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'road',\n",
    "            'LIE_NAME': None,\n",
    "            'geometry': row['geometry'].centroid,\n",
    "            'class': row['class'],\n",
    "            'length_m': row['length_m']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    # Tree nodes\n",
    "    for idx, row in trees_gdf.iterrows():\n",
    "        node_id = f\"tree_{idx}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'tree',\n",
    "            'LIE_NAME': None,\n",
    "            'geometry': row['geometry']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    # Transit nodes\n",
    "    for idx, row in transit_gdf.iterrows():\n",
    "        node_id = f\"transit_{idx}\"\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        nodes.append({\n",
    "            'node_idx': current_idx,\n",
    "            'node_id': node_id,\n",
    "            'type': 'transit',\n",
    "            'LIE_NAME': None,\n",
    "            'geometry': row['geometry'],\n",
    "            'class': row['class']\n",
    "        })\n",
    "        current_idx += 1\n",
    "    \n",
    "    nodes_df = pd.DataFrame(nodes)\n",
    "    nodes_gdf = gpd.GeoDataFrame(nodes_df, geometry='geometry', crs='EPSG:3826')\n",
    "    nodes_df = cudf.from_pandas(nodes_df.drop(columns=['geometry']))\n",
    "    \n",
    "    # Create edges based on spatial proximity\n",
    "    edges = []\n",
    "    nodes_gdf_sindex = nodes_gdf.sindex\n",
    "    \n",
    "    # Neighborhood-to-neighborhood edges (shared borders)\n",
    "    logging.info(\"Creating neighborhood-to-neighborhood edges...\")\n",
    "    for idx1, row1 in neighborhoods_gdf.iterrows():\n",
    "        geom1 = row1['geometry']\n",
    "        node_idx1 = node_id_to_index[f\"neighborhood_{row1['LIE_NAME']}\"]\n",
    "        possible_matches = list(nodes_gdf_sindex.query(geom1, predicate='intersects'))\n",
    "        for idx2 in possible_matches:\n",
    "            row2 = nodes_gdf.iloc[idx2]\n",
    "            if row2['type'] != 'neighborhood':\n",
    "                continue\n",
    "            if row1['LIE_NAME'] == row2['LIE_NAME']:\n",
    "                continue\n",
    "            geom2 = neighborhoods_gdf[neighborhoods_gdf['LIE_NAME'] == row2['LIE_NAME']]['geometry'].iloc[0]\n",
    "            if geom1.intersects(geom2):\n",
    "                node_idx2 = node_id_to_index[f\"neighborhood_{row2['LIE_NAME']}\"]\n",
    "                edges.append({\n",
    "                    'src': node_idx1,\n",
    "                    'dst': node_idx2,\n",
    "                    'weight': 1.0\n",
    "                })\n",
    "    \n",
    "    # Other edges (neighborhood to building, road, tree, transit)\n",
    "    logging.info(\"Creating edges between neighborhoods and other entities...\")\n",
    "    for idx, row in tqdm(nodes_gdf.iterrows(), total=len(nodes_gdf), desc=\"Creating edges\"):\n",
    "        if row['type'] == 'neighborhood':\n",
    "            continue\n",
    "        geom = row['geometry']\n",
    "        possible_matches = list(neighborhoods_gdf.sindex.query(geom, predicate='contains'))\n",
    "        for match_idx in possible_matches:\n",
    "            neighborhood = neighborhoods_gdf.iloc[match_idx]\n",
    "            if neighborhood['geometry'].contains(geom):\n",
    "                node_idx1 = node_id_to_index[row['node_id']]\n",
    "                node_idx2 = node_id_to_index[f\"neighborhood_{neighborhood['LIE_NAME']}\"]\n",
    "                weight = 1.0\n",
    "                if row['type'] == 'transit':\n",
    "                    weight = 2.0  # Higher weight for transit nodes\n",
    "                edges.append({\n",
    "                    'src': node_idx1,\n",
    "                    'dst': node_idx2,\n",
    "                    'weight': weight\n",
    "                })\n",
    "                edges.append({\n",
    "                    'src': node_idx2,\n",
    "                    'dst': node_idx1,\n",
    "                    'weight': weight\n",
    "                })\n",
    "    \n",
    "    edges_df = cudf.DataFrame(edges)\n",
    "    \n",
    "    # Build the graph\n",
    "    G = cugraph.Graph()\n",
    "    G.from_cudf_edgelist(\n",
    "        edges_df,\n",
    "        source='src',\n",
    "        destination='dst',\n",
    "        edge_attr='weight'\n",
    "    )\n",
    "    G._nodes = nodes_df\n",
    "    \n",
    "    # Cache the graph\n",
    "    nodes_df.to_parquet(GRAPH_NODES_CACHE_PATH)\n",
    "    edges_df.to_parquet(GRAPH_EDGES_CACHE_PATH)\n",
    "    with open(GRAPH_NODE_ID_CACHE_PATH, 'w') as f:\n",
    "        json.dump(node_id_to_index, f)\n",
    "    with open(GRAPH_DATA_HASH_PATH, 'w') as f:\n",
    "        f.write(data_hash)\n",
    "    \n",
    "    logging.info(\"Graph construction completed.\")\n",
    "    return G\n",
    "\n",
    "def prepare_gnn_data(G):\n",
    "    logging.info(\"Stage 3: Preparing data for GNN...\")\n",
    "    \n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    edges_df = G.edgelist.edgelist_df.to_pandas()\n",
    "    \n",
    "    # Create node features\n",
    "    feature_columns = [\n",
    "        'ndvi_mean', 'total_population', 'elderly_percentage', 'area_km2',\n",
    "        'area_m2', 'length_m'\n",
    "    ]\n",
    "    features = []\n",
    "    for idx, row in nodes_df.iterrows():\n",
    "        node_features = []\n",
    "        for col in feature_columns:\n",
    "            value = row.get(col, 0.0)\n",
    "            if pd.isna(value):\n",
    "                value = 0.0\n",
    "            node_features.append(value)\n",
    "        \n",
    "        # One-hot encode node type\n",
    "        node_type = row['type']\n",
    "        type_encoding = [0] * 5  # 5 types: neighborhood, building, road, tree, transit\n",
    "        type_mapping = {\n",
    "            'neighborhood': 0,\n",
    "            'building': 1,\n",
    "            'road': 2,\n",
    "            'tree': 3,\n",
    "            'transit': 4\n",
    "        }\n",
    "        type_idx = type_mapping.get(node_type, 0)\n",
    "        type_encoding[type_idx] = 1\n",
    "        node_features.extend(type_encoding)\n",
    "        \n",
    "        features.append(node_features)\n",
    "    \n",
    "    feature_matrix = np.array(features, dtype=np.float32)\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    numerical_features = feature_matrix[:, :len(feature_columns)]\n",
    "    means = numerical_features.mean(axis=0)\n",
    "    stds = numerical_features.std(axis=0)\n",
    "    stds[stds == 0] = 1  # Avoid division by zero\n",
    "    numerical_features = (numerical_features - means) / stds\n",
    "    feature_matrix[:, :len(feature_columns)] = numerical_features\n",
    "    \n",
    "    # Create edge indices for PyG\n",
    "    edge_index = torch.tensor(\n",
    "        np.array([edges_df['src'].values, edges_df['dst'].values]),\n",
    "        dtype=torch.long\n",
    "    )\n",
    "    edge_attr = torch.tensor(edges_df['weight'].values, dtype=torch.float)\n",
    "    \n",
    "    # Create target (walkability score) for neighborhood nodes\n",
    "    y = np.zeros(len(nodes_df), dtype=np.float32)\n",
    "    if 'walkability_score' in nodes_df.columns:\n",
    "        walkability_scores = nodes_df['walkability_score'].fillna(0).values\n",
    "        mask = nodes_df['type'] == 'neighborhood'\n",
    "        y[mask] = walkability_scores[mask]\n",
    "    else:\n",
    "        logging.warning(\"Walkability scores not found in nodes_df. Setting targets to 0.\")\n",
    "    \n",
    "    y = torch.tensor(y, dtype=torch.float)\n",
    "    \n",
    "    node_type_mapping = {\n",
    "        'neighborhood': 0,\n",
    "        'building': 1,\n",
    "        'road': 2,\n",
    "        'tree': 3,\n",
    "        'transit': 4\n",
    "    }\n",
    "    node_type = nodes_df['type'].map(node_type_mapping).fillna(-1).astype(int).values\n",
    "    node_type = torch.tensor(node_type, dtype=torch.long)\n",
    "    \n",
    "    data = Data(\n",
    "        x=torch.tensor(feature_matrix, dtype=torch.float),\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=y,\n",
    "        node_type=node_type\n",
    "    )\n",
    "    \n",
    "    logging.info(\"GNN data prepared.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b04d9d",
   "metadata": {},
   "source": [
    "Cell 6: Graph Construction (build_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4158e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_neighborhood_neighborhood_edges(args):\n",
    "    idx, row, neighborhoods_gdf, neighborhood_sindex = args\n",
    "    edges = []\n",
    "    geom = row['geometry']\n",
    "    possible_matches_index = list(neighborhood_sindex.intersection(geom.bounds))\n",
    "    for other_idx in possible_matches_index:\n",
    "        if other_idx != idx:\n",
    "            other_row = neighborhoods_gdf.iloc[other_idx]\n",
    "            other_geom = other_row['geometry']\n",
    "            try:\n",
    "                if geom.buffer(1e-3).intersects(other_geom.buffer(1e-3)) or geom.buffer(1e-3).touches(other_geom.buffer(1e-3)):\n",
    "                    src = f\"neighborhood_{idx}\"\n",
    "                    dst = f\"neighborhood_{other_idx}\"\n",
    "                    edges.append({'src': src, 'dst': dst})\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error checking intersection between neighborhood {idx} and {other_idx}: {e}\")\n",
    "    return edges\n",
    "\n",
    "def compute_neighborhood_building_edges(args):\n",
    "    idx, row, buildings_gdf, building_sindex = args\n",
    "    edges = []\n",
    "    geom = row['geometry']\n",
    "    possible_matches_index = list(building_sindex.intersection(geom.bounds))\n",
    "    for building_idx in possible_matches_index:\n",
    "        building_row = buildings_gdf.iloc[building_idx]\n",
    "        building_geom = building_row['geometry']\n",
    "        try:\n",
    "            if geom.buffer(1e-3).intersects(building_geom.buffer(1e-3)):\n",
    "                src = f\"neighborhood_{idx}\"\n",
    "                dst = f\"building_{building_idx}\"\n",
    "                edges.append({'src': src, 'dst': dst})\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error checking intersection between neighborhood {idx} and building {building_idx}: {e}\")\n",
    "    return edges\n",
    "\n",
    "def compute_neighborhood_road_edges(args):\n",
    "    idx, row, roads_gdf, road_sindex = args\n",
    "    edges = []\n",
    "    geom = row['geometry']\n",
    "    possible_matches_index = list(road_sindex.intersection(geom.bounds))\n",
    "    for road_idx in possible_matches_index:\n",
    "        road_row = roads_gdf.iloc[road_idx]\n",
    "        road_geom = road_row['geometry']\n",
    "        try:\n",
    "            if geom.buffer(1e-3).intersects(road_geom.buffer(1e-3)):\n",
    "                src = f\"neighborhood_{idx}\"\n",
    "                dst = f\"road_{road_idx}\"\n",
    "                edges.append({'src': src, 'dst': dst})\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error checking intersection between neighborhood {idx} and road {road_idx}: {e}\")\n",
    "    return edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cdc1557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(data, force_recompute=False):\n",
    "    import cudf\n",
    "    import cugraph\n",
    "    import logging\n",
    "    import json\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    logging.info(\"Stage 2: Building city graph...\")\n",
    "    \n",
    "    # Compute data hash to check if cached graph can be used\n",
    "    current_hash = compute_data_hash(data)\n",
    "    if not force_recompute and os.path.exists(GRAPH_NODES_CACHE_PATH) and os.path.exists(GRAPH_DATA_HASH_PATH):\n",
    "        with open(GRAPH_DATA_HASH_PATH, 'r') as f:\n",
    "            cached_hash = f.read()\n",
    "        if cached_hash == current_hash:\n",
    "            try:\n",
    "                nodes_df = cudf.read_parquet(GRAPH_NODES_CACHE_PATH)\n",
    "                edges_df = cudf.read_parquet(GRAPH_EDGES_CACHE_PATH)\n",
    "                with open(GRAPH_NODE_ID_CACHE_PATH, 'r') as f:\n",
    "                    node_id_to_vertex = json.load(f)\n",
    "                G = cugraph.Graph()\n",
    "                G._nodes = nodes_df\n",
    "                if not edges_df.empty:\n",
    "                    G.from_cudf_edgelist(edges_df, source='src', destination='dst')\n",
    "                logging.info(f\"Loaded cached graph: {len(nodes_df)} nodes, {len(edges_df)} edges\")\n",
    "                return G\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Failed to load cached graph: {e}. Recomputing graph...\")\n",
    "    \n",
    "    # Initialize node DataFrame\n",
    "    nodes = []\n",
    "    vertex_to_index = {}\n",
    "    node_id_to_vertex = {}\n",
    "    current_index = 0\n",
    "    \n",
    "    # Add neighborhood nodes\n",
    "    logging.info(\"Adding neighborhood nodes...\")\n",
    "    neighborhoods_gdf = data['neighborhoods']\n",
    "    for i, row in tqdm(neighborhoods_gdf.iterrows(), total=len(neighborhoods_gdf), desc=\"Neighborhood nodes\"):\n",
    "        node_id = f\"neighborhood_{i}\"\n",
    "        vertex_to_index[node_id] = current_index\n",
    "        node_id_to_vertex[str(i)] = node_id\n",
    "        nodes.append({\n",
    "            'index': current_index,\n",
    "            'type': 'neighborhood',\n",
    "            'node_id': node_id,\n",
    "            'LIE_NAME': row['LIE_NAME'],\n",
    "            'avg_road_accident_density': row.get('avg_road_accident_density', 0),\n",
    "            'pedestrian_road_density': row.get('pedestrian_road_density', 0),\n",
    "            'ndvi': row.get('ndvi', 0),\n",
    "            'tree_count': row.get('tree_count', 0),\n",
    "            'transit_count': row.get('transit_count', 0),\n",
    "            'accident_count': row.get('accident_count', 0),\n",
    "            'road_density': row.get('road_density', 0),\n",
    "            'intersection_density': row.get('intersection_density', 0),\n",
    "            'total_population': row.get('total_population', 0),\n",
    "            'elderly_percentage': row.get('elderly_percentage', 0),\n",
    "            'min_x': float(row.geometry.bounds[0]),\n",
    "            'min_y': float(row.geometry.bounds[1]),\n",
    "            'max_x': float(row.geometry.bounds[2]),\n",
    "            'max_y': float(row.geometry.bounds[3])\n",
    "        })\n",
    "        for cat in CATEGORY_PRIORITY.keys():\n",
    "            col = f'land_use_{cat.lower()}_percent'\n",
    "            nodes[-1][col] = row.get(col, 0)\n",
    "        current_index += 1\n",
    "    \n",
    "    # Add building nodes\n",
    "    logging.info(\"Adding building nodes...\")\n",
    "    buildings_gdf = data['buildings']\n",
    "    if 'area_m2' not in buildings_gdf.columns:\n",
    "        logging.warning(\"'area_m2' missing. Computing from geometry...\")\n",
    "        buildings_gdf['area_m2'] = buildings_gdf.geometry.area\n",
    "    else:\n",
    "        logging.info(\"Using existing 'area_m2' column.\")\n",
    "    \n",
    "    for i, row in tqdm(buildings_gdf.iterrows(), total=len(buildings_gdf), desc=\"Building nodes\"):\n",
    "        node_id = f\"building_{i}\"\n",
    "        vertex_to_index[node_id] = current_index\n",
    "        node_id_to_vertex[str(i)] = node_id\n",
    "        nodes.append({\n",
    "            'index': current_index,\n",
    "            'type': 'building',\n",
    "            'node_id': node_id,\n",
    "            'building': row.get('building', 'unknown'),\n",
    "            'area_m2': row.get('area_m2', 0),\n",
    "            'min_x': float(row.geometry.bounds[0]),\n",
    "            'min_y': float(row.geometry.bounds[1]),\n",
    "            'max_x': float(row.geometry.bounds[2]),\n",
    "            'max_y': float(row.geometry.bounds[3])\n",
    "        })\n",
    "        current_index += 1\n",
    "    \n",
    "    # Add road nodes\n",
    "    logging.info(\"Adding road nodes...\")\n",
    "    roads_gdf = data['roads']\n",
    "    for i, row in tqdm(roads_gdf.iterrows(), total=len(roads_gdf), desc=\"Road nodes\"):\n",
    "        node_id = f\"road_{i}\"\n",
    "        vertex_to_index[node_id] = current_index\n",
    "        node_id_to_vertex[str(i)] = node_id\n",
    "        nodes.append({\n",
    "            'index': current_index,\n",
    "            'type': 'road',\n",
    "            'node_id': node_id,\n",
    "            'class': row.get('class', 'unknown'),\n",
    "            'length_m': row.get('length_m', 0),\n",
    "            'min_x': float(row.geometry.bounds[0]),\n",
    "            'min_y': float(row.geometry.bounds[1]),\n",
    "            'max_x': float(row.geometry.bounds[2]),\n",
    "            'max_y': float(row.geometry.bounds[3])\n",
    "        })\n",
    "        current_index += 1\n",
    "    \n",
    "    nodes_df = cudf.DataFrame(nodes)\n",
    "    \n",
    "    # Convert GeoDataFrames to cudf for GPU processing\n",
    "    logging.info(\"Converting GeoDataFrames to cudf for GPU processing...\")\n",
    "    neighborhoods_cudf = cudf.DataFrame.from_pandas(neighborhoods_gdf.drop(columns=['geometry']))\n",
    "    buildings_cudf = cudf.DataFrame.from_pandas(buildings_gdf.drop(columns=['geometry']))\n",
    "    roads_cudf = cudf.DataFrame.from_pandas(roads_gdf.drop(columns=['geometry']))\n",
    "    \n",
    "    # Extract bounding box coordinates\n",
    "    logging.info(\"Extracting bounding box coordinates...\")\n",
    "    neighborhoods_cudf['min_x'] = cudf.Series([float(g.bounds[0]) for g in neighborhoods_gdf.geometry])\n",
    "    neighborhoods_cudf['min_y'] = cudf.Series([float(g.bounds[1]) for g in neighborhoods_gdf.geometry])\n",
    "    neighborhoods_cudf['max_x'] = cudf.Series([float(g.bounds[2]) for g in neighborhoods_gdf.geometry])\n",
    "    neighborhoods_cudf['max_y'] = cudf.Series([float(g.bounds[3]) for g in neighborhoods_gdf.geometry])\n",
    "    \n",
    "    buildings_cudf['min_x'] = cudf.Series([float(g.bounds[0]) for g in buildings_gdf.geometry])\n",
    "    buildings_cudf['min_y'] = cudf.Series([float(g.bounds[1]) for g in buildings_gdf.geometry])\n",
    "    buildings_cudf['max_x'] = cudf.Series([float(g.bounds[2]) for g in buildings_gdf.geometry])\n",
    "    buildings_cudf['max_y'] = cudf.Series([float(g.bounds[3]) for g in buildings_gdf.geometry])\n",
    "    \n",
    "    roads_cudf['min_x'] = cudf.Series([float(g.bounds[0]) for g in roads_gdf.geometry])\n",
    "    roads_cudf['min_y'] = cudf.Series([float(g.bounds[1]) for g in roads_gdf.geometry])\n",
    "    roads_cudf['max_x'] = cudf.Series([float(g.bounds[2]) for g in roads_gdf.geometry])\n",
    "    roads_cudf['max_y'] = cudf.Series([float(g.bounds[3]) for g in roads_gdf.geometry])\n",
    "    \n",
    "    logging.info(f\"neighborhoods_cudf['min_x'] dtype: {neighborhoods_cudf['min_x'].dtype}\")\n",
    "    logging.info(f\"buildings_cudf['min_x'] dtype: {buildings_cudf['min_x'].dtype}\")\n",
    "    logging.info(f\"roads_cudf['min_x'] dtype: {roads_cudf['min_x'].dtype}\")\n",
    "    \n",
    "    # Create edges using GPU-accelerated spatial joins\n",
    "    logging.info(\"Creating edges using GPU-accelerated spatial joins...\")\n",
    "    edges = []\n",
    "    edge_counts = {'neighborhood-neighborhood': 0, 'neighborhood-building': 0, 'neighborhood-road': 0}\n",
    "    \n",
    "    # Neighborhood-Neighborhood edges\n",
    "    logging.info(\"Computing neighborhood-neighborhood edges...\")\n",
    "    for i in tqdm(range(len(neighborhoods_gdf)), desc=\"Neighborhood-Neighborhood edges\"):\n",
    "        row = neighborhoods_cudf.iloc[[i]]\n",
    "        geom_min_x = float(row['min_x'].values[0])\n",
    "        geom_min_y = float(row['min_y'].values[0])\n",
    "        geom_max_x = float(row['max_x'].values[0])\n",
    "        geom_max_y = float(row['max_y'].values[0])\n",
    "        matches = neighborhoods_cudf[\n",
    "            ~((geom_max_x < neighborhoods_cudf['min_x']) |\n",
    "              (geom_min_x > neighborhoods_cudf['max_x']) |\n",
    "              (geom_max_y < neighborhoods_cudf['min_y']) |\n",
    "              (geom_min_y > neighborhoods_cudf['max_y']))\n",
    "        ]\n",
    "        for j in matches.index.values_host:\n",
    "            if j != i:\n",
    "                src_vertex = f\"neighborhood_{i}\"\n",
    "                dst_vertex = f\"neighborhood_{j}\"\n",
    "                src = vertex_to_index[src_vertex]\n",
    "                dst = vertex_to_index[dst_vertex]\n",
    "                edges.append({'src': src, 'dst': dst})\n",
    "                edge_counts['neighborhood-neighborhood'] += 1\n",
    "    \n",
    "    # Neighborhood-Building edges\n",
    "    logging.info(\"Computing neighborhood-building edges...\")\n",
    "    for i in tqdm(range(len(neighborhoods_gdf)), desc=\"Neighborhood-Building edges\"):\n",
    "        row = neighborhoods_cudf.iloc[[i]]\n",
    "        geom_min_x = float(row['min_x'].values[0])\n",
    "        geom_min_y = float(row['min_y'].values[0])\n",
    "        geom_max_x = float(row['max_x'].values[0])\n",
    "        geom_max_y = float(row['max_y'].values[0])\n",
    "        matches = buildings_cudf[\n",
    "            ~((geom_max_x < buildings_cudf['min_x']) |\n",
    "              (geom_min_x > buildings_cudf['max_x']) |\n",
    "              (geom_max_y < buildings_cudf['min_y']) |\n",
    "              (geom_min_y > buildings_cudf['max_y']))\n",
    "        ]\n",
    "        for j in matches.index.values_host:\n",
    "            src_vertex = f\"neighborhood_{i}\"\n",
    "            dst_vertex = f\"building_{j}\"\n",
    "            src = vertex_to_index[src_vertex]\n",
    "            dst = vertex_to_index[dst_vertex]\n",
    "            edges.append({'src': src, 'dst': dst})\n",
    "            edge_counts['neighborhood-building'] += 1\n",
    "    \n",
    "    # Neighborhood-Road edges\n",
    "    logging.info(\"Computing neighborhood-road edges...\")\n",
    "    for i in tqdm(range(len(neighborhoods_gdf)), desc=\"Neighborhood-Road edges\"):\n",
    "        row = neighborhoods_cudf.iloc[[i]]\n",
    "        geom_min_x = float(row['min_x'].values[0])\n",
    "        geom_min_y = float(row['min_y'].values[0])\n",
    "        geom_max_x = float(row['max_x'].values[0])\n",
    "        geom_max_y = float(row['max_y'].values[0])\n",
    "        matches = roads_cudf[\n",
    "            ~((geom_max_x < roads_cudf['min_x']) |\n",
    "              (geom_min_x > roads_cudf['max_x']) |\n",
    "              (geom_max_y < roads_cudf['min_y']) |\n",
    "              (geom_min_y > roads_cudf['max_y']))\n",
    "        ]\n",
    "        for j in matches.index.values_host:\n",
    "            src_vertex = f\"neighborhood_{i}\"\n",
    "            dst_vertex = f\"road_{j}\"\n",
    "            src = vertex_to_index[src_vertex]\n",
    "            dst = vertex_to_index[dst_vertex]\n",
    "            edges.append({'src': src, 'dst': dst})\n",
    "            edge_counts['neighborhood-road'] += 1\n",
    "    \n",
    "    logging.info(f\"Edge counts by type: {edge_counts}\")\n",
    "    \n",
    "    edges_df = cudf.DataFrame(edges)\n",
    "    logging.info(f\"Created {len(edges_df)} total edges\")\n",
    "    \n",
    "    # Validate edges\n",
    "    valid_indices = set(nodes_df['index'].to_pandas())\n",
    "    if edges_df.empty:\n",
    "        logging.warning(\"No edges created. Graph will have nodes but no edges.\")\n",
    "    else:\n",
    "        edges_df = edges_df[edges_df['src'].isin(valid_indices) & edges_df['dst'].isin(valid_indices)]\n",
    "        logging.info(f\"After validation, {len(edges_df)} edges remain\")\n",
    "        if not edges_df.empty:\n",
    "            logging.info(f\"Sample edges after validation:\\n{edges_df.head().to_pandas()}\")\n",
    "    \n",
    "    # Create graph\n",
    "    G = cugraph.Graph()\n",
    "    G._nodes = nodes_df\n",
    "    if not edges_df.empty:\n",
    "        G.from_cudf_edgelist(edges_df, source='src', destination='dst')\n",
    "    else:\n",
    "        logging.warning(\"No valid edges created. Graph will have nodes but no edges.\")\n",
    "    \n",
    "    # Save graph data to cache\n",
    "    logging.info(\"Saving graph data to cache...\")\n",
    "    try:\n",
    "        nodes_df.to_parquet(GRAPH_NODES_CACHE_PATH)\n",
    "        edges_df.to_parquet(GRAPH_EDGES_CACHE_PATH)\n",
    "        with open(GRAPH_DATA_HASH_PATH, 'w') as f:\n",
    "            f.write(current_hash)\n",
    "        with open(GRAPH_NODE_ID_CACHE_PATH, 'w') as f:\n",
    "            json.dump(node_id_to_vertex, f)\n",
    "        logging.info(\"Successfully saved graph data to cache.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save graph data to cache: {e}\")\n",
    "    \n",
    "    logging.info(f\"City graph constructed: {len(nodes_df)} nodes, {len(edges_df)} edges\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14e080",
   "metadata": {},
   "source": [
    "Cell 7: Rule-Based Walkability Scores (compute_walkability_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ef4aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_walkability_scores(G, data):\n",
    "    \"\"\"\n",
    "    Compute walkability scores for neighborhoods and update the graph.\n",
    "    \"\"\"\n",
    "    logging.info(\"Computing walkability scores for neighborhoods...\")\n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    \n",
    "    # Compute walkability components\n",
    "    walkability_components, components = compute_walkability_components_all(data['neighborhoods'], data)\n",
    "    \n",
    "    # Unpack the tuple\n",
    "    walkability_df = walkability_components\n",
    "    \n",
    "    # Validate data\n",
    "    logging.info(f\"Number of neighborhood nodes in nodes_df: {len(nodes_df[nodes_df['type'] == 'neighborhood'])}\")\n",
    "    logging.info(f\"Number of entries in walkability_components: {len(walkability_df)}\")\n",
    "    logging.info(f\"Sample LIE_NAME in nodes_df: {nodes_df[nodes_df['type'] == 'neighborhood']['LIE_NAME'].head().tolist()}\")\n",
    "    logging.info(f\"Sample LIE_NAME in walkability_components: {walkability_df['LIE_NAME'].head().tolist()}\")\n",
    "\n",
    "    # Merge walkability scores into nodes_df\n",
    "    nodes_df = nodes_df.merge(\n",
    "        walkability_df[['LIE_NAME', 'walkability_score', 'walkability_category']],\n",
    "        on='LIE_NAME',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Check for unmatched neighborhoods\n",
    "    unmatched = nodes_df[(nodes_df['type'] == 'neighborhood') & (nodes_df['walkability_score'].isna())]\n",
    "    if not unmatched.empty:\n",
    "        logging.warning(f\"Found {len(unmatched)} neighborhood nodes without walkability scores:\")\n",
    "        logging.warning(unmatched[['LIE_NAME']])\n",
    "        # Optionally fill missing scores\n",
    "        nodes_df['walkability_score'] = nodes_df['walkability_score'].fillna(0.5)\n",
    "        nodes_df['walkability_category'] = nodes_df['walkability_category'].fillna('medium')\n",
    "    else:\n",
    "        logging.info(\"All neighborhood nodes matched with walkability scores.\")\n",
    "\n",
    "    # Update the graph nodes\n",
    "    G._nodes = cudf.from_pandas(nodes_df)\n",
    "    \n",
    "    logging.info(\"Finished computing walkability scores.\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989dbf84",
   "metadata": {},
   "source": [
    "Cell 8 prepare_gnn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a057108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_gnn_data(G):\n",
    "    import torch\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    logging.info(\"Preparing data for GNN training...\")\n",
    "    nodes_df = G._nodes\n",
    "    edges_df = G.edgelist.edgelist_df if G.edgelist else cudf.DataFrame()\n",
    "    \n",
    "    numerical_features = [\n",
    "        'ndvi', 'tree_count', 'transit_count', 'accident_count',\n",
    "        'road_density', 'intersection_density', 'total_population',\n",
    "        'elderly_percentage', 'area_m2', 'length_m', 'avg_road_accident_density',\n",
    "        'pedestrian_road_density'\n",
    "    ] + [f'land_use_{cat.lower()}_percent' for cat in CATEGORY_PRIORITY.keys()]\n",
    "    \n",
    "    numerical_features.append('land_use_diversity')\n",
    "    \n",
    "    building_types = nodes_df[nodes_df['type'] == 'building']['building'].to_pandas().unique()\n",
    "    road_classes = nodes_df[nodes_df['type'] == 'road']['class'].to_pandas().unique()\n",
    "    categorical_features = (\n",
    "        [f'building_{bt}' for bt in building_types if pd.notna(bt)] +\n",
    "        [f'road_class_{rc}' for rc in road_classes if pd.notna(rc)]\n",
    "    )\n",
    "    \n",
    "    all_features = numerical_features + categorical_features\n",
    "    \n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    node_types = []\n",
    "    \n",
    "    for node_type in tqdm(['neighborhood', 'building', 'road'], desc=\"Normalizing features by node type\"):\n",
    "        subset = nodes_df[nodes_df['type'] == node_type].to_pandas()\n",
    "        if subset.empty:\n",
    "            logging.warning(f\"No nodes of type {node_type} found. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        subset_features = pd.DataFrame(0.0, index=subset.index, columns=all_features)\n",
    "        \n",
    "        if node_type == 'neighborhood':\n",
    "            # Compute land_use_diversity\n",
    "            land_use_cols = [col for col in subset.columns if col.startswith('land_use_') and col.endswith('_percent')]\n",
    "            if land_use_cols:\n",
    "                subset['land_use_diversity'] = subset[land_use_cols].apply(\n",
    "                    lambda row: -np.sum([p * np.log(p + 1e-10) for p in row / 100.0 if p > 0]), axis=1\n",
    "                )\n",
    "            else:\n",
    "                subset['land_use_diversity'] = 0\n",
    "            \n",
    "            for col in numerical_features:\n",
    "                if col in subset.columns:\n",
    "                    subset_features[col] = subset[col].astype(float).fillna(0)\n",
    "                    logging.debug(f\"{node_type} - {col} pre-normalization std: {subset[col].std():.4f}\")\n",
    "                else:\n",
    "                    logging.warning(f\"Column {col} missing in neighborhood nodes. Setting to 0.\")\n",
    "                    subset_features[col] = 0\n",
    "        elif node_type == 'building':\n",
    "            if 'area_m2' in subset.columns:\n",
    "                subset_features['area_m2'] = subset['area_m2'].astype(float).fillna(0)\n",
    "                logging.debug(f\"{node_type} - area_m2 pre-normalization std: {subset['area_m2'].std():.4f}\")\n",
    "        else:  # road\n",
    "            if 'length_m' in subset.columns:\n",
    "                subset_features['length_m'] = subset['length_m'].astype(float).fillna(0)\n",
    "                logging.debug(f\"{node_type} - length_m pre-normalization std: {subset['length_m'].std():.4f}\")\n",
    "        \n",
    "        if node_type == 'building':\n",
    "            for bt in building_types:\n",
    "                if pd.notna(bt):\n",
    "                    subset_features[f'building_{bt}'] = (subset['building'] == bt).astype(float)\n",
    "        elif node_type == 'road':\n",
    "            for rc in road_classes:\n",
    "                if pd.notna(rc):\n",
    "                    subset_features[f'road_class_{rc}'] = (subset['class'] == rc).astype(float)\n",
    "        \n",
    "        # Z-score normalization for numerical features\n",
    "        for col in numerical_features:\n",
    "            if col in subset_features.columns and subset_features[col].std() > 0:\n",
    "                subset_features[col] = (\n",
    "                    (subset_features[col] - subset_features[col].mean()) / subset_features[col].std()\n",
    "                ).fillna(0)\n",
    "                logging.debug(f\"{node_type} - {col} post-normalization std: {subset_features[col].std():.4f}\")\n",
    "            else:\n",
    "                logging.debug(f\"Column {col} has zero variance or is missing for {node_type}. Setting to 0.\")\n",
    "        \n",
    "        logging.info(f\"Node type {node_type}: {len(subset)} nodes, feature shape: {subset_features.shape}\")\n",
    "        \n",
    "        features_list.append(subset_features.values)\n",
    "        \n",
    "        if node_type == 'neighborhood':\n",
    "            labels = subset['walkability_score'].astype(float).fillna(0).values\n",
    "            labels_list.append(labels[:, None])  # Shape [n, 1]\n",
    "        else:\n",
    "            labels_list.append(np.zeros((len(subset), 1)))\n",
    "        \n",
    "        node_types.extend([node_type] * len(subset))\n",
    "    \n",
    "    try:\n",
    "        features = np.vstack(features_list)\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Failed to stack features: {e}\")\n",
    "        raise\n",
    "    \n",
    "    labels = np.vstack(labels_list)\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "    if not edges_df.empty:\n",
    "        edge_index = torch.tensor(edges_df[['src', 'dst']].to_pandas().values.T, dtype=torch.long)\n",
    "        logging.info(f\"Edge index created with {edge_index.shape[1]} edges\")\n",
    "        max_index = nodes_df['index'].max()\n",
    "        if edge_index.max() > max_index or edge_index.min() < 0:\n",
    "            logging.warning(f\"Edge indices out of bounds: min={edge_index.min()}, max={edge_index.max()}, expected max={max_index}\")\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "    else:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        logging.warning(\"No edges found in graph.\")\n",
    "    \n",
    "    data = Data(\n",
    "        x=features_tensor,\n",
    "        edge_index=edge_index,\n",
    "        y=labels_tensor\n",
    "    )\n",
    "    \n",
    "    data.node_types = node_types\n",
    "    \n",
    "    logging.info(f\"Prepared GNN data: {features_tensor.shape[0]} nodes, {edge_index.shape[1]} edges\")\n",
    "    logging.info(f\"Feature matrix shape: {features_tensor.shape}\")\n",
    "    logging.info(f\"Label tensor shape: {labels_tensor.shape}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a9a6b5",
   "metadata": {},
   "source": [
    "Cell 9: WalkabilityGNN, train_gnn_model, predict_walkability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "635ecc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNWalkabilityPredictor(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=128, num_heads=4, dropout_rate=0.3):\n",
    "        super(GNNWalkabilityPredictor, self).__init__()\n",
    "        self.conv1 = GATConv(num_features, hidden_dim, heads=num_heads, concat=True)\n",
    "        self.bn1 = BatchNorm(hidden_dim * num_heads)\n",
    "        self.conv2 = GATConv(hidden_dim * num_heads, hidden_dim // 2, heads=1, concat=True)\n",
    "        self.bn2 = BatchNorm(hidden_dim // 2)\n",
    "        self.fc1 = torch.nn.Linear(hidden_dim // 2, hidden_dim // 4)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim // 4, 1)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        if edge_index.numel() > 0:\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = self.bn1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            x = self.bn2(x)\n",
    "            x = F.relu(x)\n",
    "        else:\n",
    "            logging.warning(\"No edges in the graph. Using linear layer for node features only.\")\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "def train_gnn_model(data_gnn, hidden_dim=128, num_heads=4, dropout_rate=0.3, lr=0.005, weight_decay=1e-4, epochs=500, patience=30):\n",
    "    logging.info(\"Stage 4: Training GNN model...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_gnn = data_gnn.to(device)\n",
    "    \n",
    "    neighborhood_mask = np.array([t == 'neighborhood' for t in data_gnn.node_types])\n",
    "    train_indices = np.where(neighborhood_mask)[0]\n",
    "    \n",
    "    if len(train_indices) == 0:\n",
    "        logging.error(\"No neighborhood nodes found for training.\")\n",
    "        raise ValueError(\"No neighborhood nodes found for training.\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    train_idx = np.random.choice(train_indices, size=int(0.8 * len(train_indices)), replace=False)\n",
    "    val_idx = np.setdiff1d(train_indices, train_idx)\n",
    "    \n",
    "    train_mask = torch.zeros(data_gnn.num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(data_gnn.num_nodes, dtype=torch.bool)\n",
    "    train_mask[train_idx] = True\n",
    "    val_mask[val_idx] = True\n",
    "    data_gnn.train_mask = train_mask\n",
    "    data_gnn.val_mask = val_mask\n",
    "    \n",
    "    neighborhood_labels = data_gnn.y[neighborhood_mask].cpu().numpy()\n",
    "    logging.info(f\"Target (walkability_score) distribution for neighborhood nodes:\\n{pd.Series(neighborhood_labels.flatten()).describe()}\")\n",
    "    \n",
    "    model = GNNWalkabilityPredictor(num_features=data_gnn.x.shape[1], hidden_dim=hidden_dim, num_heads=num_heads, dropout_rate=dropout_rate).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = os.path.join(CHECKPOINT_DIR, 'best_gnn_model.pth')\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_maes = []\n",
    "    val_maes = []\n",
    "    train_r2s = []\n",
    "    val_r2s = []\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training epochs\"):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data_gnn)\n",
    "        loss = criterion(out[data_gnn.train_mask], data_gnn.y[data_gnn.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_out = model(data_gnn)\n",
    "            val_loss = criterion(val_out[data_gnn.val_mask], data_gnn.y[data_gnn.val_mask])\n",
    "            \n",
    "            train_pred = out[data_gnn.train_mask].detach().cpu().numpy()\n",
    "            train_true = data_gnn.y[data_gnn.train_mask].cpu().numpy()\n",
    "            val_pred = val_out[data_gnn.val_mask].detach().cpu().numpy()\n",
    "            val_true = data_gnn.y[data_gnn.val_mask].cpu().numpy()\n",
    "            \n",
    "            train_mae = mean_absolute_error(train_true, train_pred)\n",
    "            train_r2 = r2_score(train_true, train_pred)\n",
    "            val_mae = mean_absolute_error(val_true, val_pred)\n",
    "            val_r2 = r2_score(val_true, val_pred)\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            val_losses.append(val_loss.item())\n",
    "            train_maes.append(train_mae)\n",
    "            val_maes.append(val_mae)\n",
    "            train_r2s.append(train_r2)\n",
    "            val_r2s.append(val_r2)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            logging.info(f\"Epoch {epoch}, Train Loss: {loss.item():.4f}, Train MAE: {train_mae:.4f}, Train R2: {train_r2:.4f}, Val Loss: {val_loss.item():.4f}, Val MAE: {val_mae:.4f}, Val R2: {val_r2:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                logging.info(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    logging.info(\"Finished training GNN model.\")\n",
    "    return {\n",
    "        'model': model,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_maes': train_maes,\n",
    "        'val_maes': val_maes,\n",
    "        'train_r2s': train_r2s,\n",
    "        'val_r2s': val_r2s\n",
    "    }\n",
    "\n",
    "def predict_walkability(G, model):\n",
    "    logging.info(\"Predicting walkability scores using GNN model...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    data_gnn = prepare_gnn_data(G)\n",
    "    data_gnn = data_gnn.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(data_gnn)\n",
    "    \n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    neighborhood_mask = nodes_df['type'] == 'neighborhood'\n",
    "    nodes_df.loc[neighborhood_mask, 'walkability_gnn'] = predictions[neighborhood_mask].cpu().numpy().flatten()\n",
    "    \n",
    "    nodes_df['walkability_gnn'] = nodes_df['walkability_gnn'].clip(0, 1)\n",
    "    \n",
    "    # Compute walkability_category with dynamic thresholds for GNN predictions\n",
    "    low_threshold = nodes_df.loc[neighborhood_mask, 'walkability_gnn'].quantile(0.33)\n",
    "    high_threshold = nodes_df.loc[neighborhood_mask, 'walkability_gnn'].quantile(0.66)\n",
    "    logging.info(f\"GNN walkability category thresholds - low: {low_threshold:.4f}, high: {high_threshold:.4f}\")\n",
    "    \n",
    "    def categorize_gnn_score(score):\n",
    "        if score < low_threshold:\n",
    "            return 'low'\n",
    "        elif score < high_threshold:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'high'\n",
    "    \n",
    "    nodes_df.loc[neighborhood_mask, 'walkability_category'] = nodes_df.loc[neighborhood_mask, 'walkability_gnn'].apply(categorize_gnn_score)\n",
    "    \n",
    "    G._nodes = cudf.from_pandas(nodes_df)\n",
    "    \n",
    "    logging.info(\"Finished predicting walkability scores.\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5406821f",
   "metadata": {},
   "source": [
    "Cell 10: Interactive Map Generation (create_interactive_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d059c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_map(G, data):\n",
    "    \"\"\"Generate an interactive Kepler.gl map to visualize walkability scores and other geodata.\"\"\"\n",
    "    logging.info(\"Generating interactive Kepler.gl map...\")\n",
    "    \n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    neighborhoods_gdf = data['neighborhoods'].copy()\n",
    "\n",
    "    # Standardize LIE_NAME for merging\n",
    "    nodes_df['LIE_NAME'] = nodes_df['LIE_NAME'].astype(str).str.strip()\n",
    "    neighborhoods_gdf['LIE_NAME'] = neighborhoods_gdf['LIE_NAME'].astype(str).str.strip()\n",
    "\n",
    "    # Filter for neighborhood nodes and select necessary columns\n",
    "    neighborhood_nodes = nodes_df[nodes_df['type'] == 'neighborhood'][['LIE_NAME', 'walkability_score', 'walkability_gnn', 'walkability_category']]\n",
    "\n",
    "    # Merge data\n",
    "    map_data = neighborhoods_gdf[['LIE_NAME', 'geometry']].merge(\n",
    "        neighborhood_nodes,\n",
    "        on='LIE_NAME',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Drop duplicates in-place\n",
    "    map_data.drop_duplicates(subset=['LIE_NAME'], keep='first', inplace=True)\n",
    "\n",
    "    # Fill NaN values\n",
    "    map_data['walkability_score'] = map_data['walkability_score'].fillna(0)\n",
    "    map_data['walkability_gnn'] = map_data['walkability_gnn'].fillna(0)\n",
    "    map_data['walkability_category'] = map_data['walkability_category'].fillna('low')\n",
    "\n",
    "    # Convert to GeoDataFrame and transform CRS\n",
    "    map_data = gpd.GeoDataFrame(map_data, geometry='geometry', crs='EPSG:3826')\n",
    "    map_data['geometry'] = map_data['geometry'].to_crs('EPSG:4326')\n",
    "\n",
    "    # Prepare kepler_data\n",
    "    kepler_data = {\n",
    "        'neighborhoods': map_data[['LIE_NAME', 'walkability_score', 'walkability_gnn', 'walkability_category', 'geometry']].to_json()\n",
    "    }\n",
    "\n",
    "    # Prepare roads data\n",
    "    if 'roads' in data:\n",
    "        roads_gdf = data['roads'].copy()\n",
    "        if roads_gdf.crs != 'EPSG:4326':\n",
    "            roads_gdf = roads_gdf.to_crs('EPSG:4326')\n",
    "        road_columns = ['class', 'length_m', 'geometry']\n",
    "        available_columns = [col for col in road_columns if col in roads_gdf.columns]\n",
    "        if 'geometry' in available_columns:\n",
    "            kepler_data['roads'] = roads_gdf[available_columns].to_json()\n",
    "        else:\n",
    "            logging.warning(\"Roads GeoDataFrame missing 'geometry' column. Skipping roads layer.\")\n",
    "    else:\n",
    "        logging.warning(\"Roads data not found in data dictionary. Skipping roads layer.\")\n",
    "\n",
    "    # Prepare buildings data\n",
    "    if 'buildings' in data:\n",
    "        buildings_gdf = data['buildings'].copy()\n",
    "        if buildings_gdf.crs != 'EPSG:4326':\n",
    "            buildings_gdf = buildings_gdf.to_crs('EPSG:4326')\n",
    "        building_columns = ['building', 'area_m2', 'geometry']\n",
    "        available_columns = [col for col in building_columns if col in buildings_gdf.columns]\n",
    "        if 'geometry' in available_columns:\n",
    "            kepler_data['buildings'] = buildings_gdf[available_columns].to_json()\n",
    "        else:\n",
    "            logging.warning(\"Buildings GeoDataFrame missing 'geometry' column. Skipping buildings layer.\")\n",
    "    else:\n",
    "        logging.warning(\"Buildings data not found in data dictionary. Skipping buildings layer.\")\n",
    "\n",
    "    # Define neighborhoods layer\n",
    "    neighborhoods_layer = {\n",
    "        \"id\": \"neighborhoods\",\n",
    "        \"type\": \"geojson\",\n",
    "        \"config\": {\n",
    "            \"dataId\": \"neighborhoods\",\n",
    "            \"label\": \"Neighborhoods\",\n",
    "            \"color\": [18, 147, 154],\n",
    "            \"columns\": {\n",
    "                \"geojson\": \"geometry\"\n",
    "            },\n",
    "            \"isVisible\": True,\n",
    "            \"visConfig\": {\n",
    "                \"opacity\": 0.7,\n",
    "                \"strokeOpacity\": 0.9,\n",
    "                \"thickness\": 1,\n",
    "                \"strokeColor\": [255, 255, 255],\n",
    "                \"colorRange\": {\n",
    "                    \"name\": \"Global Warming\",\n",
    "                    \"type\": \"sequential\",\n",
    "                    \"colors\": [\n",
    "                        \"#5A1846\", \"#900C3F\", \"#C70039\",\n",
    "                        \"#E3611C\", \"#F1920E\", \"#FFC107\"\n",
    "                    ]\n",
    "                },\n",
    "                \"strokeColorRange\": {\n",
    "                    \"name\": \"Global Warming\",\n",
    "                    \"type\": \"sequential\",\n",
    "                    \"colors\": [\n",
    "                        \"#5A1846\", \"#900C3F\", \"#C70039\",\n",
    "                        \"#E3611C\", \"#F1920E\", \"#FFC107\"\n",
    "                    ]\n",
    "                },\n",
    "                \"colorField\": {\n",
    "                    \"name\": \"walkability_gnn\",\n",
    "                    \"type\": \"real\"\n",
    "                },\n",
    "                \"colorScale\": \"quantile\"\n",
    "            }\n",
    "        },\n",
    "        \"visualChannels\": {\n",
    "            \"colorField\": {\n",
    "                \"name\": \"walkability_gnn\",\n",
    "                \"type\": \"real\"\n",
    "            },\n",
    "            \"colorScale\": \"quantile\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Define roads layer if available\n",
    "    if 'roads' in kepler_data:\n",
    "        roads_layer = {\n",
    "            \"id\": \"roads\",\n",
    "            \"type\": \"geojson\",\n",
    "            \"config\": {\n",
    "                \"dataId\": \"roads\",\n",
    "                \"label\": \"Roads\",\n",
    "                \"color\": [255, 0, 0],\n",
    "                \"columns\": {\n",
    "                    \"geojson\": \"geometry\"\n",
    "                },\n",
    "                \"isVisible\": True,\n",
    "                \"visConfig\": {\n",
    "                    \"opacity\": 0.8,\n",
    "                    \"strokeOpacity\": 0.8,\n",
    "                    \"thickness\": 2,\n",
    "                    \"strokeColor\": [255, 0, 0],\n",
    "                    \"colorField\": {\n",
    "                        \"name\": \"class\",\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"colorScale\": \"ordinal\",\n",
    "                    \"colorRange\": {\n",
    "                        \"name\": \"ColorBrewer Paired-12\",\n",
    "                        \"type\": \"all\",\n",
    "                        \"category\": \"ColorBrewer\",\n",
    "                        \"colors\": [\"#a6cee3\", \"#1f78b4\", \"#b2df8a\", \"#33a02c\", \"#fb9a99\", \"#e31a1c\", \"#fdbf6f\", \"#ff7f00\", \"#cab2d6\", \"#6a3d9a\", \"#ffff99\", \"#b15928\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"visualChannels\": {\n",
    "                \"colorField\": {\n",
    "                    \"name\": \"class\",\n",
    "                    \"type\": \"string\"\n",
    "                },\n",
    "                \"colorScale\": \"ordinal\"\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        roads_layer = None\n",
    "\n",
    "    # Define buildings layer if available\n",
    "    if 'buildings' in kepler_data:\n",
    "        buildings_layer = {\n",
    "            \"id\": \"buildings\",\n",
    "            \"type\": \"geojson\",\n",
    "            \"config\": {\n",
    "                \"dataId\": \"buildings\",\n",
    "                \"label\": \"Buildings\",\n",
    "                \"color\": [0, 255, 0],\n",
    "                \"columns\": {\n",
    "                    \"geojson\": \"geometry\"\n",
    "                },\n",
    "                \"isVisible\": True,\n",
    "                \"visConfig\": {\n",
    "                    \"opacity\": 0.5,\n",
    "                    \"strokeOpacity\": 0.5,\n",
    "                    \"thickness\": 0.5,\n",
    "                    \"strokeColor\": [0, 0, 0],\n",
    "                    \"colorField\": {\n",
    "                        \"name\": \"building\",\n",
    "                        \"type\": \"string\"\n",
    "                    },\n",
    "                    \"colorScale\": \"ordinal\",\n",
    "                    \"colorRange\": {\n",
    "                        \"name\": \"ColorBrewer Set3-12\",\n",
    "                        \"type\": \"all\",\n",
    "                        \"category\": \"ColorBrewer\",\n",
    "                        \"colors\": [\"#8dd3c7\", \"#ffffb3\", \"#bebada\", \"#fb8072\", \"#80b1d3\", \"#fdb462\", \"#b3de69\", \"#fccde5\", \"#d9d9d9\", \"#bc80bd\", \"#ccebc5\", \"#ffed6f\"]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"visualChannels\": {\n",
    "                \"colorField\": {\n",
    "                    \"name\": \"building\",\n",
    "                    \"type\": \"string\"\n",
    "                },\n",
    "                \"colorScale\": \"ordinal\"\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        buildings_layer = None\n",
    "\n",
    "    # Create layers list\n",
    "    layers = [neighborhoods_layer]\n",
    "    if roads_layer:\n",
    "        layers.append(roads_layer)\n",
    "    if buildings_layer:\n",
    "        layers.append(buildings_layer)\n",
    "\n",
    "    # Define tooltips\n",
    "    tooltips = {\n",
    "        \"neighborhoods\": [\n",
    "            {\"name\": \"LIE_NAME\", \"format\": None},\n",
    "            {\"name\": \"walkability_score\", \"format\": \"{:.3f}\"},\n",
    "            {\"name\": \"walkability_gnn\", \"format\": \"{:.3f}\"},\n",
    "            {\"name\": \"walkability_category\", \"format\": None}\n",
    "        ]\n",
    "    }\n",
    "    if 'roads' in kepler_data:\n",
    "        tooltips['roads'] = [\n",
    "            {\"name\": \"class\", \"format\": None},\n",
    "            {\"name\": \"length_m\", \"format\": \"{:.2f}\"}\n",
    "        ]\n",
    "    if 'buildings' in kepler_data:\n",
    "        tooltips['buildings'] = [\n",
    "            {\"name\": \"building\", \"format\": None},\n",
    "            {\"name\": \"area_m2\", \"format\": \"{:.2f}\"}\n",
    "        ]\n",
    "\n",
    "    # Update config\n",
    "    config = {\n",
    "        \"version\": \"v1\",\n",
    "        \"config\": {\n",
    "            \"visState\": {\n",
    "                \"layers\": layers,\n",
    "                \"interactionConfig\": {\n",
    "                    \"tooltip\": {\n",
    "                        \"fieldsToShow\": tooltips,\n",
    "                        \"enabled\": True\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"mapState\": {\n",
    "                \"latitude\": 25.0330,\n",
    "                \"longitude\": 121.5654,\n",
    "                \"zoom\": 11\n",
    "            },\n",
    "            \"mapStyle\": {\n",
    "                \"styleType\": \"dark\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    map_1 = KeplerGl(height=800, data=kepler_data, config=config)\n",
    "    map_path = os.path.join(BASE_DIR, 'taipei_walkability_map.html')\n",
    "    map_1.save_to_html(file_name=map_path)\n",
    "    logging.info(f\"Interactive map generated and saved as {map_path}\")\n",
    "    print(f\"Map saved to {map_path}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0683d",
   "metadata": {},
   "source": [
    "Cell 11: Main Execution (main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a36c3936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and prepare data for analysis.\"\"\"\n",
    "    logging.info(\"Starting load_and_prepare_data...\")\n",
    "    data = {}\n",
    "    \n",
    "    try:\n",
    "        # Load neighborhoods\n",
    "        logging.info(f\"Loading neighborhoods from {LANDUSE_NDVI_PATH}\")\n",
    "        data['neighborhoods'] = gpd.read_file(LANDUSE_NDVI_PATH)\n",
    "        logging.info(f\"Neighborhoods loaded: shape={data['neighborhoods'].shape}, columns={list(data['neighborhoods'].columns)}\")\n",
    "        \n",
    "        # Compute area_km2 if missing\n",
    "        if 'area_km2' not in data['neighborhoods'].columns:\n",
    "            logging.warning(\"'area_km2' column missing in neighborhoods. Computing from geometry...\")\n",
    "            if data['neighborhoods'].crs is None:\n",
    "                logging.info(\"No CRS defined for neighborhoods. Assuming EPSG:3826.\")\n",
    "                data['neighborhoods'].set_crs('EPSG:3826', inplace=True)\n",
    "            data['neighborhoods']['area_km2'] = data['neighborhoods'].geometry.area / 1e6  # Convert m² to km²\n",
    "            logging.info(f\"Computed area_km2 stats:\\n{data['neighborhoods']['area_km2'].describe()}\")\n",
    "        \n",
    "        # Load roads\n",
    "        logging.info(f\"Loading roads from {OSM_ROADS_PATH}\")\n",
    "        data['roads'] = gpd.read_parquet(OSM_ROADS_PATH)\n",
    "        logging.info(f\"Roads loaded: shape={data['roads'].shape}, columns={list(data['roads'].columns)}\")\n",
    "        \n",
    "        # Compute length_m if missing\n",
    "        if 'length_m' not in data['roads'].columns:\n",
    "            logging.warning(\"'length_m' column missing in roads. Computing from geometry...\")\n",
    "            if data['roads'].crs is None:\n",
    "                logging.info(\"No CRS defined for roads. Assuming EPSG:3826.\")\n",
    "                data['roads'].set_crs('EPSG:3826', inplace=True)\n",
    "            data['roads']['length_m'] = data['roads'].geometry.length\n",
    "            logging.info(f\"Computed length_m stats:\\n{data['roads']['length_m'].describe()}\")\n",
    "        \n",
    "        # Load accidents\n",
    "        logging.info(f\"Loading accidents from {ACCIDENTS_PATH}\")\n",
    "        data['accidents'] = gpd.read_file(ACCIDENTS_PATH)\n",
    "        logging.info(f\"Accidents loaded: shape={data['accidents'].shape}, columns={list(data['accidents'].columns)}\")\n",
    "        \n",
    "        # Load urban masterplan\n",
    "        logging.info(f\"Loading urban masterplan from {URBAN_MASTERPLAN_PATH}\")\n",
    "        data['urban_masterplan'] = gpd.read_file(URBAN_MASTERPLAN_PATH)\n",
    "        logging.info(f\"Urban masterplan loaded: shape={data['urban_masterplan'].shape}, columns={list(data['urban_masterplan'].columns)}\")\n",
    "        if 'Category' not in data['urban_masterplan'].columns:\n",
    "            logging.warning(\"'Category' column missing in urban_masterplan. Adding default category.\")\n",
    "            data['urban_masterplan']['Category'] = 'Unknown'\n",
    "        \n",
    "        # Load transit\n",
    "        logging.info(f\"Loading transit from {OSM_TRANSIT_PATH}\")\n",
    "        data['transit'] = gpd.read_parquet(OSM_TRANSIT_PATH)\n",
    "        logging.info(f\"Transit loaded: shape={data['transit'].shape}, columns={list(data['transit'].columns)}\")\n",
    "        \n",
    "        # Load trees\n",
    "        logging.info(f\"Loading trees from {OSM_TREES_PATH}\")\n",
    "        data['trees'] = gpd.read_parquet(OSM_TREES_PATH)\n",
    "        logging.info(f\"Trees loaded: shape={data['trees'].shape}, columns={list(data['trees'].columns)}\")\n",
    "        \n",
    "        # Load population\n",
    "        logging.info(f\"Loading population from {POPULATION_PATH}\")\n",
    "        with open(POPULATION_PATH, 'r') as f:\n",
    "            pop_data = json.load(f)\n",
    "        data['population'] = pd.DataFrame(pop_data)\n",
    "        logging.info(f\"Population loaded: shape={data['population'].shape}, columns={list(data['population'].columns)}\")\n",
    "        \n",
    "        # Validate data\n",
    "        for key, df in data.items():\n",
    "            if isinstance(df, gpd.GeoDataFrame):\n",
    "                if df.empty:\n",
    "                    logging.error(f\"{key} is empty.\")\n",
    "                    raise ValueError(f\"{key} is empty.\")\n",
    "                if df.geometry.isna().any():\n",
    "                    logging.error(f\"Missing geometries in {key}.\")\n",
    "                    raise ValueError(f\"Missing geometries in {key}.\")\n",
    "                if not all(df.geometry.is_valid):\n",
    "                    logging.warning(f\"Invalid geometries in {key}. Attempting to fix.\")\n",
    "                    df['geometry'] = df['geometry'].apply(make_valid)\n",
    "        \n",
    "        # Add category_priority and land_use_weights to data\n",
    "        data['category_priority'] = CATEGORY_PRIORITY\n",
    "        data['land_use_weights'] = land_use_weights\n",
    "        \n",
    "        logging.info(\"load_and_prepare_data completed successfully.\")\n",
    "        return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in load_and_prepare_data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f13b11d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 17:04:14,765 - INFO - Ensured subgraph directory exists: /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/subgraphs\n",
      "2025-05-05 17:04:14,765 - INFO - Starting load_and_prepare_data...\n",
      "2025-05-05 17:04:14,766 - INFO - Loading neighborhoods from /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/neighborhoods_with_ndvi_numerical_corrected.geojson\n",
      "2025-05-05 17:04:14,903 - INFO - Neighborhoods loaded: shape=(456, 57), columns=['LIE_NAME', 'SECT_NAME', '2024population', 'land_use_city_open_area_count', 'land_use_city_open_area_area_m2', 'land_use_city_open_area_percent', 'land_use_commercial_count', 'land_use_commercial_area_m2', 'land_use_commercial_percent', 'land_use_infrastructure_count', 'land_use_infrastructure_area_m2', 'land_use_infrastructure_percent', 'land_use_government_count', 'land_use_government_area_m2', 'land_use_government_percent', 'land_use_public_transportation_count', 'land_use_public_transportation_area_m2', 'land_use_public_transportation_percent', 'land_use_education_count', 'land_use_education_area_m2', 'land_use_education_percent', 'land_use_medical_count', 'land_use_medical_area_m2', 'land_use_medical_percent', 'land_use_amenity_count', 'land_use_amenity_area_m2', 'land_use_amenity_percent', 'land_use_road_count', 'land_use_road_area_m2', 'land_use_road_percent', 'land_use_pedestrian_count', 'land_use_pedestrian_area_m2', 'land_use_pedestrian_percent', 'land_use_natural_count', 'land_use_natural_area_m2', 'land_use_natural_percent', 'land_use_special_zone_count', 'land_use_special_zone_area_m2', 'land_use_special_zone_percent', 'land_use_river_count', 'land_use_river_area_m2', 'land_use_river_percent', 'land_use_military_count', 'land_use_military_area_m2', 'land_use_military_percent', 'land_use_residential_count', 'land_use_residential_area_m2', 'land_use_residential_percent', 'land_use_industrial_count', 'land_use_industrial_area_m2', 'land_use_industrial_percent', 'land_use_agriculture_count', 'land_use_agriculture_area_m2', 'land_use_agriculture_percent', 'ndvi_mean', 'ndvi_median', 'geometry']\n",
      "2025-05-05 17:04:14,903 - WARNING - 'area_km2' column missing in neighborhoods. Computing from geometry...\n",
      "2025-05-05 17:04:14,906 - INFO - Computed area_km2 stats:\n",
      "count    456.000000\n",
      "mean       0.588925\n",
      "std        1.351428\n",
      "min        0.031744\n",
      "25%        0.134566\n",
      "50%        0.209650\n",
      "75%        0.425264\n",
      "max       16.324434\n",
      "Name: area_km2, dtype: float64\n",
      "2025-05-05 17:04:14,906 - INFO - Loading roads from /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/taipei_segments_cleaned_verified.geoparquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting load_and_prepare_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 17:04:15,096 - INFO - Roads loaded: shape=(81444, 2), columns=['class', 'geometry']\n",
      "2025-05-05 17:04:15,097 - WARNING - 'length_m' column missing in roads. Computing from geometry...\n",
      "2025-05-05 17:04:15,105 - INFO - Computed length_m stats:\n",
      "count     81444.000000\n",
      "mean        145.622456\n",
      "std        2304.902398\n",
      "min           0.030284\n",
      "25%          28.160770\n",
      "50%          61.698697\n",
      "75%         130.534001\n",
      "max      426414.891763\n",
      "Name: length_m, dtype: float64\n",
      "2025-05-05 17:04:15,105 - INFO - Loading accidents from /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/2023_accidents.geojson\n",
      "2025-05-05 17:04:15,658 - INFO - Accidents loaded: shape=(56133, 8), columns=['Month', 'Day', 'Hours', 'Minute', 'Location', 'Speed_limit', 'Roadtype', 'geometry']\n",
      "2025-05-05 17:04:15,658 - INFO - Loading urban masterplan from /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/Taipei_urban_masterplan.geojson\n",
      "2025-05-05 17:04:16,334 - INFO - Urban masterplan loaded: shape=(15521, 15), columns=['編號', '圖層', '顏色', '街廓編號', '分區代碼', '分區簡稱', '使用分區', '分區說明', '原屬分區', '變更前代碼', '變更前簡稱', '變更前分區', 'Category', 'Area', 'geometry']\n",
      "2025-05-05 17:04:16,335 - INFO - Loading transit from /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/taipei_infrastructure.geoparquet\n",
      "2025-05-05 17:04:16,435 - INFO - Transit loaded: shape=(29892, 11), columns=['id', 'geometry', 'version', 'sources', 'subtype', 'class', 'surface', 'names', 'level', 'source_tags', 'wikidata']\n",
      "2025-05-05 17:04:16,436 - INFO - Loading trees from /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/taipei_land.geoparquet\n",
      "2025-05-05 17:04:16,462 - INFO - Trees loaded: shape=(5019, 12), columns=['id', 'geometry', 'version', 'sources', 'subtype', 'class', 'surface', 'names', 'level', 'source_tags', 'wikidata', 'elevation']\n",
      "2025-05-05 17:04:16,463 - INFO - Loading population from /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/population_corrected.json\n",
      "2025-05-05 17:04:16,466 - INFO - Population loaded: shape=(456, 3), columns=['LIE_NAME', 'Total_Population', 'Elderly_Percentage']\n",
      "2025-05-05 17:04:16,523 - WARNING - Invalid geometries in urban_masterplan. Attempting to fix.\n",
      "2025-05-05 17:04:16,751 - INFO - load_and_prepare_data completed successfully.\n",
      "2025-05-05 17:04:16,829 - INFO - urban_masterplan shape: (15521, 15)\n",
      "2025-05-05 17:04:16,829 - INFO - urban_masterplan columns: ['編號', '圖層', '顏色', '街廓編號', '分區代碼', '分區簡稱', '使用分區', '分區說明', '原屬分區', '變更前代碼', '變更前簡稱', '變更前分區', 'Category', 'Area', 'geometry']\n",
      "2025-05-05 17:04:16,832 - INFO - urban_masterplan sample (first 2 rows):\n",
      "  編號  圖層  顏色  街廓編號 分區代碼 分區簡稱  使用分區  分區說明  原屬分區 變更前代碼 變更前簡稱 變更前分區        Category      Area                                                                                                                                                                                                                                                                                                                                          geometry\n",
      "0  1  32  19  None  PEA    公  公園用地  None  None  None  None  None  City_Open_Area   823.190                                                                                                                           MULTIPOLYGON (((121.52863 25.0478, 121.52853 25.04812, 121.52855 25.04814, 121.52891 25.04806, 121.52892 25.04802, 121.52888 25.04798, 121.52884 25.04795, 121.52871 25.04783, 121.52866 25.04779, 121.52863 25.0478)))\n",
      "1  2  32  19  None  PEA    公  公園用地  None  None  None  None  None  City_Open_Area  4721.047  MULTIPOLYGON (((121.52755 25.04719, 121.52614 25.04763, 121.52611 25.04767, 121.52615 25.04781, 121.52618 25.04783, 121.52619 25.04782, 121.52639 25.04779, 121.52755 25.04748, 121.52755 25.04748, 121.52776 25.04743, 121.52778 25.0474, 121.52775 25.04732, 121.52771 25.04718, 121.52767 25.04716, 121.52755 25.04719, 121.52755 25.04719)))\n",
      "2025-05-05 17:04:16,832 - INFO - Computing correlation between road types and accident density...\n",
      "2025-05-05 17:04:16,847 - INFO - Reprojecting accidents from EPSG:4326 to EPSG:3826\n",
      "2025-05-05 17:04:16,915 - INFO - Roads CRS: {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"ProjectedCRS\", \"name\": \"TWD97 / TM2 zone 121\", \"base_crs\": {\"name\": \"TWD97\", \"datum\": {\"type\": \"GeodeticReferenceFrame\", \"name\": \"Taiwan Datum 1997\", \"ellipsoid\": {\"name\": \"GRS 1980\", \"semi_major_axis\": 6378137, \"inverse_flattening\": 298.257222101}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"id\": {\"authority\": \"EPSG\", \"code\": 3824}}, \"conversion\": {\"name\": \"Taiwan 2-degree TM zone 121\", \"method\": {\"name\": \"Transverse Mercator\", \"id\": {\"authority\": \"EPSG\", \"code\": 9807}}, \"parameters\": [{\"name\": \"Latitude of natural origin\", \"value\": 0, \"unit\": \"degree\", \"id\": {\"authority\": \"EPSG\", \"code\": 8801}}, {\"name\": \"Longitude of natural origin\", \"value\": 121, \"unit\": \"degree\", \"id\": {\"authority\": \"EPSG\", \"code\": 8802}}, {\"name\": \"Scale factor at natural origin\", \"value\": 0.9999, \"unit\": \"unity\", \"id\": {\"authority\": \"EPSG\", \"code\": 8805}}, {\"name\": \"False easting\", \"value\": 250000, \"unit\": \"metre\", \"id\": {\"authority\": \"EPSG\", \"code\": 8806}}, {\"name\": \"False northing\", \"value\": 0, \"unit\": \"metre\", \"id\": {\"authority\": \"EPSG\", \"code\": 8807}}]}, \"coordinate_system\": {\"subtype\": \"Cartesian\", \"axis\": [{\"name\": \"Easting\", \"abbreviation\": \"X\", \"direction\": \"east\", \"unit\": \"metre\"}, {\"name\": \"Northing\", \"abbreviation\": \"Y\", \"direction\": \"north\", \"unit\": \"metre\"}]}, \"scope\": \"Engineering survey, topographic mapping.\", \"area\": \"Taiwan, Republic of China - between 120\\u00b0E and 122\\u00b0E, onshore and offshore - Taiwan Island.\", \"bbox\": {\"south_latitude\": 20.41, \"west_longitude\": 119.99, \"north_latitude\": 26.72, \"east_longitude\": 122.06}, \"id\": {\"authority\": \"EPSG\", \"code\": 3826}}, Bounds: [ -49201.34316331 2687086.8646322   340402.47812579 3107530.84473387]\n",
      "2025-05-05 17:04:16,916 - INFO - Neighborhoods CRS: EPSG:3826, Bounds: [ 296266.05303084 2761514.89561711  317197.26073793 2789176.16901603]\n",
      "2025-05-05 17:04:16,918 - INFO - Accidents CRS: EPSG:3826, Bounds: [ 295756.69149719 2761989.77126713  315058.49224312 2786732.8587656 ]\n",
      "2025-05-05 17:04:16,922 - INFO - Roads geometry types: ['LineString']\n",
      "2025-05-05 17:04:16,923 - INFO - Neighborhoods geometry types: ['Polygon']\n",
      "2025-05-05 17:04:16,924 - INFO - Sample road geometries:\n",
      "0    LINESTRING (324778.51079599274 2780945.2627613...\n",
      "1    LINESTRING (296169.2235130913 2759463.11395603...\n",
      "2    LINESTRING (296381.58548612776 2758713.2679554...\n",
      "3    LINESTRING (297130.61534516735 2759165.1029101...\n",
      "4    LINESTRING (296863.93588555494 2759022.6435459...\n",
      "Name: geometry, dtype: object\n",
      "2025-05-05 17:04:16,925 - INFO - Sample neighborhood geometries:\n",
      "0    POLYGON ((302666.54271737445 2785226.842290448...\n",
      "1    POLYGON ((307802.16998795647 2787372.759303745...\n",
      "2    POLYGON ((302320.04429191677 2784654.093771082...\n",
      "3    POLYGON ((308159.7721281759 2784411.3655833476...\n",
      "4    POLYGON ((302319.03440019337 2784368.948626034...\n",
      "Name: geometry, dtype: object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data Structure Summary ---\n",
      "\n",
      "Dataset: neighborhoods\n",
      "Shape: (456, 58)\n",
      "Columns and Data Types:\n",
      "LIE_NAME                                    object\n",
      "SECT_NAME                                   object\n",
      "2024population                               int32\n",
      "land_use_city_open_area_count                int32\n",
      "land_use_city_open_area_area_m2            float64\n",
      "land_use_city_open_area_percent            float64\n",
      "land_use_commercial_count                    int32\n",
      "land_use_commercial_area_m2                float64\n",
      "land_use_commercial_percent                float64\n",
      "land_use_infrastructure_count                int32\n",
      "land_use_infrastructure_area_m2            float64\n",
      "land_use_infrastructure_percent            float64\n",
      "land_use_government_count                    int32\n",
      "land_use_government_area_m2                float64\n",
      "land_use_government_percent                float64\n",
      "land_use_public_transportation_count         int32\n",
      "land_use_public_transportation_area_m2     float64\n",
      "land_use_public_transportation_percent     float64\n",
      "land_use_education_count                     int32\n",
      "land_use_education_area_m2                 float64\n",
      "land_use_education_percent                 float64\n",
      "land_use_medical_count                       int32\n",
      "land_use_medical_area_m2                   float64\n",
      "land_use_medical_percent                   float64\n",
      "land_use_amenity_count                       int32\n",
      "land_use_amenity_area_m2                   float64\n",
      "land_use_amenity_percent                   float64\n",
      "land_use_road_count                          int32\n",
      "land_use_road_area_m2                      float64\n",
      "land_use_road_percent                      float64\n",
      "land_use_pedestrian_count                    int32\n",
      "land_use_pedestrian_area_m2                float64\n",
      "land_use_pedestrian_percent                float64\n",
      "land_use_natural_count                       int32\n",
      "land_use_natural_area_m2                   float64\n",
      "land_use_natural_percent                   float64\n",
      "land_use_special_zone_count                  int32\n",
      "land_use_special_zone_area_m2              float64\n",
      "land_use_special_zone_percent              float64\n",
      "land_use_river_count                         int32\n",
      "land_use_river_area_m2                     float64\n",
      "land_use_river_percent                     float64\n",
      "land_use_military_count                      int32\n",
      "land_use_military_area_m2                  float64\n",
      "land_use_military_percent                  float64\n",
      "land_use_residential_count                   int32\n",
      "land_use_residential_area_m2               float64\n",
      "land_use_residential_percent               float64\n",
      "land_use_industrial_count                    int32\n",
      "land_use_industrial_area_m2                float64\n",
      "land_use_industrial_percent                float64\n",
      "land_use_agriculture_count                   int32\n",
      "land_use_agriculture_area_m2               float64\n",
      "land_use_agriculture_percent               float64\n",
      "ndvi_mean                                  float64\n",
      "ndvi_median                                float64\n",
      "geometry                                  geometry\n",
      "area_km2                                   float64\n",
      "dtype: object\n",
      "Missing values (total): 0\n",
      "Missing values per column:\n",
      "LIE_NAME                                  0\n",
      "SECT_NAME                                 0\n",
      "2024population                            0\n",
      "land_use_city_open_area_count             0\n",
      "land_use_city_open_area_area_m2           0\n",
      "land_use_city_open_area_percent           0\n",
      "land_use_commercial_count                 0\n",
      "land_use_commercial_area_m2               0\n",
      "land_use_commercial_percent               0\n",
      "land_use_infrastructure_count             0\n",
      "land_use_infrastructure_area_m2           0\n",
      "land_use_infrastructure_percent           0\n",
      "land_use_government_count                 0\n",
      "land_use_government_area_m2               0\n",
      "land_use_government_percent               0\n",
      "land_use_public_transportation_count      0\n",
      "land_use_public_transportation_area_m2    0\n",
      "land_use_public_transportation_percent    0\n",
      "land_use_education_count                  0\n",
      "land_use_education_area_m2                0\n",
      "land_use_education_percent                0\n",
      "land_use_medical_count                    0\n",
      "land_use_medical_area_m2                  0\n",
      "land_use_medical_percent                  0\n",
      "land_use_amenity_count                    0\n",
      "land_use_amenity_area_m2                  0\n",
      "land_use_amenity_percent                  0\n",
      "land_use_road_count                       0\n",
      "land_use_road_area_m2                     0\n",
      "land_use_road_percent                     0\n",
      "land_use_pedestrian_count                 0\n",
      "land_use_pedestrian_area_m2               0\n",
      "land_use_pedestrian_percent               0\n",
      "land_use_natural_count                    0\n",
      "land_use_natural_area_m2                  0\n",
      "land_use_natural_percent                  0\n",
      "land_use_special_zone_count               0\n",
      "land_use_special_zone_area_m2             0\n",
      "land_use_special_zone_percent             0\n",
      "land_use_river_count                      0\n",
      "land_use_river_area_m2                    0\n",
      "land_use_river_percent                    0\n",
      "land_use_military_count                   0\n",
      "land_use_military_area_m2                 0\n",
      "land_use_military_percent                 0\n",
      "land_use_residential_count                0\n",
      "land_use_residential_area_m2              0\n",
      "land_use_residential_percent              0\n",
      "land_use_industrial_count                 0\n",
      "land_use_industrial_area_m2               0\n",
      "land_use_industrial_percent               0\n",
      "land_use_agriculture_count                0\n",
      "land_use_agriculture_area_m2              0\n",
      "land_use_agriculture_percent              0\n",
      "ndvi_mean                                 0\n",
      "ndvi_median                               0\n",
      "geometry                                  0\n",
      "area_km2                                  0\n",
      "dtype: int64\n",
      "Unique LIE_NAME: 456\n",
      "Sample data (first 2 rows):\n",
      "  LIE_NAME SECT_NAME  2024population  land_use_city_open_area_count  \\\n",
      "0      湖田里       北投區             856                              0   \n",
      "1      菁山里       士林區            1509                              0   \n",
      "\n",
      "   land_use_city_open_area_area_m2  land_use_city_open_area_percent  \\\n",
      "0                              0.0                              0.0   \n",
      "1                              0.0                              0.0   \n",
      "\n",
      "   land_use_commercial_count  land_use_commercial_area_m2  \\\n",
      "0                          0                          0.0   \n",
      "1                          0                          0.0   \n",
      "\n",
      "   land_use_commercial_percent  land_use_infrastructure_count  ...  \\\n",
      "0                          0.0                              0  ...   \n",
      "1                          0.0                              4  ...   \n",
      "\n",
      "   land_use_industrial_count  land_use_industrial_area_m2  \\\n",
      "0                          0                          0.0   \n",
      "1                          0                          0.0   \n",
      "\n",
      "   land_use_industrial_percent  land_use_agriculture_count  \\\n",
      "0                          0.0                           0   \n",
      "1                          0.0                           0   \n",
      "\n",
      "   land_use_agriculture_area_m2  land_use_agriculture_percent  ndvi_mean  \\\n",
      "0                           0.0                           0.0    0.78596   \n",
      "1                           0.0                           0.0    0.78907   \n",
      "\n",
      "   ndvi_median                                           geometry   area_km2  \n",
      "0     0.817502  POLYGON ((302666.543 2785226.842, 302675.82 27...  16.324434  \n",
      "1     0.827372  POLYGON ((307802.17 2787372.759, 307831.907 27...  11.719218  \n",
      "\n",
      "[2 rows x 58 columns]\n",
      "\n",
      "Dataset: roads\n",
      "Shape: (81444, 3)\n",
      "Columns and Data Types:\n",
      "class         object\n",
      "geometry    geometry\n",
      "length_m     float64\n",
      "dtype: object\n",
      "Missing values (total): 604\n",
      "Missing values per column:\n",
      "class       604\n",
      "geometry      0\n",
      "length_m      0\n",
      "dtype: int64\n",
      "Road class counts:\n",
      "class\n",
      "service          21575\n",
      "footway          19776\n",
      "residential      15429\n",
      "tertiary          5402\n",
      "steps             4301\n",
      "secondary         4059\n",
      "path              3857\n",
      "unclassified      1957\n",
      "primary           1292\n",
      "cycleway           878\n",
      "track              741\n",
      "trunk              609\n",
      "pedestrian         323\n",
      "motorway           316\n",
      "living_street      267\n",
      "unknown             56\n",
      "bridleway            2\n",
      "Name: count, dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "     class                                           geometry       length_m\n",
      "0     None  LINESTRING (324778.511 2780945.263, 324826.86 ...  426414.891763\n",
      "1  service  LINESTRING (296169.224 2759463.114, 296160.343...    1055.960489\n",
      "\n",
      "Dataset: accidents\n",
      "Shape: (56133, 8)\n",
      "Columns and Data Types:\n",
      "Month            object\n",
      "Day               int32\n",
      "Hours             int32\n",
      "Minute            int32\n",
      "Location         object\n",
      "Speed_limit       int32\n",
      "Roadtype          int32\n",
      "geometry       geometry\n",
      "dtype: object\n",
      "Missing values (total): 0\n",
      "Missing values per column:\n",
      "Month          0\n",
      "Day            0\n",
      "Hours          0\n",
      "Minute         0\n",
      "Location       0\n",
      "Speed_limit    0\n",
      "Roadtype       0\n",
      "geometry       0\n",
      "dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "     Month  Day  Hours  Minute              Location  Speed_limit  Roadtype  \\\n",
      "0  January    1      0      46  大安區仁愛路4段與忠孝東路4段216巷口           50         4   \n",
      "1  January    1      0      46  大安區仁愛路4段與忠孝東路4段216巷口           50         4   \n",
      "\n",
      "                     geometry  \n",
      "0  POINT (121.55304 25.03797)  \n",
      "1  POINT (121.55304 25.03797)  \n",
      "\n",
      "Dataset: urban_masterplan\n",
      "Shape: (15521, 15)\n",
      "Columns and Data Types:\n",
      "編號            object\n",
      "圖層            object\n",
      "顏色            object\n",
      "街廓編號          object\n",
      "分區代碼          object\n",
      "分區簡稱          object\n",
      "使用分區          object\n",
      "分區說明          object\n",
      "原屬分區          object\n",
      "變更前代碼         object\n",
      "變更前簡稱         object\n",
      "變更前分區         object\n",
      "Category      object\n",
      "Area         float64\n",
      "geometry    geometry\n",
      "dtype: object\n",
      "Missing values (total): 92705\n",
      "Missing values per column:\n",
      "編號              0\n",
      "圖層              5\n",
      "顏色              5\n",
      "街廓編號        15508\n",
      "分區代碼            5\n",
      "分區簡稱            5\n",
      "使用分區            0\n",
      "分區說明        15409\n",
      "原屬分區        15205\n",
      "變更前代碼       15521\n",
      "變更前簡稱       15521\n",
      "變更前分區       15521\n",
      "Category        0\n",
      "Area            0\n",
      "geometry        0\n",
      "dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "  編號  圖層  顏色  街廓編號 分區代碼 分區簡稱  使用分區  分區說明  原屬分區 變更前代碼 變更前簡稱 變更前分區  \\\n",
      "0  1  32  19  None  PEA    公  公園用地  None  None  None  None  None   \n",
      "1  2  32  19  None  PEA    公  公園用地  None  None  None  None  None   \n",
      "\n",
      "         Category      Area                                           geometry  \n",
      "0  City_Open_Area   823.190  MULTIPOLYGON (((121.52863 25.0478, 121.52853 2...  \n",
      "1  City_Open_Area  4721.047  MULTIPOLYGON (((121.52755 25.04719, 121.52614 ...  \n",
      "\n",
      "Dataset: transit\n",
      "Shape: (29892, 11)\n",
      "Columns and Data Types:\n",
      "id               object\n",
      "geometry       geometry\n",
      "version           int32\n",
      "sources          object\n",
      "subtype          object\n",
      "class            object\n",
      "surface          object\n",
      "names            object\n",
      "level           float64\n",
      "source_tags      object\n",
      "wikidata         object\n",
      "dtype: object\n",
      "Missing values (total): 101060\n",
      "Missing values per column:\n",
      "id                 0\n",
      "geometry           0\n",
      "version            0\n",
      "sources            0\n",
      "subtype            0\n",
      "class              0\n",
      "surface        28287\n",
      "names          17731\n",
      "level          25803\n",
      "source_tags        0\n",
      "wikidata       29239\n",
      "dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "                                 id  \\\n",
      "0  08b4ba0ac6758fff0001be0bbe7a4ada   \n",
      "1  08b4ba0ae332afff0001a76b5977464d   \n",
      "\n",
      "                                            geometry  version  \\\n",
      "0  LINESTRING (121.69639 24.99576, 121.69386 24.9...        0   \n",
      "1                         POINT (121.46384 24.93811)        0   \n",
      "\n",
      "                                             sources  subtype       class  \\\n",
      "0  [{'property': '', 'dataset': 'OpenStreetMap', ...    power  power_line   \n",
      "1  [{'property': '', 'dataset': 'OpenStreetMap', ...  barrier        gate   \n",
      "\n",
      "  surface names  level                         source_tags wikidata  \n",
      "0    None  None    NaN  [(power, line), (voltage, 345000)]     None  \n",
      "1    None  None    NaN                   [(barrier, gate)]     None  \n",
      "\n",
      "Dataset: trees\n",
      "Shape: (5019, 12)\n",
      "Columns and Data Types:\n",
      "id               object\n",
      "geometry       geometry\n",
      "version           int32\n",
      "sources          object\n",
      "subtype          object\n",
      "class            object\n",
      "surface          object\n",
      "names            object\n",
      "level           float64\n",
      "source_tags      object\n",
      "wikidata         object\n",
      "elevation       float64\n",
      "dtype: object\n",
      "Missing values (total): 24487\n",
      "Missing values per column:\n",
      "id                0\n",
      "geometry          0\n",
      "version           0\n",
      "sources           0\n",
      "subtype           0\n",
      "class             0\n",
      "surface        5013\n",
      "names          4553\n",
      "level          5015\n",
      "source_tags       0\n",
      "wikidata       4943\n",
      "elevation      4963\n",
      "dtype: int64\n",
      "Sample data (first 2 rows):\n",
      "                                 id  \\\n",
      "0  08b4ba2399d31fff0003c5f62a41c335   \n",
      "1  08b4ba0ae3a22fff0003ca54f368c81d   \n",
      "\n",
      "                                            geometry  version  \\\n",
      "0  POLYGON ((120.13944 22.99742, 120.13979 22.997...        0   \n",
      "1  POLYGON ((121.4716 24.95532, 121.47132 24.9554...        0   \n",
      "\n",
      "                                             sources subtype   class surface  \\\n",
      "0  [{'property': '', 'dataset': 'OpenStreetMap', ...    land  island    None   \n",
      "1  [{'property': '', 'dataset': 'OpenStreetMap', ...  forest    wood    None   \n",
      "\n",
      "                                               names  level  \\\n",
      "0  {'primary': '臺灣', 'common': [('af', 'Taiwan'),...    NaN   \n",
      "1                                               None    NaN   \n",
      "\n",
      "                               source_tags wikidata  elevation  \n",
      "0  [(place, island), (type, multipolygon)]   Q22502        NaN  \n",
      "1  [(natural, wood), (type, multipolygon)]     None        NaN  \n",
      "\n",
      "Dataset: population\n",
      "Shape: (456, 3)\n",
      "Columns and Data Types:\n",
      "LIE_NAME               object\n",
      "Total_Population        int64\n",
      "Elderly_Percentage    float64\n",
      "dtype: object\n",
      "Missing values (total): 0\n",
      "Missing values per column:\n",
      "LIE_NAME              0\n",
      "Total_Population      0\n",
      "Elderly_Percentage    0\n",
      "dtype: int64\n",
      "Unique LIE_NAME: 456\n",
      "Sample data (first 2 rows):\n",
      "  LIE_NAME  Total_Population  Elderly_Percentage\n",
      "0      南福里             12021               16.10\n",
      "1      奇岩里             11200               22.68\n",
      "--- End of Data Structure Summary ---\n",
      "\n",
      "Starting compute_road_type_accident_correlation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31606/128715011.py:36: UserWarning: Legend does not support handles for PatchCollection instances.\n",
      "See: https://matplotlib.org/stable/tutorials/intermediate/legend_guide.html#implementing-a-custom-legend-handler\n",
      "  plt.legend()\n",
      "2025-05-05 17:04:18,705 - INFO - Overlay plot saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/roads_neighborhoods_overlap.png\n",
      "2025-05-05 17:04:19,172 - INFO - Assigning accidents to nearest road...\n",
      "2025-05-05 17:04:24,302 - INFO - Matched 56133 accidents out of 56133\n",
      "2025-05-05 17:04:24,307 - INFO - Reassigning 4991 accidents from footway/cycleway...\n",
      "2025-05-05 17:04:24,407 - INFO - Reassigned 1429 accidents to wider roads\n",
      "2025-05-05 17:04:24,415 - INFO - Accidents by road type:\n",
      "class\n",
      "bridleway            0\n",
      "cycleway           247\n",
      "footway           3315\n",
      "living_street       79\n",
      "motorway           109\n",
      "path                86\n",
      "pedestrian          84\n",
      "primary           6535\n",
      "residential      10110\n",
      "secondary        16180\n",
      "service           5011\n",
      "steps               47\n",
      "tertiary          9135\n",
      "track                8\n",
      "trunk             2493\n",
      "unclassified      1665\n",
      "unknown             66\n",
      "Name: accident_count, dtype: int64\n",
      "2025-05-05 17:04:24,421 - INFO - length_m stats:\n",
      "count     75149.000000\n",
      "mean        157.303341\n",
      "std        2399.131756\n",
      "min          10.000152\n",
      "25%          35.129323\n",
      "50%          68.801448\n",
      "75%         139.983163\n",
      "max      426414.891763\n",
      "Name: length_m, dtype: float64\n",
      "2025-05-05 17:04:24,423 - INFO - accident_count stats:\n",
      "count    75149.000000\n",
      "mean         0.739172\n",
      "std          3.223498\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max        202.000000\n",
      "Name: accident_count, dtype: float64\n",
      "2025-05-05 17:04:24,427 - INFO - NaN in accident_density: 0\n",
      "2025-05-05 17:04:24,430 - INFO - accident_density stats:\n",
      "count    75149.000000\n",
      "mean         0.483172\n",
      "std          1.070430\n",
      "min          0.095310\n",
      "25%          0.095310\n",
      "50%          0.095310\n",
      "75%          0.095310\n",
      "max          7.051682\n",
      "Name: accident_density, dtype: float64\n",
      "2025-05-05 17:04:24,665 - INFO - Bar chart saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_bar.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bar chart saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_bar.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 17:04:25,025 - INFO - Box chart saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_box.png\n",
      "2025-05-05 17:04:25,209 - INFO - Scatter plot saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_scatter.png\n",
      "2025-05-05 17:04:25,211 - INFO - Top 3 road types by accident density:\n",
      "        class  accident_density\n",
      "8   secondary              2.14\n",
      "12    primary              2.10\n",
      "16      trunk              1.16\n",
      "2025-05-05 17:04:25,212 - INFO - Computing average road accident density per neighborhood...\n",
      "2025-05-05 17:04:25,213 - INFO - Roads DataFrame shape before join: (75149, 7)\n",
      "2025-05-05 17:04:25,213 - INFO - Neighborhoods DataFrame shape before join: (456, 58)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box chart saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_box.png\n",
      "Scatter plot saved to /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/road_type_accident_scatter.png\n",
      "Top 3 road types by accident density:\n",
      "        class  accident_density\n",
      "8   secondary              2.14\n",
      "12    primary              2.10\n",
      "16      trunk              1.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 17:04:25,541 - INFO - Road-neighborhood join resulted in 85340 matches with columns: ['geometry', 'class', 'length_m', 'width_rank', 'accident_density', 'index_right', 'LIE_NAME']\n",
      "2025-05-05 17:04:25,543 - INFO - Non-NaN LIE_NAME count: 60885\n",
      "2025-05-05 17:04:25,544 - INFO - Non-NaN accident_density count: 85340\n",
      "2025-05-05 17:04:25,545 - INFO - Unique LIE_NAME values: 456\n",
      "2025-05-05 17:04:25,549 - INFO - Number of neighborhoods with calculated avg_accident_density: 456\n",
      "2025-05-05 17:04:25,549 - INFO - NaN in avg_accident_density: 0\n",
      "2025-05-05 17:04:25,551 - INFO - Assigned avg_road_accident_density to 456 neighborhoods\n",
      "2025-05-05 17:04:25,553 - INFO - Avg road accident density stats:\n",
      "count    456.000000\n",
      "mean       0.818358\n",
      "std        0.343088\n",
      "min        0.101213\n",
      "25%        0.545024\n",
      "50%        0.806933\n",
      "75%        1.051564\n",
      "max        1.904460\n",
      "Name: avg_road_accident_density, dtype: float64\n",
      "2025-05-05 17:04:25,569 - INFO - Computing pedestrian road density per neighborhood...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting compute_pedestrian_road_density...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 17:04:25,979 - INFO - pedestrian_road_density stats:\n",
      "count    456.000000\n",
      "mean      15.841759\n",
      "std       10.276229\n",
      "min        0.000000\n",
      "25%        8.202007\n",
      "50%       15.006599\n",
      "75%       21.558429\n",
      "max       57.267153\n",
      "Name: pedestrian_road_density, dtype: float64\n",
      "2025-05-05 17:04:25,983 - INFO - Stage 2: Building city graph...\n",
      "2025-05-05 17:04:25,984 - INFO - Dataset neighborhoods column types:\n",
      "LIE_NAME                                    object\n",
      "SECT_NAME                                   object\n",
      "2024population                               int32\n",
      "land_use_city_open_area_count                int32\n",
      "land_use_city_open_area_area_m2            float64\n",
      "land_use_city_open_area_percent            float64\n",
      "land_use_commercial_count                    int32\n",
      "land_use_commercial_area_m2                float64\n",
      "land_use_commercial_percent                float64\n",
      "land_use_infrastructure_count                int32\n",
      "land_use_infrastructure_area_m2            float64\n",
      "land_use_infrastructure_percent            float64\n",
      "land_use_government_count                    int32\n",
      "land_use_government_area_m2                float64\n",
      "land_use_government_percent                float64\n",
      "land_use_public_transportation_count         int32\n",
      "land_use_public_transportation_area_m2     float64\n",
      "land_use_public_transportation_percent     float64\n",
      "land_use_education_count                     int32\n",
      "land_use_education_area_m2                 float64\n",
      "land_use_education_percent                 float64\n",
      "land_use_medical_count                       int32\n",
      "land_use_medical_area_m2                   float64\n",
      "land_use_medical_percent                   float64\n",
      "land_use_amenity_count                       int32\n",
      "land_use_amenity_area_m2                   float64\n",
      "land_use_amenity_percent                   float64\n",
      "land_use_road_count                          int32\n",
      "land_use_road_area_m2                      float64\n",
      "land_use_road_percent                      float64\n",
      "land_use_pedestrian_count                    int32\n",
      "land_use_pedestrian_area_m2                float64\n",
      "land_use_pedestrian_percent                float64\n",
      "land_use_natural_count                       int32\n",
      "land_use_natural_area_m2                   float64\n",
      "land_use_natural_percent                   float64\n",
      "land_use_special_zone_count                  int32\n",
      "land_use_special_zone_area_m2              float64\n",
      "land_use_special_zone_percent              float64\n",
      "land_use_river_count                         int32\n",
      "land_use_river_area_m2                     float64\n",
      "land_use_river_percent                     float64\n",
      "land_use_military_count                      int32\n",
      "land_use_military_area_m2                  float64\n",
      "land_use_military_percent                  float64\n",
      "land_use_residential_count                   int32\n",
      "land_use_residential_area_m2               float64\n",
      "land_use_residential_percent               float64\n",
      "land_use_industrial_count                    int32\n",
      "land_use_industrial_area_m2                float64\n",
      "land_use_industrial_percent                float64\n",
      "land_use_agriculture_count                   int32\n",
      "land_use_agriculture_area_m2               float64\n",
      "land_use_agriculture_percent               float64\n",
      "ndvi_mean                                  float64\n",
      "ndvi_median                                float64\n",
      "geometry                                  geometry\n",
      "area_km2                                   float64\n",
      "avg_road_accident_density                  float64\n",
      "pedestrian_road_density                    float64\n",
      "dtype: object\n",
      "2025-05-05 17:04:25,991 - INFO - Dataset roads column types:\n",
      "class         object\n",
      "geometry    geometry\n",
      "length_m     float64\n",
      "dtype: object\n",
      "2025-05-05 17:04:25,993 - INFO - Dataset accidents column types:\n",
      "Month            object\n",
      "Day               int32\n",
      "Hours             int32\n",
      "Minute            int32\n",
      "Location         object\n",
      "Speed_limit       int32\n",
      "Roadtype          int32\n",
      "geometry       geometry\n",
      "dtype: object\n",
      "2025-05-05 17:04:25,996 - INFO - Dataset urban_masterplan column types:\n",
      "編號            object\n",
      "圖層            object\n",
      "顏色            object\n",
      "街廓編號          object\n",
      "分區代碼          object\n",
      "分區簡稱          object\n",
      "使用分區          object\n",
      "分區說明          object\n",
      "原屬分區          object\n",
      "變更前代碼         object\n",
      "變更前簡稱         object\n",
      "變更前分區         object\n",
      "Category      object\n",
      "Area         float64\n",
      "geometry    geometry\n",
      "dtype: object\n",
      "2025-05-05 17:04:26,000 - INFO - Dataset transit column types:\n",
      "id               object\n",
      "geometry       geometry\n",
      "version           int32\n",
      "sources          object\n",
      "subtype          object\n",
      "class            object\n",
      "surface          object\n",
      "names            object\n",
      "level           float64\n",
      "source_tags      object\n",
      "wikidata         object\n",
      "dtype: object\n",
      "2025-05-05 17:04:26,004 - INFO - Dataset trees column types:\n",
      "id               object\n",
      "geometry       geometry\n",
      "version           int32\n",
      "sources          object\n",
      "subtype          object\n",
      "class            object\n",
      "surface          object\n",
      "names            object\n",
      "level           float64\n",
      "source_tags      object\n",
      "wikidata         object\n",
      "elevation       float64\n",
      "dtype: object\n",
      "2025-05-05 17:04:26,008 - INFO - Dataset population column types:\n",
      "LIE_NAME               object\n",
      "Total_Population        int64\n",
      "Elderly_Percentage    float64\n",
      "dtype: object\n",
      "2025-05-05 17:04:26,009 - INFO - Adding neighborhood nodes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting build_graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neighborhood nodes: 100%|██████████| 456/456 [00:00<00:00, 10078.85it/s]\n",
      "2025-05-05 17:04:26,057 - INFO - Adding building nodes...\n",
      "2025-05-05 17:04:26,057 - ERROR - Pipeline failed with error: 'buildings'\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'buildings'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 469\u001b[39m\n\u001b[32m    466\u001b[39m     logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining and validation R2 plot saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.path.join(BASE_DIR,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtraining_validation_r2.png\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     results = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforce_recompute_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m     plot_training_history(results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(force_recompute_graph)\u001b[39m\n\u001b[32m     48\u001b[39m start_time = time.time()\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting build_graph...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m G = \u001b[43mbuild_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_recompute\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_recompute_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m timings[\u001b[33m'\u001b[39m\u001b[33mbuild_graph\u001b[39m\u001b[33m'\u001b[39m] = time.time() - start_time\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Validate edge counts\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mbuild_graph\u001b[39m\u001b[34m(data, force_recompute)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Add building nodes\u001b[39;00m\n\u001b[32m     70\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33mAdding building nodes...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m buildings_gdf = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbuildings\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33marea_m2\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m buildings_gdf.columns:\n\u001b[32m     73\u001b[39m     logging.warning(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33marea_m2\u001b[39m\u001b[33m'\u001b[39m\u001b[33m missing. Computing from geometry...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'buildings'"
     ]
    }
   ],
   "source": [
    "def main(force_recompute_graph=False):\n",
    "    \"\"\"Main execution pipeline for the analysis.\"\"\"\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    os.makedirs(SUBGRAPH_DIR, exist_ok=True)\n",
    "    logging.info(f\"Ensured subgraph directory exists: {SUBGRAPH_DIR}\")\n",
    "\n",
    "    # Import f_oneway for ANOVA test\n",
    "    from scipy.stats import f_oneway\n",
    "\n",
    "    # Track timing for each step\n",
    "    timings = {}\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load and prepare data\n",
    "        start_time = time.time()\n",
    "        print(\"Starting load_and_prepare_data...\")\n",
    "        data = load_and_prepare_data()\n",
    "        print_data_structure(data)\n",
    "        timings['load_and_prepare_data'] = time.time() - start_time\n",
    "\n",
    "        # Validate urban_masterplan\n",
    "        if 'urban_masterplan' not in data:\n",
    "            logging.error(\"urban_masterplan missing from data dictionary.\")\n",
    "            raise KeyError(\"urban_masterplan missing from data dictionary.\")\n",
    "        if not isinstance(data['urban_masterplan'], gpd.GeoDataFrame):\n",
    "            logging.error(\"urban_masterplan is not a GeoDataFrame.\")\n",
    "            raise TypeError(\"urban_masterplan is not a GeoDataFrame.\")\n",
    "        logging.info(f\"urban_masterplan shape: {data['urban_masterplan'].shape}\")\n",
    "        logging.info(f\"urban_masterplan columns: {list(data['urban_masterplan'].columns)}\")\n",
    "        logging.info(f\"urban_masterplan sample (first 2 rows):\\n{data['urban_masterplan'].head(2).to_string()}\")\n",
    "\n",
    "        # Step 2: Compute road type accident correlation\n",
    "        start_time = time.time()\n",
    "        print(\"Starting compute_road_type_accident_correlation...\")\n",
    "        road_accident_summary, roads_data, neighborhoods_gdf = compute_road_type_accident_correlation(\n",
    "            data['roads'], data['neighborhoods'], data['accidents']\n",
    "        )\n",
    "        data['neighborhoods'] = neighborhoods_gdf\n",
    "        timings['compute_road_type_accident_correlation'] = time.time() - start_time\n",
    "\n",
    "        # Step 2.5: Compute pedestrian road density\n",
    "        start_time = time.time()\n",
    "        print(\"Starting compute_pedestrian_road_density...\")\n",
    "        data['neighborhoods'] = compute_pedestrian_road_density(data['roads'], data['neighborhoods'])\n",
    "        timings['compute_pedestrian_road_density'] = time.time() - start_time\n",
    "\n",
    "        # Step 3: Build graph\n",
    "        start_time = time.time()\n",
    "        print(\"Starting build_graph...\")\n",
    "        G = build_graph(data, force_recompute=force_recompute_graph)\n",
    "        timings['build_graph'] = time.time() - start_time\n",
    "\n",
    "        # Validate edge counts\n",
    "        edge_count = G.edgelist.edgelist_df.shape[0] if G.edgelist else 0\n",
    "        logging.info(f\"Graph edge count: {edge_count}\")\n",
    "        if edge_count == 0:\n",
    "            logging.warning(\"Graph has no edges. GNN will not utilize graph structure.\")\n",
    "\n",
    "        # Step 4: Compute walkability scores\n",
    "        start_time = time.time()\n",
    "        print(\"Starting compute_walkability_scores...\")\n",
    "        walkability_components, components = compute_walkability_components_all(data['neighborhoods'], data)\n",
    "        logging.info(f\"Columns in walkability_components: {list(walkability_components.columns)}\")\n",
    "        G = compute_walkability_scores(G, data)\n",
    "        timings['compute_walkability_scores'] = time.time() - start_time\n",
    "\n",
    "        # Update data['neighborhoods'] with the cluster column\n",
    "        data['neighborhoods'] = data['neighborhoods'].merge(\n",
    "            walkability_components[['LIE_NAME', 'cluster']],\n",
    "            on='LIE_NAME',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Step 5: Prepare GNN data\n",
    "        start_time = time.time()\n",
    "        print(\"Starting prepare_gnn_data...\")\n",
    "        data_gnn = prepare_gnn_data(G)\n",
    "        timings['prepare_gnn_data'] = time.time() - start_time\n",
    "\n",
    "        # Step 6: Train GNN model\n",
    "        start_time = time.time()\n",
    "        print(\"Starting train_gnn_model...\")\n",
    "        results = train_gnn_model(data_gnn)\n",
    "        model = results['model']\n",
    "        timings['train_gnn_model'] = time.time() - start_time\n",
    "\n",
    "        # Step 7: Predict walkability\n",
    "        start_time = time.time()\n",
    "        print(\"Starting predict_walkability...\")\n",
    "        G = predict_walkability(G, model)\n",
    "        timings['predict_walkability'] = time.time() - start_time\n",
    "\n",
    "        # Step 8: Create interactive map\n",
    "        start_time = time.time()\n",
    "        print(\"Starting create_interactive_map...\")\n",
    "        create_interactive_map(G, data)\n",
    "        timings['create_interactive_map'] = time.time() - start_time\n",
    "\n",
    "        # Final validation: Check walkability scores\n",
    "        nodes_df = G._nodes.to_pandas()\n",
    "        neighborhood_nodes = nodes_df[nodes_df['type'] == 'neighborhood']\n",
    "        walkability_score_stats = neighborhood_nodes['walkability_score'].describe()\n",
    "        walkability_gnn_stats = neighborhood_nodes['walkability_gnn'].describe()\n",
    "        walkability_category_dist = neighborhood_nodes['walkability_category'].value_counts()\n",
    "        non_zero_walkability = (neighborhood_nodes['walkability_score'] > 0).sum()\n",
    "        non_zero_walkability_gnn = (neighborhood_nodes['walkability_gnn'] > 0).sum()\n",
    "        \n",
    "        logging.info(\"Final validation - Walkability scores in neighborhood nodes:\")\n",
    "        logging.info(f\"Walkability score distribution:\\n{walkability_score_stats}\")\n",
    "        logging.info(f\"Walkability GNN distribution:\\n{walkability_gnn_stats}\")\n",
    "        logging.info(f\"Walkability category distribution:\\n{walkability_category_dist}\")\n",
    "        logging.info(f\"Number of neighborhood nodes with non-zero walkability_score: {non_zero_walkability}/{len(neighborhood_nodes)}\")\n",
    "        logging.info(f\"Number of neighborhood nodes with non-zero walkability_gnn: {non_zero_walkability_gnn}/{len(neighborhood_nodes)}\")\n",
    "\n",
    "        # Check for low variation in walkability scores\n",
    "        if walkability_score_stats['std'] < 0.05:\n",
    "            logging.warning(\"Walkability scores have low variation (std < 0.05). Components may need adjustment.\")\n",
    "        if walkability_gnn_stats['std'] < 0.05:\n",
    "            logging.warning(\"GNN predictions have low variation (std < 0.05). Check edge creation and model training.\")\n",
    "\n",
    "        # Compute correlation between walkability_score and walkability_gnn\n",
    "        corr, p_value = pearsonr(neighborhood_nodes['walkability_score'], neighborhood_nodes['walkability_gnn'])\n",
    "        logging.info(f\"Correlation between walkability_score and walkability_gnn: {corr:.2f} (p-value: {p_value:.2f})\")\n",
    "        if corr < 0.5:\n",
    "            logging.warning(\"Low correlation between walkability_score and walkability_gnn. GNN predictions may not align well with rule-based scores.\")\n",
    "\n",
    "        # Generate additional charts\n",
    "        # 1. Distribution of accident_density Before and After Smoothing\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "        roads_data['accident_density_adjusted'] = roads_data['accident_density_raw'] + 0.01\n",
    "        roads_data['accident_density_adjusted'] = np.log1p(roads_data['accident_density_adjusted'])\n",
    "        sns.histplot(roads_data['accident_density_raw'], kde=True, color='blue', label='Raw (Before Smoothing)', stat='density', log_scale=True, ax=ax1)\n",
    "        sns.histplot(roads_data['accident_density_adjusted'], kde=True, color='orange', label='After Smoothing & Log Transform', stat='density', ax=ax1)\n",
    "        ax1.set_xlabel('Accident Density (accidents/km, log scale)')\n",
    "        ax1.set_ylabel('Density')\n",
    "        ax1.legend(loc='upper left')\n",
    "        ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "        ax2 = ax1.twiny()\n",
    "        ax2.set_xlim(ax1.get_xlim())\n",
    "        ax2.set_xticks(ax1.get_xticks())\n",
    "        ax2.set_xticklabels([f\"{10**tick:.0f}\" for tick in ax1.get_xticks()])\n",
    "        ax2.set_xlabel('Accident Density (accidents/km, linear scale)')\n",
    "\n",
    "        ax3 = ax1.twinx()\n",
    "        sns.ecdfplot(roads_data['accident_density_raw'], color='blue', linestyle='-.', linewidth=2, ax=ax3, label='Raw CDF')\n",
    "        sns.ecdfplot(roads_data['accident_density_adjusted'], color='orange', linestyle='-.', linewidth=2, ax=ax3, label='Transformed CDF')\n",
    "        ax3.set_ylabel('Cumulative Probability')\n",
    "        ax3.legend(loc='upper right')\n",
    "\n",
    "        plt.title('Distribution of Accident Density Before and After Smoothing')\n",
    "        plt.tight_layout()\n",
    "        accident_density_dist_path = os.path.join(BASE_DIR, 'accident_density_distribution.png')\n",
    "        plt.savefig(accident_density_dist_path)\n",
    "        plt.close()\n",
    "        logging.info(f\"Accident density distribution plot saved to {accident_density_dist_path}\")\n",
    "\n",
    "        # 2. Comparison of avg_road_accident_density Across Neighborhood Clusters\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        logging.info(f\"Columns in data['neighborhoods']: {list(data['neighborhoods'].columns)}\")\n",
    "        if 'avg_road_accident_density' not in data['neighborhoods'].columns:\n",
    "            logging.error(\"'avg_road_accident_density' column missing in data['neighborhoods']\")\n",
    "            raise KeyError(\"'avg_road_accident_density' column missing\")\n",
    "        sns.boxplot(x='cluster', y='avg_road_accident_density', data=data['neighborhoods'], palette='Set2')\n",
    "        sns.stripplot(x='cluster', y='avg_road_accident_density', data=data['neighborhoods'], color='black', size=3, alpha=0.5)\n",
    "        plt.xlabel('Neighborhood Cluster (0=Urban, 1=Suburban, 2=Rural)')\n",
    "        plt.ylabel('Avg. Road Accident Density\\n(accidents/km, log scale)')\n",
    "        plt.title('Average Road Accident Density by Neighborhood Cluster')\n",
    "        cluster_counts = data['neighborhoods']['cluster'].value_counts().sort_index()\n",
    "        max_y = data['neighborhoods']['avg_road_accident_density'].max() + 0.2\n",
    "        for i, count in enumerate(cluster_counts):\n",
    "            plt.text(i, max_y, f'n={count}', horizontalalignment='center', fontsize=10)\n",
    "        clusters = data['neighborhoods'].groupby('cluster')['avg_road_accident_density'].apply(list)\n",
    "        f_stat, p_value = f_oneway(*clusters)\n",
    "        plt.text(1, max_y + 0.1, f'ANOVA p={p_value:.4f}', horizontalalignment='center', fontsize=10)\n",
    "        plt.tight_layout()\n",
    "        accident_density_cluster_path = os.path.join(BASE_DIR, 'accident_density_by_cluster.png')\n",
    "        plt.savefig(accident_density_cluster_path)\n",
    "        plt.close()\n",
    "        logging.info(f\"Accident density by cluster plot saved to {accident_density_cluster_path}\")\n",
    "        logging.info(f\"ANOVA for avg_road_accident_density across clusters: F={f_stat:.2f}, p={p_value:.4f}\")\n",
    "\n",
    "        # 3. Walkability Score Distributions\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.kdeplot(neighborhood_nodes['walkability_score'], label='Rule-Based Walkability Score', color='blue')\n",
    "        sns.kdeplot(neighborhood_nodes['walkability_gnn'], label='GNN Predicted Walkability', color='orange')\n",
    "        plt.xlabel('Walkability Score')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title('Distribution of Walkability Scores: Rule-Based vs. GNN Predicted')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        walkability_dist_path = os.path.join(BASE_DIR, 'walkability_score_distribution.png')\n",
    "        plt.savefig(walkability_dist_path)\n",
    "        plt.close()\n",
    "        logging.info(f\"Walkability score distribution plot saved to {walkability_dist_path}\")\n",
    "\n",
    "        # 4. Walkability Category Distribution\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.countplot(x='walkability_category', data=neighborhood_nodes, order=['low', 'medium', 'high'])\n",
    "        plt.xlabel('Walkability Category')\n",
    "        plt.ylabel('Number of Neighborhoods')\n",
    "        plt.title('Distribution of Walkability Categories')\n",
    "        plt.tight_layout()\n",
    "        walkability_category_path = os.path.join(BASE_DIR, 'walkability_category_distribution.png')\n",
    "        plt.savefig(walkability_category_path)\n",
    "        plt.close()\n",
    "        logging.info(f\"Walkability category distribution plot saved to {walkability_category_path}\")\n",
    "\n",
    "        # 5. Correlation Between walkability_score and walkability_gnn\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(x='walkability_score', y='walkability_gnn', data=neighborhood_nodes)\n",
    "        sns.regplot(x='walkability_score', y='walkability_gnn', data=neighborhood_nodes, scatter=False, color='red')\n",
    "        plt.xlabel('Rule-Based Walkability Score')\n",
    "        plt.ylabel('GNN Predicted Walkability Score')\n",
    "        plt.title(f'Correlation: Rule-Based vs. GNN Predicted Walkability (r={corr:.2f})')\n",
    "        plt.tight_layout()\n",
    "        walkability_correlation_path = os.path.join(BASE_DIR, 'walkability_correlation.png')\n",
    "        plt.savefig(walkability_correlation_path)\n",
    "        plt.close()\n",
    "        logging.info(f\"Walkability correlation plot saved to {walkability_correlation_path}\")\n",
    "\n",
    "        # 6. Component Contributions to walkability_score\n",
    "        if 'green_space' not in walkability_components.columns:\n",
    "            logging.error(\"'green_space' column is missing in walkability_components.\")\n",
    "            raise KeyError(\"'green_space' column is missing.\")\n",
    "        logging.info(f\"Raw green_space stats (before normalization):\\n{walkability_components['green_space'].describe()}\")\n",
    "        logging.info(f\"Green space normalized stats:\\n{walkability_components['green_space_norm'].describe()}\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(walkability_components['green_space'], kde=True, color='green', label='Raw Green Space', stat='density')\n",
    "        sns.histplot(walkability_components['green_space_norm'], kde=True, color='darkgreen', label='Normalized Green Space', stat='density')\n",
    "        plt.xlabel('Green Space Value')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title('Distribution of Green Space (Raw vs. Normalized)')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        green_space_dist_path = os.path.join(BASE_DIR, 'green_space_distribution.png')\n",
    "        plt.savefig(green_space_dist_path)\n",
    "        plt.close()\n",
    "        logging.info(f\"Green space distribution plot saved to {green_space_dist_path}\")\n",
    "\n",
    "        logging.info(f\"Components: {list(components.keys())}\")\n",
    "        logging.info(f\"Number of components: {len(components)}\")\n",
    "        logging.info(f\"Walkability components columns: {list(walkability_components.columns)}\")\n",
    "\n",
    "        components = {\n",
    "            'land_use_score': 'higher_better',\n",
    "            'intersection_density': 'higher_better',\n",
    "            'population_density': 'higher_better',\n",
    "            'transit_density': 'higher_better',\n",
    "            'green_space': 'higher_better',\n",
    "            'accident_density': 'lower_better',\n",
    "            'sidewalk_coverage': 'higher_better',\n",
    "            'amenities_density': 'higher_better',\n",
    "            'pop_transit_interaction': 'higher_better',\n",
    "            'safety_green_interaction': 'lower_better'\n",
    "        }\n",
    "\n",
    "        component_cols = [f'{comp}_norm' for comp in components]\n",
    "        missing_cols = [col for col in component_cols if col not in walkability_components.columns]\n",
    "        if missing_cols:\n",
    "            logging.error(f\"Missing component columns in walkability_components: {missing_cols}\")\n",
    "            raise ValueError(f\"Missing component columns: {missing_cols}\")\n",
    "\n",
    "        walkability_components[component_cols].to_csv(os.path.join(BASE_DIR, 'walkability_components.csv'), index=False)\n",
    "        logging.info(f\"walkability_components saved to {os.path.join(BASE_DIR, 'walkability_components.csv')}\")\n",
    "        logging.info(f\"walkability_components shape: {walkability_components.shape}\")\n",
    "        logging.info(f\"walkability_components summary statistics:\\n{walkability_components[component_cols].describe()}\")\n",
    "        logging.info(f\"walkability_components non-NaN counts:\\n{walkability_components[component_cols].notna().sum()}\")\n",
    "        logging.info(f\"walkability_components sample data (first 5 rows):\\n{walkability_components[component_cols].head()}\")\n",
    "\n",
    "        base_weights = {\n",
    "            'land_use_score': 0.20,\n",
    "            'intersection_density': 0.05,\n",
    "            'population_density': 0.20,\n",
    "            'transit_density': 0.20,\n",
    "            'green_space': 0.15,\n",
    "            'accident_density': 0.10,\n",
    "            'sidewalk_coverage': 0.05,\n",
    "            'amenities_density': 0.05,\n",
    "            'pop_transit_interaction': 0.05,\n",
    "            'safety_green_interaction': 0.05\n",
    "        }\n",
    "\n",
    "        def adjust_weights(cluster):\n",
    "            weights = base_weights.copy()\n",
    "            if cluster == 0:  # Urban\n",
    "                weights['transit_density'] += 0.20\n",
    "                weights['amenities_density'] += 0.20\n",
    "                weights['green_space'] -= 0.20\n",
    "            elif cluster == 1:  # Suburban\n",
    "                weights['sidewalk_coverage'] += 0.20\n",
    "                weights['population_density'] -= 0.20\n",
    "            else:  # Rural\n",
    "                weights['green_space'] += 0.20\n",
    "                weights['accident_density'] += 0.20\n",
    "                weights['transit_density'] -= 0.20\n",
    "            total = sum(weights.values())\n",
    "            return {k: v/total for k, v in weights.items()}\n",
    "\n",
    "        weighted_contributions = pd.DataFrame(index=walkability_components.index, columns=component_cols)\n",
    "        for idx, row in walkability_components.iterrows():\n",
    "            cluster = row['cluster']\n",
    "            weights = adjust_weights(cluster)\n",
    "            for comp in components:\n",
    "                norm_col = f'{comp}_norm'\n",
    "                weighted_contributions.at[idx, norm_col] = weights.get(comp, 0.05) * row[norm_col]\n",
    "\n",
    "        component_means = pd.Series(index=component_cols, dtype=float)\n",
    "        component_stds = pd.Series(index=component_cols, dtype=float)\n",
    "        for col in component_cols:\n",
    "            if col in weighted_contributions.columns and weighted_contributions[col].notna().sum() > 0:\n",
    "                component_means[col] = weighted_contributions[col].mean()\n",
    "                component_stds[col] = weighted_contributions[col].std()\n",
    "            else:\n",
    "                logging.warning(f\"No valid data for component {col}. Setting mean and std to 0.\")\n",
    "                component_means[col] = 0.0\n",
    "                component_stds[col] = 0.0\n",
    "        logging.info(f\"Computed component_means (weighted):\\n{component_means}\")\n",
    "        logging.info(f\"Computed component_stds (weighted):\\n{component_stds}\")\n",
    "\n",
    "        plot_data_list = [\n",
    "            {'Component': col, 'Mean': component_means[col], 'Std': component_stds[col]}\n",
    "            for col in component_cols\n",
    "        ]\n",
    "        plot_data = pd.DataFrame(plot_data_list)\n",
    "        plot_data['Mean'] = plot_data['Mean'].astype(float)\n",
    "        plot_data['Std'] = plot_data['Std'].astype(float)\n",
    "        plot_data.to_csv(os.path.join(BASE_DIR, 'plot_data.csv'), index=False)\n",
    "        logging.info(f\"plot_data saved to {os.path.join(BASE_DIR, 'plot_data.csv')}\")\n",
    "        logging.info(f\"plot_data shape: {plot_data.shape}\")\n",
    "        logging.info(f\"plot_data dtypes: {plot_data.dtypes}\")\n",
    "        logging.info(f\"plot_data contents:\\n{plot_data}\")\n",
    "\n",
    "        if len(plot_data) != len(component_cols):\n",
    "            logging.error(f\"plot_data has incorrect number of rows: {len(plot_data)}, expected {len(component_cols)}\")\n",
    "            raise ValueError(f\"plot_data has incorrect number of rows: {len(plot_data)}\")\n",
    "\n",
    "        logging.info(f\"Input to sns.barplot: x='Component', y='Mean', data=\\n{plot_data[['Component', 'Mean']]}\")\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='Component', y='Mean', data=plot_data, capsize=0.2)\n",
    "        plt.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "        plt.xlabel('Component')\n",
    "        plt.ylabel('Mean Weighted Contribution to Walkability Score')\n",
    "        plt.title('Average Weighted Contribution of Components to Walkability Score')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        component_contribution_path = os.path.join(BASE_DIR, 'component_contributions.png')\n",
    "        plt.savefig(component_contribution_path)\n",
    "        plt.close()\n",
    "        logging.info(f\"Component contributions plot saved to {component_contribution_path}\")\n",
    "\n",
    "        # 7. Spatial Distribution of Walkability Scores\n",
    "        import contextily as ctx\n",
    "        neighborhoods_gdf = data['neighborhoods'].merge(\n",
    "            neighborhood_nodes[['LIE_NAME', 'walkability_score', 'walkability_gnn']],\n",
    "            on='LIE_NAME',\n",
    "            how='left'\n",
    "        )\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        neighborhoods_gdf.plot(\n",
    "            column='walkability_score', \n",
    "            cmap='YlGn', \n",
    "            legend=True, \n",
    "            ax=ax,\n",
    "            legend_kwds={\n",
    "                'label': \"Walkability Score\",\n",
    "                'orientation': \"horizontal\",\n",
    "                'pad': 0.05,\n",
    "                'fraction': 0.05,\n",
    "                'aspect': 30\n",
    "            }\n",
    "        )\n",
    "        ctx.add_basemap(\n",
    "            ax, \n",
    "            crs=neighborhoods_gdf.crs.to_string(), \n",
    "            source=ctx.providers.OpenStreetMap.Mapnik,\n",
    "            attribution=\"© OpenStreetMap contributors\",\n",
    "            attribution_size=8\n",
    "        )\n",
    "        plt.title('Spatial Distribution of Rule-Based Walkability Scores', pad=20)\n",
    "        plt.xlabel('Easting (EPSG:3826)', labelpad=10)\n",
    "        plt.ylabel('Northing (EPSG:3826)', labelpad=10)\n",
    "        plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.15)\n",
    "        walkability_map_path = os.path.join(BASE_DIR, 'walkability_score_map.png')\n",
    "        plt.savefig(walkability_map_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        logging.info(f\"Walkability score map saved to {walkability_map_path}\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 10))\n",
    "        neighborhoods_gdf.plot(\n",
    "            column='walkability_gnn', \n",
    "            cmap='YlGn', \n",
    "            legend=True, \n",
    "            ax=ax,\n",
    "            legend_kwds={\n",
    "                'label': \"GNN Predicted Walkability\",\n",
    "                'orientation': \"horizontal\",\n",
    "                'pad': 0.05,\n",
    "                'fraction': 0.05,\n",
    "                'aspect': 30\n",
    "            }\n",
    "        )\n",
    "        ctx.add_basemap(\n",
    "            ax, \n",
    "            crs=neighborhoods_gdf.crs.to_string(), \n",
    "            source=ctx.providers.OpenStreetMap.Mapnik,\n",
    "            attribution=\"© OpenStreetMap contributors\",\n",
    "            attribution_size=8\n",
    "        )\n",
    "        plt.title('Spatial Distribution of GNN Predicted Walkability Scores', pad=20)\n",
    "        plt.xlabel('Easting (EPSG:3826)', labelpad=10)\n",
    "        plt.ylabel('Northing (EPSG:3826)', labelpad=10)\n",
    "        plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.15)\n",
    "        walkability_gnn_map_path = os.path.join(BASE_DIR, 'walkability_gnn_map.png')\n",
    "        plt.savefig(walkability_gnn_map_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        logging.info(f\"GNN walkability map saved to {walkability_gnn_map_path}\")\n",
    "\n",
    "        logging.info(\"Processing complete. Timing summary:\")\n",
    "        for step, duration in timings.items():\n",
    "            logging.info(f\"{step}: {duration:.2f} seconds\")\n",
    "        \n",
    "        print(\"Pipeline completed successfully.\")\n",
    "        print(G.edgelist.edgelist_df.to_pandas().head())\n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Pipeline failed with error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def plot_training_history(results):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(results['train_losses'], label='Train Loss')\n",
    "    plt.plot(results['val_losses'], label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'training_validation_loss.png'))\n",
    "    plt.close()\n",
    "    logging.info(f\"Training and validation loss plot saved to {os.path.join(BASE_DIR, 'training_validation_loss.png')}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(results['train_maes'], label='Train MAE')\n",
    "    plt.plot(results['val_maes'], label='Val MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation MAE')\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'training_validation_mae.png'))\n",
    "    plt.close()\n",
    "    logging.info(f\"Training and validation MAE plot saved to {os.path.join(BASE_DIR, 'training_validation_mae.png')}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(results['train_r2s'], label='Train R2')\n",
    "    plt.plot(results['val_r2s'], label='Val R2')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('R2 Score')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation R2 Score')\n",
    "    plt.savefig(os.path.join(BASE_DIR, 'training_validation_r2.png'))\n",
    "    plt.close()\n",
    "    logging.info(f\"Training and validation R2 plot saved to {os.path.join(BASE_DIR, 'training_validation_r2.png')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main(force_recompute_graph=True)\n",
    "    plot_training_history(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
