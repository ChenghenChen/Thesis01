{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dc2c30cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.02.02 25.02.00\n"
     ]
    }
   ],
   "source": [
    "import cudf, cugraph\n",
    "print(cudf.__version__, cugraph.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8268bf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial population total rows: 456\n",
      "Initial population unique LIE_NAME: 454\n",
      "Initial population duplicates: 2\n",
      "Population duplicate LIE_NAME details:\n",
      " index LIE_NAME\n",
      "    55      中央里\n",
      "   208      新安里\n",
      "   377      中央里\n",
      "   443      新安里\n",
      "\n",
      "After population correction:\n",
      "Total rows: 456\n",
      "Unique LIE_NAME: 456\n",
      "Duplicates: 0\n",
      "Corrected population JSON saved to: /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/population_corrected.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "BASE_DIR = \"/home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data\"\n",
    "POPULATION_PATH = os.path.join(BASE_DIR, \"population.json\")\n",
    "OUTPUT_POPULATION_PATH = os.path.join(BASE_DIR, \"population_corrected.json\")\n",
    "\n",
    "pop_df = pd.read_json(POPULATION_PATH)\n",
    "pop_df = pop_df.rename(columns={'District': 'LIE_NAME'})\n",
    "print(f\"Initial population total rows: {len(pop_df)}\")\n",
    "print(f\"Initial population unique LIE_NAME: {pop_df['LIE_NAME'].nunique()}\")\n",
    "print(f\"Initial population duplicates: {pop_df['LIE_NAME'].duplicated().sum()}\")\n",
    "if pop_df['LIE_NAME'].duplicated().sum() > 0:\n",
    "    print(\"Population duplicate LIE_NAME details:\")\n",
    "    print(pop_df[pop_df['LIE_NAME'].duplicated(keep=False)][['LIE_NAME']].reset_index().to_string(index=False))\n",
    "\n",
    "pop_df.loc[55, 'LIE_NAME'] = '北投中央里'\n",
    "pop_df.loc[377, 'LIE_NAME'] = '中山中央里'\n",
    "pop_df.loc[208, 'LIE_NAME'] = '士林新安里'\n",
    "pop_df.loc[443, 'LIE_NAME'] = '萬華新安里'\n",
    "\n",
    "print(f\"\\nAfter population correction:\")\n",
    "print(f\"Total rows: {len(pop_df)}\")\n",
    "print(f\"Unique LIE_NAME: {pop_df['LIE_NAME'].nunique()}\")\n",
    "print(f\"Duplicates: {pop_df['LIE_NAME'].duplicated().sum()}\")\n",
    "\n",
    "pop_df.to_json(OUTPUT_POPULATION_PATH, orient='records', force_ascii=False)\n",
    "print(f\"Corrected population JSON saved to: {OUTPUT_POPULATION_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac3ad7",
   "metadata": {},
   "source": [
    "Cell 1: Imports ,Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e388b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import cugraph\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "from shapely import make_valid\n",
    "from shapely.errors import GEOSException\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from IPython.display import Image, display\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px  \n",
    "import plotly.graph_objects as go  \n",
    "import cupy\n",
    "import hashlib\n",
    "import json\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, GATConv, BatchNorm\n",
    "from keplergl import KeplerGl\n",
    "\n",
    "try:\n",
    "    import cuspatial\n",
    "    CUSPATIAL_AVAILABLE = False\n",
    "except ImportError:\n",
    "    logging.warning(\"CuSpatial not available, falling back to CPU-based computation.\")\n",
    "    CUSPATIAL_AVAILABLE = False\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Noto Sans CJK TC', 'Noto Serif CJK TC', 'Noto Sans Mono CJK TC', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "BASE_DIR = \"/home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data\"\n",
    "LANDUSE_NDVI_PATH = os.path.join(BASE_DIR, \"neighborhoods_with_ndvi_numerical_corrected.geojson\")\n",
    "OSM_BUILDINGS_PATH = os.path.join(BASE_DIR, \"Taipei_Buildings_fulldata.geojson\")\n",
    "OSM_ROADS_PATH = os.path.join(BASE_DIR, \"taipei_segments_cleaned_verified.geoparquet\")\n",
    "OSM_TREES_PATH = os.path.join(BASE_DIR, \"taipei_land.geoparquet\")\n",
    "OSM_TRANSIT_PATH = os.path.join(BASE_DIR, \"taipei_infrastructure.geoparquet\")\n",
    "URBAN_MASTERPLAN_PATH = os.path.join(BASE_DIR, \"Taipei_urban_masterplan.geojson\")\n",
    "ACCIDENTS_PATH = os.path.join(BASE_DIR, \"2023_accidents.geojson\")\n",
    "POPULATION_PATH = os.path.join(BASE_DIR, \"population_corrected.json\")\n",
    "SUBGRAPH_DIR = os.path.join(BASE_DIR, \"subgraphs\")\n",
    "CHECKPOINT_DIR = os.path.join(BASE_DIR, \"checkpoints\")\n",
    "INTERSECTION_CACHE_PATH = os.path.join(BASE_DIR, \"neighborhoods_with_intersections.geoparquet\")\n",
    "GRAPH_NODES_CACHE_PATH = os.path.join(BASE_DIR, \"graph_nodes.parquet\")\n",
    "GRAPH_EDGES_CACHE_PATH = os.path.join(BASE_DIR, \"graph_edges.parquet\")\n",
    "GRAPH_NODE_ID_CACHE_PATH = os.path.join(BASE_DIR, \"graph_node_id_to_index.json\")\n",
    "GRAPH_DATA_HASH_PATH = os.path.join(BASE_DIR, \"graph_data_hash.txt\")\n",
    "\n",
    "os.makedirs(SUBGRAPH_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "CATEGORY_PRIORITY = {\n",
    "    'City_Open_Area': 10,\n",
    "    'Pedestrian': 9,\n",
    "    'Public_Transportation': 8,\n",
    "    'Amenity': 7,\n",
    "    'Education': 6,\n",
    "    'Medical': 5,\n",
    "    'Commercial': 4,\n",
    "    'Residential': 3,\n",
    "    'Natural': 2,\n",
    "    'Road': 1,\n",
    "    'River': 1,\n",
    "    'Infrastructure': 1,\n",
    "    'Government': 1,\n",
    "    'Special_Zone': 1,\n",
    "    'Military': 1,\n",
    "    'Industrial': 1,\n",
    "    'Agriculture': 1\n",
    "}\n",
    "\n",
    "land_use_weights = {\n",
    "    'city_open_area': 0.8,\n",
    "    'commercial': 0.7,\n",
    "    'infrastructure': 0.4,\n",
    "    'government': 0.5,\n",
    "    'public_transportation': 0.8,\n",
    "    'education': 0.7,\n",
    "    'medical': 0.6,\n",
    "    'amenity': 0.8,\n",
    "    'road': 0.3,\n",
    "    'pedestrian': 1.0,\n",
    "    'natural': 0.7,\n",
    "    'special_zone': 0.4,\n",
    "    'river': 0.7,\n",
    "    'military': 0.2,\n",
    "    'residential': 0.6,\n",
    "    'industrial': 0.3,\n",
    "    'agriculture': 0.4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4722f69a",
   "metadata": {},
   "source": [
    "Cell 2: Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "713654c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_structure(data_dict):\n",
    "    print(\"\\n--- Detailed Data Structure Overview ---\")\n",
    "    for key, df in data_dict.items():\n",
    "        if isinstance(df, (gpd.GeoDataFrame, pd.DataFrame, cudf.DataFrame)):\n",
    "            df = df.to_pandas() if isinstance(df, cudf.DataFrame) else df\n",
    "            print(f\"\\nDataset: {key}\")\n",
    "            print(f\"Shape: {df.shape}\")\n",
    "            print(f\"Columns: {list(df.columns)}\")\n",
    "            print(f\"Data types:\\n{df.dtypes}\")\n",
    "            print(f\"Missing values per column:\\n{df.isnull().sum()}\")\n",
    "            if not df.select_dtypes(include=['float64', 'int64']).empty:\n",
    "                summary = df.describe().round(2)\n",
    "                print(f\"Summary statistics for numerical columns:\\n{summary}\")\n",
    "            for col in df.columns:\n",
    "                if col in ['LIE_NAME', 'vertex', 'type', 'class', 'building', 'Category']:\n",
    "                    unique_count = df[col].nunique()\n",
    "                    print(f\"Unique values in {col}: {unique_count}\")\n",
    "                    duplicates = df[col].duplicated().sum()\n",
    "                    if duplicates > 0:\n",
    "                        print(f\"Duplicates in {col}: {duplicates}\")\n",
    "                        print(f\"Sample duplicates in {col}:\\n{df[df[col].duplicated(keep=False)][col].head()}\")\n",
    "                    if col == 'class' and key == 'roads':\n",
    "                        print(f\"Value counts for {col}:\\n{df[col].value_counts()}\")\n",
    "            print(\"Sample data (first 5 rows):\")\n",
    "            sample = df.head(5).copy()\n",
    "            for col in sample.select_dtypes(include=['float64', 'int64']).columns:\n",
    "                sample[col] = sample[col].round(2)\n",
    "            print(sample)\n",
    "    print(\"--- End of Detailed Data Structure Overview ---\\n\")\n",
    "\n",
    "def fix_geometry(geom, buffer_size=1e-5):\n",
    "    if geom is None or geom.is_empty:\n",
    "        return geom\n",
    "    geom = make_valid(geom)\n",
    "    if not geom.is_valid:\n",
    "        geom = geom.buffer(buffer_size)\n",
    "        geom = make_valid(geom)\n",
    "    if not geom.is_valid:\n",
    "        logging.warning(f\"Geometry remains invalid after fixing: {geom.bounds}\")\n",
    "    return geom\n",
    "\n",
    "def print_percentage_calculation(neighborhoods_gdf, urban_masterplan_gdf, sample_size=3):\n",
    "    print(\"\\n--- Percentage Calculation Process ---\")\n",
    "    sample_neighborhoods = neighborhoods_gdf.sample(min(sample_size, len(neighborhoods_gdf)), random_state=42)\n",
    "    \n",
    "    for idx, row in sample_neighborhoods.iterrows():\n",
    "        lie_name = row['LIE_NAME']\n",
    "        print(f\"\\nNeighborhood: {lie_name} (Index: {idx})\")\n",
    "        \n",
    "        neighborhood_geom = fix_geometry(row['geometry'])\n",
    "        if not neighborhood_geom.is_valid:\n",
    "            print(f\"Neighborhood geometry is invalid after fixing: {lie_name}\")\n",
    "            continue\n",
    "        \n",
    "        relevant_masterplan = urban_masterplan_gdf[urban_masterplan_gdf.intersects(neighborhood_geom)]\n",
    "        if relevant_masterplan.empty:\n",
    "            print(\"No master plan polygons intersect with this neighborhood.\")\n",
    "            continue\n",
    "        \n",
    "        temp_gdf = gpd.GeoDataFrame({'geometry': [neighborhood_geom]}, crs='EPSG:3826')\n",
    "        intersected = gpd.overlay(temp_gdf, relevant_masterplan, how='intersection', keep_geom_type=False)\n",
    "        if intersected.empty:\n",
    "            print(\"No valid intersections after overlay.\")\n",
    "            continue\n",
    "        \n",
    "        intersected['geometry'] = intersected['geometry'].apply(fix_geometry)\n",
    "        intersected = intersected[intersected.geometry.is_valid & ~intersected.geometry.is_empty]\n",
    "        if intersected.empty:\n",
    "            print(\"No valid geometries after fixing intersected polygons.\")\n",
    "            continue\n",
    "        \n",
    "        intersected['priority'] = intersected['Category'].map(CATEGORY_PRIORITY)\n",
    "        intersected = intersected.sort_values(by='priority', ascending=False)\n",
    "        \n",
    "        total_area_geom = intersected.geometry.union_all()\n",
    "        total_area_geom = fix_geometry(total_area_geom)\n",
    "        if not total_area_geom.is_valid or total_area_geom.is_empty:\n",
    "            print(\"Total area geometry is invalid after fixing.\")\n",
    "            continue\n",
    "        total_area = total_area_geom.area\n",
    "        print(f\"Total unique master plan area: {total_area:.2f} m²\")\n",
    "        \n",
    "        remaining_geom = total_area_geom\n",
    "        category_areas = {}\n",
    "        unique_categories = intersected['Category'].unique()\n",
    "        \n",
    "        for category in sorted(unique_categories, key=lambda x: CATEGORY_PRIORITY.get(x, 0), reverse=True):\n",
    "            category_rows = intersected[intersected['Category'] == category]\n",
    "            category_geom = category_rows.geometry.union_all()\n",
    "            category_geom = fix_geometry(category_geom)\n",
    "            if not category_geom.is_valid or category_geom.is_empty:\n",
    "                print(f\"Geometry for category {category} is invalid after fixing.\")\n",
    "                category_areas[category] = 0.0\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                category_area_geom = category_geom.intersection(remaining_geom)\n",
    "                category_area_geom = fix_geometry(category_area_geom)\n",
    "                if not category_area_geom.is_valid or category_area_geom.is_empty:\n",
    "                    print(f\"Intersection geometry for category {category} is invalid after fixing.\")\n",
    "                    category_areas[category] = 0.0\n",
    "                    continue\n",
    "                \n",
    "                category_area = category_area_geom.area\n",
    "                category_areas[category] = category_area\n",
    "                print(f\"Area of {category} (priority {CATEGORY_PRIORITY.get(category, 0)}): {category_area:.2f} m²\")\n",
    "                \n",
    "                remaining_geom = remaining_geom.difference(category_area_geom)\n",
    "                remaining_geom = fix_geometry(remaining_geom)\n",
    "                if not remaining_geom.is_valid or remaining_geom.is_empty:\n",
    "                    print(f\"Remaining geometry is invalid after subtracting {category}.\")\n",
    "                    break\n",
    "            except GEOSException as e:\n",
    "                print(f\"Topology error for category {category}: {e}\")\n",
    "                category_areas[category] = 0.0\n",
    "                continue\n",
    "        \n",
    "        print(\"\\nPercentages:\")\n",
    "        total_percentage = 0.0\n",
    "        for category, area in category_areas.items():\n",
    "            percentage = (area / total_area * 100) if total_area > 0 else 0.0\n",
    "            total_percentage += percentage\n",
    "            print(f\"{category}: {percentage:.2f}%\")\n",
    "        print(f\"Sum of percentages: {total_percentage:.2f}%\")\n",
    "    print(\"--- End of Percentage Calculation Process ---\\n\")\n",
    "    \n",
    "def compute_data_hash(data_dict):\n",
    "    hasher = hashlib.sha256()\n",
    "    \n",
    "    for key, df in data_dict.items():\n",
    "        if isinstance(df, (gpd.GeoDataFrame, pd.DataFrame, cudf.DataFrame)):\n",
    "            df = df.to_pandas() if isinstance(df, cudf.DataFrame) else df\n",
    "            hasher.update(str(df.shape).encode('utf-8'))\n",
    "            hasher.update(str(sorted(df.columns)).encode('utf-8'))\n",
    "            sample = df.head(5).to_json()\n",
    "            hasher.update(sample.encode('utf-8'))\n",
    "    \n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def compute_road_type_accident_correlation(roads_gdf, neighborhoods_gdf, accidents_gdf):\n",
    "    \"\"\"\n",
    "    Compute correlation between OSM road types and accident density (accidents per km of road length).\n",
    "    Uses road class as a proxy for width, with ordinal ranking based on OSM hierarchy.\n",
    "    Generates bar, box, and scatter plots for visualization.\n",
    "    \"\"\"\n",
    "    logging.info(\"Computing correlation between road types and accident density...\")\n",
    "    \n",
    "    # Define ordinal width proxy based on OSM highway hierarchy\n",
    "    width_ranking = {\n",
    "        'motorway': 5,\n",
    "        'trunk': 5,\n",
    "        'primary': 4,\n",
    "        'secondary': 4,\n",
    "        'tertiary': 3,\n",
    "        'residential': 3,\n",
    "        'living_street': 3,\n",
    "        'service': 2,\n",
    "        'track': 2,\n",
    "        'path': 1,\n",
    "        'footway': 1,\n",
    "        'cycleway': 1,\n",
    "        'steps': 1,\n",
    "        'pedestrian': 1,\n",
    "        'unclassified': 0,\n",
    "        'bridleway': 0,\n",
    "        'unknown': 0\n",
    "    }\n",
    "    \n",
    "    # Assign width rank to roads\n",
    "    roads_gdf = roads_gdf.copy()\n",
    "    roads_gdf['width_rank'] = roads_gdf['class'].map(width_ranking).fillna(0).astype(int)\n",
    "    \n",
    "    # Assign accidents to the nearest road using sjoin_nearest\n",
    "    logging.info(\"Assigning accidents to nearest road...\")\n",
    "    joined_gdf = gpd.sjoin_nearest(accidents_gdf, roads_gdf, how='left', distance_col='distance')\n",
    "    \n",
    "    # Count accidents per road segment\n",
    "    accident_counts = joined_gdf.groupby(joined_gdf.index_right).size().reindex(roads_gdf.index, fill_value=0)\n",
    "    roads_gdf['accident_count'] = accident_counts\n",
    "    \n",
    "    # Filter short roads to avoid inflated accident density\n",
    "    roads_gdf = roads_gdf[roads_gdf['length_m'] >= 10]\n",
    "    \n",
    "    # Compute accident density (accidents per km)\n",
    "    roads_gdf['accident_density'] = roads_gdf['accident_count'] / (roads_gdf['length_m'] / 1000)\n",
    "    roads_gdf['accident_density'] = roads_gdf['accident_density'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # Log road type counts\n",
    "    logging.info(f\"Road type counts:\\n{roads_gdf['class'].value_counts()}\")\n",
    "    print(f\"Road type counts:\\n{roads_gdf['class'].value_counts()}\")\n",
    "    \n",
    "    # Aggregate by road class for summary\n",
    "    summary = roads_gdf.groupby('class').agg({\n",
    "        'length_m': 'sum',\n",
    "        'accident_count': 'sum',\n",
    "        'accident_density': 'mean',\n",
    "        'width_rank': 'first'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Filter out classes with negligible data\n",
    "    summary = summary[summary['length_m'] > 1000]  # At least 1km total length\n",
    "    summary = summary[summary['width_rank'] > 0]   # Exclude unknown, unclassified, bridleway\n",
    "    \n",
    "    # Log summary\n",
    "    print(\"\\n--- Road Type Accident Density Summary ---\")\n",
    "    print(summary[['class', 'length_m', 'accident_count', 'accident_density', 'width_rank']].round(2))\n",
    "    \n",
    "    # Compute Spearman's rank correlation\n",
    "    from scipy.stats import spearmanr\n",
    "    if len(summary) >= 2:\n",
    "        corr, p_value = spearmanr(summary['width_rank'], summary['accident_density'])\n",
    "        logging.info(f\"Spearman's correlation between road width rank and accident density: {corr:.3f} (p-value: {p_value:.3f})\")\n",
    "        print(f\"Spearman's correlation: {corr:.3f} (p-value: {p_value:.3f})\")\n",
    "    else:\n",
    "        logging.warning(\"Insufficient road types for correlation analysis.\")\n",
    "        print(\"Insufficient road types for correlation analysis.\")\n",
    "    \n",
    "    # Compute average accident density per neighborhood for walkability\n",
    "    logging.info(\"Computing average road accident density per neighborhood...\")\n",
    "    road_neighborhoods = gpd.sjoin(roads_gdf[['geometry', 'class', 'length_m', 'width_rank', 'accident_density']], \n",
    "                                   neighborhoods_gdf[['geometry', 'LIE_NAME']], \n",
    "                                   how='left', predicate='intersects')\n",
    "    avg_accident_density = road_neighborhoods.groupby('LIE_NAME')['accident_density'].mean().reindex(neighborhoods_gdf['LIE_NAME'], fill_value=0)\n",
    "    neighborhoods_gdf['avg_road_accident_density'] = avg_accident_density\n",
    "    \n",
    "    # Visualize with multiple charts\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # 1. Bar Chart: Mean accident density by road type\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    summary_sorted = summary.sort_values('width_rank', ascending=False)\n",
    "    sns.barplot(data=summary_sorted, x='class', y='accident_density', hue='width_rank', dodge=False)\n",
    "    plt.xlabel('Road Type')\n",
    "    plt.ylabel('Mean Accident Density (Accidents per km)')\n",
    "    plt.title('Mean Accident Density by Road Type')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Width Rank')\n",
    "    plt.tight_layout()\n",
    "    bar_path = os.path.join(BASE_DIR, 'road_type_accident_bar.png')\n",
    "    plt.savefig(bar_path)\n",
    "    plt.close()\n",
    "    logging.info(f\"Bar chart saved to {bar_path}\")\n",
    "    print(f\"Bar chart saved to {bar_path}\")\n",
    "    \n",
    "    # 2. Box Plot: Distribution of accident density by road type\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(data=roads_gdf[roads_gdf['class'].isin(summary['class'])], \n",
    "                x='class', y='accident_density', hue='width_rank', dodge=False)\n",
    "    plt.xlabel('Road Type')\n",
    "    plt.ylabel('Accident Density (Accidents per km)')\n",
    "    plt.title('Distribution of Accident Density by Road Type')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yscale('log')  # Log scale for skewed data\n",
    "    plt.legend(title='Width Rank')\n",
    "    plt.tight_layout()\n",
    "    box_path = os.path.join(BASE_DIR, 'road_type_accident_box.png')\n",
    "    plt.savefig(box_path)\n",
    "    plt.close()\n",
    "    logging.info(f\"Box chart saved to {box_path}\")\n",
    "    print(f\"Box chart saved to {box_path}\")\n",
    "    \n",
    "    # 3. Scatter Plot: Accident density vs. width rank with trend line\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=summary, x='width_rank', y='accident_density', \n",
    "                    size='length_m', sizes=(50, 500), hue='class', style='class', alpha=0.7)\n",
    "    # Add trend line\n",
    "    z = np.polyfit(summary['width_rank'], summary['accident_density'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(summary['width_rank'], p(summary['width_rank']), \"r--\", alpha=0.5)\n",
    "    plt.xlabel('Road Width Rank (1=Path, 5=Motorway)')\n",
    "    plt.ylabel('Mean Accident Density (Accidents per km)')\n",
    "    plt.title('Road Type vs. Accident Density')\n",
    "    plt.yscale('log')  # Log scale for skewed data\n",
    "    plt.grid(True)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    scatter_path = os.path.join(BASE_DIR, 'road_type_accident_scatter.png')\n",
    "    plt.savefig(scatter_path)\n",
    "    plt.close()\n",
    "    logging.info(f\"Scatter plot saved to {scatter_path}\")\n",
    "    print(f\"Scatter plot saved to {scatter_path}\")\n",
    "    \n",
    "    # Log top accident-prone road types\n",
    "    top_types = summary.nlargest(3, 'accident_density')[['class', 'accident_density']]\n",
    "    logging.info(f\"Top 3 road types by accident density:\\n{top_types.round(2)}\")\n",
    "    print(f\"Top 3 road types by accident density:\\n{top_types.round(2)}\")\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ae081",
   "metadata": {},
   "source": [
    "Cell 3: Walkability Computation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3b05a23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_walkability_components(neighborhoods_gdf, sample_size=5):\n",
    "    sample_gdf = neighborhoods_gdf.sample(min(sample_size, len(neighborhoods_gdf)), random_state=42)\n",
    "    \n",
    "    components = {\n",
    "        'LIE_NAME': [],\n",
    "        'land_use_diversity': [],\n",
    "        'green_space_score': [],\n",
    "        'transit_score': [],\n",
    "        'road_connectivity': [],\n",
    "        'safety_score': [],\n",
    "        'elderly_accessibility': [],\n",
    "        'walkability_score': []\n",
    "    }\n",
    "    \n",
    "    ndvi_min, ndvi_max = neighborhoods_gdf['ndvi_mean'].min(), neighborhoods_gdf['ndvi_mean'].max()\n",
    "    tree_min, tree_max = neighborhoods_gdf['tree_count'].min(), neighborhoods_gdf['tree_count'].max()\n",
    "    transit_min, transit_max = neighborhoods_gdf['transit_count'].min(), neighborhoods_gdf['transit_count'].max()\n",
    "    \n",
    "    if 'intersection_density' in neighborhoods_gdf.columns:\n",
    "        intersection_density_min = neighborhoods_gdf['intersection_density'].min()\n",
    "        intersection_density_max = neighborhoods_gdf['intersection_density'].max()\n",
    "    else:\n",
    "        logging.warning(\"'intersection_density' column missing in neighborhoods_gdf. Defaulting to 0.\")\n",
    "        intersection_density_min = 0\n",
    "        intersection_density_max = 1\n",
    "    \n",
    "    # For accident_count\n",
    "    accident_count_min, accident_count_max = neighborhoods_gdf['accident_count'].min(), neighborhoods_gdf['accident_count'].max()\n",
    "    # For avg_road_accident_density\n",
    "    if 'avg_road_accident_density' in neighborhoods_gdf.columns:\n",
    "        accident_density_max = neighborhoods_gdf['avg_road_accident_density'].max()\n",
    "    else:\n",
    "        logging.warning(\"'avg_road_accident_density' column missing. Defaulting to 0.\")\n",
    "        accident_density_max = 1\n",
    "    \n",
    "    for idx, row in sample_gdf.iterrows():\n",
    "        land_use_cols = [f\"land_use_{category.lower()}_percent\" for category in CATEGORY_PRIORITY.keys()]\n",
    "        land_use_values = [row.get(col, 0.0) / 100 for col in land_use_cols if col in row]\n",
    "        land_use_values = [v for v in land_use_values if v > 0]\n",
    "        if land_use_values:\n",
    "            entropy = -np.sum([p * np.log2(p) for p in land_use_values])\n",
    "            max_entropy = np.log2(len(land_use_values)) if len(land_use_values) > 0 else 1\n",
    "            land_use_diversity = entropy / max_entropy if max_entropy > 0 else 0\n",
    "        else:\n",
    "            land_use_diversity = 0\n",
    "        \n",
    "        ndvi_normalized = ((row['ndvi_mean'] - ndvi_min) / (ndvi_max - ndvi_min + 1e-6))\n",
    "        tree_normalized = ((row['tree_count'] - tree_min) / (tree_max - tree_min + 1e-6))\n",
    "        open_area = row.get('land_use_city_open_area_percent', 0.0) / 100\n",
    "        green_space_score = (ndvi_normalized + tree_normalized + open_area) / 3\n",
    "        \n",
    "        transit_raw = (row['transit_count'] - transit_min) / (transit_max - transit_min + 1e-6)\n",
    "        transit_score = np.log1p(transit_raw * 10) / np.log1p(10)\n",
    "        \n",
    "        intersection_density = row.get('intersection_density', 0.0)\n",
    "        intersection_density_normalized = (intersection_density - intersection_density_min) / (intersection_density_max - intersection_density_min + 1e-6)\n",
    "        road_connectivity = np.log1p(intersection_density_normalized * 10) / np.log1p(10)\n",
    "        \n",
    "        # Hybrid safety score: combine accident_count and avg_road_accident_density\n",
    "        accident_count_density = row['accident_count'] / row['area_km2'] if row['area_km2'] > 0 else 0\n",
    "        accident_count_density_max = (accident_count_max / neighborhoods_gdf['area_km2'].min()) if neighborhoods_gdf['area_km2'].min() > 0 else 1\n",
    "        accident_count_density = min(accident_count_density, accident_count_density_max * 0.5)  # Cap to reduce urban bias\n",
    "        safety_score_count = 1 - (accident_count_density / (accident_count_density_max + 1e-6))\n",
    "        \n",
    "        accident_density = row.get('avg_road_accident_density', 0.0)\n",
    "        safety_score_roads = 1 - (accident_density / (accident_density_max + 1e-6))\n",
    "        \n",
    "        # Combine both safety metrics (50% weight each)\n",
    "        safety_score = 0.5 * safety_score_count + 0.5 * safety_score_roads\n",
    "        \n",
    "        elderly_accessibility = row['elderly_percentage'] / 100\n",
    "        \n",
    "        base_score = (\n",
    "            0.3 * land_use_diversity +\n",
    "            0.3 * green_space_score +\n",
    "            0.2 * transit_score +\n",
    "            0.2 * road_connectivity\n",
    "        )\n",
    "        \n",
    "        safety_modifier = 0.3 + 0.7 * safety_score  # Increased weight for safety\n",
    "        elderly_modifier = 1 + elderly_accessibility\n",
    "        walkability_score = base_score * safety_modifier * elderly_modifier\n",
    "        \n",
    "        walkability_score = 1 / (1 + np.exp(-5 * (walkability_score - 1)))\n",
    "        \n",
    "        components['LIE_NAME'].append(row['LIE_NAME'])\n",
    "        components['land_use_diversity'].append(land_use_diversity)\n",
    "        components['green_space_score'].append(green_space_score)\n",
    "        components['transit_score'].append(transit_score)\n",
    "        components['road_connectivity'].append(road_connectivity)\n",
    "        components['safety_score'].append(safety_score)\n",
    "        components['elderly_accessibility'].append(elderly_accessibility)\n",
    "        components['walkability_score'].append(walkability_score)\n",
    "    \n",
    "    return pd.DataFrame(components)\n",
    "\n",
    "def compute_walkability_components_all(neighborhoods_df):\n",
    "    components = {\n",
    "        'LIE_NAME': [],\n",
    "        'land_use_diversity': [],\n",
    "        'green_space_score': [],\n",
    "        'transit_score': [],\n",
    "        'road_connectivity': [],\n",
    "        'safety_score': [],\n",
    "        'elderly_accessibility': [],\n",
    "        'walkability_score': []\n",
    "    }\n",
    "    \n",
    "    ndvi_min, ndvi_max = neighborhoods_df['ndvi_mean'].min(), neighborhoods_df['ndvi_mean'].max()\n",
    "    tree_min, tree_max = neighborhoods_df['tree_count'].min(), neighborhoods_df['tree_count'].max()\n",
    "    transit_min, transit_max = neighborhoods_df['transit_count'].min(), neighborhoods_df['transit_count'].max()\n",
    "    \n",
    "    if 'intersection_density' in neighborhoods_df.columns:\n",
    "        intersection_density_min = neighborhoods_df['intersection_density'].min()\n",
    "        intersection_density_max = neighborhoods_df['intersection_density'].max()\n",
    "    else:\n",
    "        logging.warning(\"'intersection_density' column missing. Defaulting to 0.\")\n",
    "        intersection_density_min = 0\n",
    "        intersection_density_max = 1\n",
    "    \n",
    "    # For accident_count\n",
    "    accident_count_min, accident_count_max = neighborhoods_df['accident_count'].min(), neighborhoods_df['accident_count'].max()\n",
    "    # For avg_road_accident_density\n",
    "    if 'avg_road_accident_density' in neighborhoods_df.columns:\n",
    "        accident_density_max = neighborhoods_df['avg_road_accident_density'].max()\n",
    "    else:\n",
    "        logging.warning(\"'avg_road_accident_density' column missing. Defaulting to 0.\")\n",
    "        accident_density_max = 1\n",
    "    \n",
    "    for idx, row in neighborhoods_df.iterrows():\n",
    "        land_use_cols = [f\"land_use_{category.lower()}_percent\" for category in CATEGORY_PRIORITY.keys()]\n",
    "        land_use_values = [row.get(col, 0.0) / 100 for col in land_use_cols if col in row]\n",
    "        land_use_values = [v for v in land_use_values if v > 0]\n",
    "        if land_use_values:\n",
    "            entropy = -np.sum([p * np.log2(p) for p in land_use_values])\n",
    "            max_entropy = np.log2(len(land_use_values)) if len(land_use_values) > 0 else 1\n",
    "            land_use_diversity = entropy / max_entropy if max_entropy > 0 else 0\n",
    "        else:\n",
    "            land_use_diversity = 0\n",
    "        \n",
    "        ndvi_normalized = ((row['ndvi_mean'] - ndvi_min) / (ndvi_max - ndvi_min + 1e-6))\n",
    "        tree_normalized = ((row['tree_count'] - tree_min) / (tree_max - tree_min + 1e-6))\n",
    "        open_area = row.get('land_use_city_open_area_percent', 0.0) / 100\n",
    "        green_space_score = (ndvi_normalized + tree_normalized + open_area) / 3\n",
    "        \n",
    "        transit_raw = (row['transit_count'] - transit_min) / (transit_max - transit_min + 1e-6)\n",
    "        transit_score = np.log1p(transit_raw * 10) / np.log1p(10)\n",
    "        \n",
    "        intersection_density = row.get('intersection_density', 0.0)\n",
    "        intersection_density_normalized = (intersection_density - intersection_density_min) / (intersection_density_max - intersection_density_min + 1e-6)\n",
    "        road_connectivity = np.log1p(intersection_density_normalized * 10) / np.log1p(10)\n",
    "        \n",
    "        # Hybrid safety score: combine accident_count and avg_road_accident_density\n",
    "        accident_count_density = row['accident_count'] / row['area_km2'] if row['area_km2'] > 0 else 0\n",
    "        accident_count_density_max = (accident_count_max / neighborhoods_df['area_km2'].min()) if neighborhoods_df['area_km2'].min() > 0 else 1\n",
    "        accident_count_density = min(accident_count_density, accident_count_density_max * 0.5)  # Cap to reduce urban bias\n",
    "        safety_score_count = 1 - (accident_count_density / (accident_count_density_max + 1e-6))\n",
    "        \n",
    "        accident_density = row.get('avg_road_accident_density', 0.0)\n",
    "        safety_score_roads = 1 - (accident_density / (accident_density_max + 1e-6))\n",
    "        \n",
    "        # Combine both safety metrics (50% weight each)\n",
    "        safety_score = 0.5 * safety_score_count + 0.5 * safety_score_roads\n",
    "        \n",
    "        elderly_accessibility = row['elderly_percentage'] / 100\n",
    "        \n",
    "        base_score = (\n",
    "            0.3 * land_use_diversity +\n",
    "            0.3 * green_space_score +\n",
    "            0.2 * transit_score +\n",
    "            0.2 * road_connectivity\n",
    "        )\n",
    "        \n",
    "        safety_modifier = 0.3 + 0.7 * safety_score  # Increased weight for safety\n",
    "        elderly_modifier = 1 + elderly_accessibility\n",
    "        walkability_score = base_score * safety_modifier * elderly_modifier\n",
    "        \n",
    "        walkability_score = 1 / (1 + np.exp(-5 * (walkability_score - 1)))\n",
    "        \n",
    "        components['LIE_NAME'].append(row['LIE_NAME'])\n",
    "        components['land_use_diversity'].append(land_use_diversity)\n",
    "        components['green_space_score'].append(green_space_score)\n",
    "        components['transit_score'].append(transit_score)\n",
    "        components['road_connectivity'].append(road_connectivity)\n",
    "        components['safety_score'].append(safety_score)\n",
    "        components['elderly_accessibility'].append(elderly_accessibility)\n",
    "        components['walkability_score'].append(walkability_score)\n",
    "    \n",
    "    walkability_df = pd.DataFrame(components)\n",
    "    print(\"Walkability score distribution:\")\n",
    "    print(walkability_df['walkability_score'].describe())\n",
    "    \n",
    "    corr, p_value = pearsonr(walkability_df['walkability_score'], neighborhoods_df['transit_count'])\n",
    "    logging.info(f\"Correlation between walkability score and transit count: {corr:.2f} (p-value: {p_value:.2f})\")\n",
    "    \n",
    "    return walkability_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d0683",
   "metadata": {},
   "source": [
    "Cell 4 Main Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c3d3ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    logging.info(\"Stage 1: Loading and preparing data...\")\n",
    "    with tqdm(total=8, desc=\"Loading files\") as pbar:\n",
    "        neighborhoods_gdf = gpd.read_file(\n",
    "            LANDUSE_NDVI_PATH,\n",
    "            encoding='utf-8-sig',\n",
    "            columns=['LIE_NAME', 'geometry', 'land_use_residential_percent', 'land_use_commercial_percent',\n",
    "                     'land_use_education_percent', 'ndvi_mean']\n",
    "        ).to_crs('EPSG:3826')\n",
    "        total_rows = len(neighborhoods_gdf)\n",
    "        unique_names = neighborhoods_gdf['LIE_NAME'].nunique()\n",
    "        duplicates = neighborhoods_gdf['LIE_NAME'].duplicated().sum()\n",
    "        logging.info(f\"Loaded {total_rows} neighborhoods (expected 456)\")\n",
    "        logging.info(f\"Unique LIE_NAME values: {unique_names}\")\n",
    "        if duplicates > 0:\n",
    "            logging.warning(f\"Found {duplicates} duplicate LIE_NAME values:\")\n",
    "            duplicate_names = neighborhoods_gdf[neighborhoods_gdf['LIE_NAME'].duplicated(keep=False)][['LIE_NAME']].reset_index()\n",
    "            logging.info(f\"Duplicate LIE_NAME details:\\n{duplicate_names.to_string(index=False)}\")\n",
    "            raise ValueError(\"Duplicate LIE_NAME values detected. Please correct the data.\")\n",
    "        if unique_names != 456:\n",
    "            logging.error(f\"Expected 456 unique LIE_NAME values, found {unique_names}.\")\n",
    "            raise ValueError(\"LIE_NAME count mismatch.\")\n",
    "        neighborhoods_gdf = neighborhoods_gdf.reset_index(drop=True)\n",
    "        pbar.update(1)\n",
    "\n",
    "        buildings_gdf = gpd.read_file(OSM_BUILDINGS_PATH, columns=['geometry', 'building']).to_crs('EPSG:3826')\n",
    "        buildings_gdf['area_m2'] = buildings_gdf.geometry.area\n",
    "        pbar.update(1)\n",
    "\n",
    "        roads_gdf = gpd.read_parquet(OSM_ROADS_PATH, columns=['geometry', 'class']).to_crs('EPSG:3826')\n",
    "        roads_gdf['length_m'] = roads_gdf.geometry.length\n",
    "        logging.info(f\"Found {roads_gdf['class'].isnull().sum()} roads with missing 'class' values.\")\n",
    "        roads_gdf['class'] = roads_gdf['class'].fillna('unknown')\n",
    "        pbar.update(1)\n",
    "\n",
    "        trees_gdf = gpd.read_parquet(OSM_TREES_PATH, columns=['geometry', 'subtype', 'class']).to_crs('EPSG:3826')\n",
    "        trees_gdf = trees_gdf[trees_gdf['subtype'] == 'tree']\n",
    "        pbar.update(1)\n",
    "\n",
    "        transit_gdf = gpd.read_parquet(OSM_TRANSIT_PATH, columns=['geometry', 'class']).to_crs('EPSG:3826')\n",
    "        transit_gdf = transit_gdf[transit_gdf['class'].isin(['stop_position', 'bus_stop'])]\n",
    "        pbar.update(1)\n",
    "\n",
    "        urban_masterplan_gdf = gpd.read_file(URBAN_MASTERPLAN_PATH, columns=['geometry', 'Category']).to_crs('EPSG:3826')\n",
    "        if 'area' not in urban_masterplan_gdf.columns:\n",
    "            urban_masterplan_gdf['area'] = urban_masterplan_gdf.geometry.area\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Load accidents with severity if available\n",
    "        try:\n",
    "            accidents_gdf = gpd.read_file(ACCIDENTS_PATH, columns=['geometry', 'severity']).to_crs('EPSG:3826')\n",
    "            logging.info(\"Loaded accidents with severity column.\")\n",
    "        except ValueError:\n",
    "            accidents_gdf = gpd.read_file(ACCIDENTS_PATH, columns=['geometry']).to_crs('EPSG:3826')\n",
    "            logging.warning(\"No severity column in accidents data. Using count only.\")\n",
    "        pbar.update(1)\n",
    "\n",
    "        population_df = pd.read_json(POPULATION_PATH, encoding='utf-8')\n",
    "        population_df.rename(columns={'District': 'LIE_NAME', 'Total_Population': 'total_population',\n",
    "                                      'Elderly_Percentage': 'elderly_percentage'}, inplace=True)\n",
    "        pbar.update(1)\n",
    "\n",
    "    logging.info(\"Validating and fixing geometries...\")\n",
    "    neighborhoods_gdf['geometry'] = neighborhoods_gdf['geometry'].apply(fix_geometry)\n",
    "    urban_masterplan_gdf['geometry'] = urban_masterplan_gdf['geometry'].apply(fix_geometry)\n",
    "    roads_gdf['geometry'] = roads_gdf['geometry'].apply(fix_geometry)\n",
    "    accidents_gdf['geometry'] = accidents_gdf['geometry'].apply(fix_geometry)\n",
    "\n",
    "    invalid_neighborhoods = neighborhoods_gdf[~neighborhoods_gdf.geometry.is_valid]\n",
    "    if not invalid_neighborhoods.empty:\n",
    "        logging.warning(f\"Found {len(invalid_neighborhoods)} invalid geometries in neighborhoods_gdf.\")\n",
    "    invalid_masterplan = urban_masterplan_gdf[~urban_masterplan_gdf.geometry.is_valid]\n",
    "    if not invalid_masterplan.empty:\n",
    "        logging.warning(f\"Found {len(invalid_masterplan)} invalid geometries in urban_masterplan_gdf.\")\n",
    "    invalid_roads = roads_gdf[~roads_gdf.geometry.is_valid]\n",
    "    if not invalid_roads.empty:\n",
    "        logging.warning(f\"Found {len(invalid_roads)} invalid geometries in roads_gdf.\")\n",
    "    invalid_accidents = accidents_gdf[~accidents_gdf.geometry.is_valid]\n",
    "    if not invalid_accidents.empty:\n",
    "        logging.warning(f\"Found {len(invalid_accidents)} invalid geometries in accidents_gdf.\")\n",
    "\n",
    "    logging.info(\"Performing spatial joins and aggregations...\")\n",
    "    neighborhoods_gdf['area_km2'] = neighborhoods_gdf.geometry.area / 1e6\n",
    "\n",
    "    # Compute accident_count with border sharing\n",
    "    logging.info(\"Computing accident counts per neighborhood with border sharing...\")\n",
    "    # Step 1: Create a buffered version of neighborhood boundaries\n",
    "    buffer_distance = 10  # meters, adjustable\n",
    "    neighborhoods_buffered = neighborhoods_gdf.copy()\n",
    "    neighborhoods_buffered['geometry'] = neighborhoods_buffered['geometry'].boundary.buffer(buffer_distance)\n",
    "    \n",
    "    # Step 2: Assign accidents to neighborhoods (initial assignment)\n",
    "    accident_counts = gpd.sjoin(neighborhoods_gdf, accidents_gdf, how='left', predicate='contains')\n",
    "    accident_counts = accident_counts.groupby(level=0).size().reindex(neighborhoods_gdf.index, fill_value=0)\n",
    "    neighborhoods_gdf['accident_count'] = accident_counts\n",
    "    \n",
    "    # Step 3: Identify accidents near borders and share them\n",
    "    accidents_near_borders = gpd.sjoin(accidents_gdf, neighborhoods_buffered[['geometry', 'LIE_NAME']], how='left', predicate='intersects')\n",
    "    # Group by accident to see how many neighborhoods are near each accident\n",
    "    accidents_near_borders['weight'] = accidents_near_borders.groupby(accidents_near_borders.index).transform('size')\n",
    "    accidents_near_borders['weight'] = 1 / accidents_near_borders['weight']  # Equal sharing\n",
    "    # Aggregate shared counts\n",
    "    shared_counts = accidents_near_borders.groupby('LIE_NAME')['weight'].sum().reindex(neighborhoods_gdf['LIE_NAME'], fill_value=0)\n",
    "    neighborhoods_gdf['accident_count'] = neighborhoods_gdf['accident_count'] + shared_counts\n",
    "    neighborhoods_gdf['accident_count'] = neighborhoods_gdf['accident_count'].round().astype(int)\n",
    "\n",
    "    tree_counts = gpd.sjoin(neighborhoods_gdf, trees_gdf, how='left', predicate='contains')\n",
    "    tree_counts = tree_counts.groupby(level=0).size().reindex(neighborhoods_gdf.index, fill_value=0)\n",
    "    neighborhoods_gdf['tree_count'] = tree_counts\n",
    "\n",
    "    transit_counts = gpd.sjoin(neighborhoods_gdf, transit_gdf, how='left', predicate='contains')\n",
    "    transit_counts = transit_counts.groupby(level=0).size().reindex(neighborhoods_gdf.index, fill_value=0)\n",
    "    neighborhoods_gdf['transit_count'] = transit_counts\n",
    "\n",
    "    road_lengths = gpd.sjoin(roads_gdf, neighborhoods_gdf, how='left', predicate='intersects')\n",
    "    road_lengths = road_lengths.groupby('index_right')['length_m'].sum().reindex(neighborhoods_gdf.index, fill_value=0)\n",
    "    neighborhoods_gdf['road_density'] = road_lengths / (neighborhoods_gdf['area_km2'] * 1000)\n",
    "\n",
    "    logging.info(\"Computing intersection counts...\")\n",
    "    neighborhoods_gdf = compute_intersection_counts(neighborhoods_gdf, roads_gdf)\n",
    "\n",
    "    neighborhoods_gdf = neighborhoods_gdf.merge(\n",
    "        population_df[['LIE_NAME', 'total_population', 'elderly_percentage']],\n",
    "        on='LIE_NAME',\n",
    "        how='left'\n",
    "    )\n",
    "    neighborhoods_gdf['total_population'] = neighborhoods_gdf['total_population'].fillna(0)\n",
    "    neighborhoods_gdf['elderly_percentage'] = neighborhoods_gdf['elderly_percentage'].fillna(0)\n",
    "\n",
    "    logging.info(\"Computing land use percentages...\")\n",
    "    temp_gdf = gpd.GeoDataFrame({'geometry': neighborhoods_gdf['geometry']}, crs='EPSG:3826')\n",
    "    intersected = gpd.overlay(temp_gdf, urban_masterplan_gdf, how='intersection', keep_geom_type=False)\n",
    "    intersected['geometry'] = intersected['geometry'].apply(fix_geometry)\n",
    "    intersected = intersected[intersected.geometry.is_valid & ~intersected.geometry.is_empty]\n",
    "    \n",
    "    if not intersected.empty:\n",
    "        intersected['priority'] = intersected['Category'].map(CATEGORY_PRIORITY)\n",
    "        intersected = intersected.sort_values(by='priority', ascending=False)\n",
    "        intersected['area'] = intersected.geometry.area\n",
    "        \n",
    "        for category in CATEGORY_PRIORITY.keys():\n",
    "            col_name = f\"land_use_{category.lower()}_percent\"\n",
    "            category_areas = intersected[intersected['Category'] == category].groupby(level=0)['area'].sum()\n",
    "            total_areas = intersected.groupby(level=0)['area'].sum()\n",
    "            percentages = (category_areas / total_areas * 100).reindex(neighborhoods_gdf.index, fill_value=0.0)\n",
    "            neighborhoods_gdf[col_name] = percentages\n",
    "    else:\n",
    "        for category in CATEGORY_PRIORITY.keys():\n",
    "            col_name = f\"land_use_{category.lower()}_percent\"\n",
    "            neighborhoods_gdf[col_name] = 0.0\n",
    "\n",
    "    # Clustering-based imputation for missing land use percentages\n",
    "    logging.info(\"Imputing missing land use percentages using clustering...\")\n",
    "    from sklearn.cluster import KMeans\n",
    "    cluster_features = neighborhoods_gdf[['ndvi_mean', 'total_population']].fillna(0)\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42).fit(cluster_features)\n",
    "    neighborhoods_gdf['cluster'] = kmeans.labels_\n",
    "    for category in CATEGORY_PRIORITY.keys():\n",
    "        col_name = f\"land_use_{category.lower()}_percent\"\n",
    "        medians = neighborhoods_gdf.groupby('cluster')[col_name].median().fillna(0)\n",
    "        neighborhoods_gdf[col_name] = neighborhoods_gdf.apply(\n",
    "            lambda row: medians[row['cluster']] if pd.isna(row[col_name]) or row[col_name] == 0.0 else row[col_name], axis=1)\n",
    "    neighborhoods_gdf.drop('cluster', axis=1, inplace=True)\n",
    "    logging.info(\"Clustering-based imputation completed.\")\n",
    "\n",
    "    print_data_structure({\n",
    "        'neighborhoods': neighborhoods_gdf,\n",
    "        'buildings': buildings_gdf,\n",
    "        'roads': roads_gdf,\n",
    "        'trees': trees_gdf,\n",
    "        'transit': transit_gdf,\n",
    "        'urban_masterplan': urban_masterplan_gdf,\n",
    "        'accidents': accidents_gdf,\n",
    "        'population': population_df\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        'neighborhoods': neighborhoods_gdf,\n",
    "        'buildings': buildings_gdf,\n",
    "        'roads': roads_gdf,\n",
    "        'trees': trees_gdf,\n",
    "        'transit': transit_gdf,\n",
    "        'urban_masterplan': urban_masterplan_gdf,\n",
    "        'accidents': accidents_gdf,\n",
    "        'population': population_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48993e82",
   "metadata": {},
   "source": [
    "Cell 5 compute_intersection_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d2d1971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_intersection_counts(neighborhoods_gdf, roads_gdf):\n",
    "    logging.info(\"Computing intersection counts per neighborhood...\")\n",
    "    if CUSPATIAL_AVAILABLE:\n",
    "        logging.info(\"Using cuspatial for intersection counts...\")\n",
    "        try:\n",
    "            road_points = roads_gdf.geometry.apply(lambda x: x.boundary).explode(index_parts=True)\n",
    "            road_points = road_points[road_points.geom_type == 'Point']\n",
    "            points = cuspatial.GeoSeries(road_points)\n",
    "            polys = cuspatial.GeoSeries(neighborhoods_gdf.geometry)\n",
    "            hits = cuspatial.points_in_polygon(points, polys)\n",
    "            intersection_counts = hits.sum(axis=1).reindex(neighborhoods_gdf.index, fill_value=0)\n",
    "            neighborhoods_gdf['intersection_count'] = intersection_counts\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"CuSpatial failed: {e}. Falling back to geopandas...\")\n",
    "            endpoint_to_roads = {}\n",
    "            for road_idx, geom in tqdm(roads_gdf.geometry.items(), total=len(roads_gdf), desc=\"Building endpoint-to-road mapping WIP\"):\n",
    "                if geom.geom_type == 'LineString':\n",
    "                    start = geom.coords[0]\n",
    "                    end = geom.coords[-1]\n",
    "                    endpoint_to_roads.setdefault(start, []).append(road_idx)\n",
    "                    endpoint_to_roads.setdefault(end, []).append(road_idx)\n",
    "                elif geom.geom_type == 'MultiLineString':\n",
    "                    for line in geom.geoms:\n",
    "                        start = line.coords[0]\n",
    "                        end = line.coords[-1]\n",
    "                        endpoint_to_roads.setdefault(start, []).append(road_idx)\n",
    "                        endpoint_to_roads.setdefault(end, []).append(road_idx)\n",
    "            \n",
    "            intersections = []\n",
    "            for node in tqdm(endpoint_to_roads.keys(), desc=\"Identifying intersections\"):\n",
    "                if len(endpoint_to_roads[node]) > 2:\n",
    "                    intersections.append(Point(node))\n",
    "            \n",
    "            intersections_gdf = gpd.GeoDataFrame(geometry=intersections, crs='EPSG:3826')\n",
    "            intersection_counts = gpd.sjoin(neighborhoods_gdf, intersections_gdf, how='left', predicate='contains')\n",
    "            intersection_counts = intersection_counts.groupby(level=0).size().reindex(neighborhoods_gdf.index, fill_value=0)\n",
    "            neighborhoods_gdf['intersection_count'] = intersection_counts\n",
    "    else:\n",
    "        logging.info(\"Using geopandas for intersection counts...\")\n",
    "        endpoint_to_roads = {}\n",
    "        for road_idx, geom in tqdm(roads_gdf.geometry.items(), total=len(roads_gdf), desc=\"Building endpoint-to-road mapping\"):\n",
    "            if geom.geom_type == 'LineString':\n",
    "                start = geom.coords[0]\n",
    "                end = geom.coords[-1]\n",
    "                endpoint_to_roads.setdefault(start, []).append(road_idx)\n",
    "                endpoint_to_roads.setdefault(end, []).append(road_idx)\n",
    "            elif geom.geom_type == 'MultiLineString':\n",
    "                for line in geom.geoms:\n",
    "                    start = line.coords[0]\n",
    "                    end = line.coords[-1]\n",
    "                    endpoint_to_roads.setdefault(start, []).append(road_idx)\n",
    "                    endpoint_to_roads.setdefault(end, []).append(road_idx)\n",
    "        \n",
    "        intersections = []\n",
    "        for node in tqdm(endpoint_to_roads.keys(), desc=\"Identifying intersections\"):\n",
    "            if len(endpoint_to_roads[node]) > 2:\n",
    "                intersections.append(Point(node))\n",
    "        \n",
    "        intersections_gdf = gpd.GeoDataFrame(geometry=intersections, crs='EPSG:3826')\n",
    "        intersection_counts = gpd.sjoin(neighborhoods_gdf, intersections_gdf, how='left', predicate='contains')\n",
    "        intersection_counts = intersection_counts.groupby(level=0).size().reindex(neighborhoods_gdf.index, fill_value=0)\n",
    "        neighborhoods_gdf['intersection_count'] = intersection_counts\n",
    "\n",
    "    neighborhoods_gdf['intersection_density'] = neighborhoods_gdf['intersection_count'] / neighborhoods_gdf['area_km2']\n",
    "    neighborhoods_gdf['intersection_density'] = neighborhoods_gdf['intersection_density'].fillna(0.0)\n",
    "    logging.info(\"Intersection counts completed.\")\n",
    "    return neighborhoods_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b04d9d",
   "metadata": {},
   "source": [
    "Cell 6: Graph Construction (build_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4158e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(data, force_recompute=False):\n",
    "    logging.info(\"Stage 2: Building city graph...\")\n",
    "    G = cugraph.Graph(directed=False)\n",
    "    \n",
    "    current_hash = compute_data_hash(data)\n",
    "    \n",
    "    if not force_recompute and os.path.exists(GRAPH_NODES_CACHE_PATH) and os.path.exists(GRAPH_EDGES_CACHE_PATH) and os.path.exists(GRAPH_DATA_HASH_PATH):\n",
    "        with open(GRAPH_DATA_HASH_PATH, 'r') as f:\n",
    "            cached_hash = f.read().strip()\n",
    "        if cached_hash == current_hash:\n",
    "            logging.info(\"Loading graph from cache...\")\n",
    "            try:\n",
    "                nodes_df = cudf.read_parquet(GRAPH_NODES_CACHE_PATH)\n",
    "                edges_df = cudf.read_parquet(GRAPH_EDGES_CACHE_PATH)\n",
    "                G._nodes = nodes_df\n",
    "                if not edges_df.empty:\n",
    "                    G.from_cudf_edgelist(edges_df, source='src', destination='dst')\n",
    "                logging.info(f\"City graph loaded from cache: {len(nodes_df)} nodes, {len(edges_df)} edges\")\n",
    "                return G\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Failed to load cached graph: {e}. Recomputing...\")\n",
    "    \n",
    "    nodes = []\n",
    "    edges = []\n",
    "    node_id_to_index = {}\n",
    "    current_idx = 0\n",
    "    \n",
    "    logging.info(\"Adding neighborhood nodes...\")\n",
    "    neighborhoods_gdf = data['neighborhoods']\n",
    "    logging.info(f\"Processing {len(neighborhoods_gdf)} neighborhoods (expected 456)\")\n",
    "    if len(neighborhoods_gdf) != 456 or neighborhoods_gdf['LIE_NAME'].nunique() != 456:\n",
    "        logging.error(f\"Neighborhood count mismatch: {len(neighborhoods_gdf)} rows, {neighborhoods_gdf['LIE_NAME'].nunique()} unique names.\")\n",
    "        raise ValueError(\"Expected 456 unique neighborhoods.\")\n",
    "    for idx, row in neighborhoods_gdf.iterrows():\n",
    "        node_id = f\"neighborhood_{idx}\"\n",
    "        node_data = {\n",
    "            'vertex': node_id,\n",
    "            'type': 'neighborhood',\n",
    "            'LIE_NAME': row['LIE_NAME'],\n",
    "            'ndvi_mean': row['ndvi_mean'],\n",
    "            'tree_count': row['tree_count'],\n",
    "            'transit_count': row['transit_count'],\n",
    "            'accident_count': row['accident_count'],\n",
    "            'road_density': row['road_density'],\n",
    "            'intersection_count': row.get('intersection_count', 0),\n",
    "            'intersection_density': row.get('intersection_density', 0.0),\n",
    "            'total_population': row['total_population'],\n",
    "            'elderly_percentage': row['elderly_percentage'],\n",
    "            'area_km2': row['area_km2']\n",
    "        }\n",
    "        for category in CATEGORY_PRIORITY.keys():\n",
    "            col_name = f\"land_use_{category.lower()}_percent\"\n",
    "            node_data[col_name] = row.get(col_name, 0.0)\n",
    "        \n",
    "        nodes.append(node_data)\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        current_idx += 1\n",
    "    logging.info(f\"Added {len(nodes)} neighborhood nodes\")\n",
    "    \n",
    "    logging.info(\"Adding building nodes...\")\n",
    "    buildings_gdf = data['buildings']\n",
    "    for idx, row in buildings_gdf.iterrows():\n",
    "        node_id = f\"building_{idx}\"\n",
    "        node_data = {\n",
    "            'vertex': node_id,\n",
    "            'type': 'building',\n",
    "            'building': row['building'] if pd.notna(row['building']) else 'unknown',\n",
    "            'area_m2': row['area_m2']\n",
    "        }\n",
    "        nodes.append(node_data)\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        current_idx += 1\n",
    "    \n",
    "    logging.info(\"Adding road nodes...\")\n",
    "    roads_gdf = data['roads']\n",
    "    for idx, row in roads_gdf.iterrows():\n",
    "        node_id = f\"road_{idx}\"\n",
    "        node_data = {\n",
    "            'vertex': node_id,\n",
    "            'type': 'road',\n",
    "            'class': row['class'],\n",
    "            'length_m': row['length_m']\n",
    "        }\n",
    "        nodes.append(node_data)\n",
    "        node_id_to_index[node_id] = current_idx\n",
    "        current_idx += 1\n",
    "    \n",
    "    nodes_df = cudf.DataFrame(nodes)\n",
    "    logging.info(f\"Created {len(nodes_df)} nodes, including {len(nodes_df[nodes_df['type'] == 'neighborhood'])} neighborhoods\")\n",
    "    \n",
    "    logging.info(\"Building edges based on spatial proximity...\")\n",
    "    neighborhoods_gdf = neighborhoods_gdf.copy()\n",
    "    buildings_gdf = buildings_gdf.copy()\n",
    "    roads_gdf = data['roads'].copy()\n",
    "    \n",
    "    neighborhoods_gdf['geometry'] = neighborhoods_gdf['geometry'].apply(fix_geometry)\n",
    "    buildings_gdf['geometry'] = buildings_gdf['geometry'].apply(fix_geometry)\n",
    "    roads_gdf['geometry'] = roads_gdf['geometry'].apply(fix_geometry)\n",
    "    \n",
    "    neighborhood_sindex = neighborhoods_gdf.sindex\n",
    "    building_sindex = buildings_gdf.sindex\n",
    "    road_sindex = roads_gdf.sindex\n",
    "    \n",
    "    logging.info(\"Computing neighborhood-neighborhood edges...\")\n",
    "    for i, row_i in tqdm(neighborhoods_gdf.iterrows(), total=len(neighborhoods_gdf), desc=\"Neighborhood-neighborhood edges\"):\n",
    "        node_i = f\"neighborhood_{i}\"\n",
    "        geom_i = row_i['geometry']\n",
    "        possible_matches = list(neighborhood_sindex.query(geom_i, predicate='touches'))\n",
    "        for j in possible_matches:\n",
    "            if i < j:\n",
    "                node_j = f\"neighborhood_{j}\"\n",
    "                geom_j = neighborhoods_gdf.iloc[j]['geometry']\n",
    "                try:\n",
    "                    if geom_i.touches(geom_j):\n",
    "                        edges.append({'src': node_i, 'dst': node_j})\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error checking touches between {node_i} and {node_j}: {e}\")\n",
    "    \n",
    "    logging.info(\"Computing neighborhood-building edges...\")\n",
    "    if CUSPATIAL_AVAILABLE:\n",
    "        logging.info(\"Using cuspatial for neighborhood-building edges...\")\n",
    "        building_points = buildings_gdf['geometry'].centroid\n",
    "        building_cudf = cuspatial.GeoSeries(building_points)\n",
    "        batch_size = 31  # cuSpatial limit\n",
    "        for start_idx in tqdm(range(0, len(neighborhoods_gdf), batch_size), desc=\"Neighborhood-building edges (cuspatial)\"):\n",
    "            end_idx = min(start_idx + batch_size, len(neighborhoods_gdf))\n",
    "            batch_gdf = neighborhoods_gdf.iloc[start_idx:end_idx]\n",
    "            poly = cuspatial.GeoSeries(batch_gdf['geometry'])\n",
    "            try:\n",
    "                hits = cuspatial.point_in_polygon(building_cudf, poly)\n",
    "                for i, idx in enumerate(batch_gdf.index):\n",
    "                    hit_indices = hits[hits[i]].index.to_pandas()\n",
    "                    node_i = f\"neighborhood_{idx}\"\n",
    "                    for j in hit_indices:\n",
    "                        edges.append({'src': node_i, 'dst': f\"building_{j}\"})\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"cuspatial error for batch {start_idx}-{end_idx}: {e}. Falling back to geopandas...\")\n",
    "                for idx in batch_gdf.index:\n",
    "                    node_i = f\"neighborhood_{idx}\"\n",
    "                    geom_i = batch_gdf.loc[idx, 'geometry']\n",
    "                    possible_matches = list(building_sindex.query(geom_i, predicate='contains'))\n",
    "                    for j in possible_matches:\n",
    "                        node_j = f\"building_{j}\"\n",
    "                        geom_j = buildings_gdf.iloc[j]['geometry']\n",
    "                        try:\n",
    "                            if geom_i.contains(geom_j):\n",
    "                                edges.append({'src': node_i, 'dst': node_j})\n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"Error checking containment between {node_i} and {node_j}: {e}\")\n",
    "    else:\n",
    "        logging.info(\"Using geopandas for neighborhood-building edges...\")\n",
    "        for i, row_i in tqdm(neighborhoods_gdf.iterrows(), total=len(neighborhoods_gdf), desc=\"Neighborhood-building edges\"):\n",
    "            node_i = f\"neighborhood_{i}\"\n",
    "            geom_i = row_i['geometry']\n",
    "            possible_matches = list(building_sindex.query(geom_i, predicate='contains'))\n",
    "            for j in possible_matches:\n",
    "                node_j = f\"building_{j}\"\n",
    "                geom_j = buildings_gdf.iloc[j]['geometry']\n",
    "                try:\n",
    "                    if geom_i.contains(geom_j):\n",
    "                        edges.append({'src': node_i, 'dst': node_j})\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error checking containment between {node_i} and {node_j}: {e}\")\n",
    "    \n",
    "    logging.info(\"Computing neighborhood-road edges...\")\n",
    "    for i, row_i in tqdm(neighborhoods_gdf.iterrows(), total=len(neighborhoods_gdf), desc=\"Neighborhood-road edges\"):\n",
    "        node_i = f\"neighborhood_{i}\"\n",
    "        geom_i = row_i['geometry']\n",
    "        possible_matches = list(road_sindex.query(geom_i, predicate='intersects'))\n",
    "        for j in possible_matches:\n",
    "            node_j = f\"road_{j}\"\n",
    "            geom_j = roads_gdf.iloc[j]['geometry']\n",
    "            try:\n",
    "                if geom_i.intersects(geom_j):\n",
    "                    edges.append({'src': node_i, 'dst': node_j})\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error checking intersection between {node_i} and {node_j}: {e}\")\n",
    "    \n",
    "    edges_df = cudf.DataFrame(edges)\n",
    "    \n",
    "    valid_nodes = set(nodes_df['vertex'].to_pandas())\n",
    "    edges_df = edges_df[edges_df['src'].isin(valid_nodes) & edges_df['dst'].isin(valid_nodes)]\n",
    "    \n",
    "    G._nodes = nodes_df\n",
    "    if not edges_df.empty:\n",
    "        G.from_cudf_edgelist(edges_df, source='src', destination='dst')\n",
    "    else:\n",
    "        logging.warning(\"No valid edges created. Graph will have nodes but no edges.\")\n",
    "    \n",
    "    logging.info(\"Saving graph data to cache...\")\n",
    "    try:\n",
    "        nodes_df.to_parquet(GRAPH_NODES_CACHE_PATH)\n",
    "        edges_df.to_parquet(GRAPH_EDGES_CACHE_PATH)\n",
    "        with open(GRAPH_DATA_HASH_PATH, 'w') as f:\n",
    "            f.write(current_hash)\n",
    "        with open(GRAPH_NODE_ID_CACHE_PATH, 'w') as f:\n",
    "            json.dump(node_id_to_index, f)\n",
    "        logging.info(\"Successfully saved graph data to cache.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save graph data to cache: {e}\")\n",
    "    \n",
    "    logging.info(f\"City graph constructed: {len(nodes_df)} nodes, {len(edges_df)} edges\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14e080",
   "metadata": {},
   "source": [
    "Cell 7: Rule-Based Walkability Scores (compute_walkability_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3ef4aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_walkability_scores(G):\n",
    "    logging.info(\"Stage 3: Calculating rule-based walkability scores...\")\n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    \n",
    "    neighborhood_nodes = nodes_df[nodes_df['type'] == 'neighborhood']\n",
    "    \n",
    "    walkability_components = compute_walkability_components_all(neighborhood_nodes)\n",
    "    \n",
    "    nodes_df['walkability_score'] = np.nan\n",
    "    \n",
    "    lie_name_to_score = {}\n",
    "    for _, row in walkability_components.iterrows():\n",
    "        lie_name = row['LIE_NAME']\n",
    "        score = row['walkability_score']\n",
    "        lie_name_to_score.setdefault(lie_name, []).append(score)\n",
    "    \n",
    "    for lie_name, scores in lie_name_to_score.items():\n",
    "        mask = nodes_df['LIE_NAME'] == lie_name\n",
    "        if mask.any():\n",
    "            nodes_df.loc[mask, 'walkability_score'] = np.mean(scores)\n",
    "        else:\n",
    "            logging.warning(f\"No node found for LIE_NAME {lie_name} when assigning walkability score.\")\n",
    "    \n",
    "    G._nodes = cudf.from_pandas(nodes_df)\n",
    "    \n",
    "    logging.info(\"Rule-based walkability scores calculated and added to graph.\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7022dd9e",
   "metadata": {},
   "source": [
    "Cell 8:Subgraph Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989dbf84",
   "metadata": {},
   "source": [
    "Cell 9 prepare_gnn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8a057108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_gnn_data(G):\n",
    "    logging.info(\"Preparing GNN data...\")\n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    edges_df = G.edgelist.edgelist_df.to_pandas()\n",
    "    \n",
    "    try:\n",
    "        with open(GRAPH_NODE_ID_CACHE_PATH, 'r') as f:\n",
    "            node_id_to_index = json.load(f)\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to load node_id_to_index: {e}. Rebuilding mapping...\")\n",
    "        node_id_to_index = {row['vertex']: idx for idx, row in nodes_df.iterrows()}\n",
    "    \n",
    "    feature_cols = [\n",
    "        'land_use_residential_percent', 'land_use_commercial_percent', 'land_use_education_percent',\n",
    "        'land_use_city_open_area_percent', 'land_use_public_transportation_percent', 'land_use_pedestrian_percent',\n",
    "        'ndvi_mean', 'tree_count', 'transit_count', 'accident_count', 'road_density', 'intersection_density',\n",
    "        'total_population', 'elderly_percentage'\n",
    "    ]\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        if col not in nodes_df.columns:\n",
    "            nodes_df[col] = 0.0\n",
    "        else:\n",
    "            nodes_df[col] = nodes_df[col].fillna(0.0)\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        if nodes_df[col].std() > 0:\n",
    "            nodes_df[col] = (nodes_df[col] - nodes_df[col].mean()) / nodes_df[col].std()\n",
    "    \n",
    "    feature_matrix = nodes_df[feature_cols].values\n",
    "    feature_matrix = np.nan_to_num(feature_matrix, nan=0.0)\n",
    "    \n",
    "    edges_df['src_idx'] = edges_df['src'].map(node_id_to_index)\n",
    "    edges_df['dst_idx'] = edges_df['dst'].map(node_id_to_index)\n",
    "    \n",
    "    edges_df = edges_df.dropna(subset=['src_idx', 'dst_idx'])\n",
    "    \n",
    "    edges_df['src_idx'] = edges_df['src_idx'].astype(int)\n",
    "    edges_df['dst_idx'] = edges_df['dst_idx'].astype(int)\n",
    "    \n",
    "    edge_index = torch.tensor(\n",
    "        [edges_df['src_idx'].values, edges_df['dst_idx'].values],\n",
    "        dtype=torch.long\n",
    "    )\n",
    "    \n",
    "    edge_attr = None\n",
    "    if 'weight' in edges_df.columns:\n",
    "        edge_attr = torch.tensor(edges_df['weight'].values, dtype=torch.float)\n",
    "    \n",
    "    y = torch.tensor(nodes_df['walkability_score'].fillna(0.0).values, dtype=torch.float)\n",
    "    \n",
    "    node_type_mapping = {\n",
    "        'neighborhood': 0,\n",
    "        'building': 1,\n",
    "        'road': 2,\n",
    "        'tree': 3,\n",
    "        'transit': 4\n",
    "    }\n",
    "    node_type = nodes_df['type'].map(node_type_mapping).fillna(-1).astype(int).values\n",
    "    node_type = torch.tensor(node_type, dtype=torch.long)\n",
    "    \n",
    "    data = Data(\n",
    "        x=torch.tensor(feature_matrix, dtype=torch.float),\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=y,\n",
    "        node_type=node_type\n",
    "    )\n",
    "    \n",
    "    logging.info(\"GNN data prepared.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a9a6b5",
   "metadata": {},
   "source": [
    "Cell 10: WalkabilityGNN, train_gnn_model, predict_walkability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "635ecc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WalkabilityGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
    "        super(WalkabilityGNN, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        x = self.conv1(x, edge_index, edge_attr).relu()\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        return x\n",
    "\n",
    "def train_gnn_model(data):\n",
    "    logging.info(\"Stage 4: Training GNN model...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = WalkabilityGNN(\n",
    "        in_channels=data.x.shape[1],\n",
    "        hidden_channels=128,\n",
    "        out_channels=1,\n",
    "        heads=4\n",
    "    ).to(device)\n",
    "    data = data.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(200):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr).squeeze()\n",
    "        mask = data.node_type == 0\n",
    "        loss = F.mse_loss(out[mask], data.y[mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        if epoch % 10 == 0:\n",
    "            logging.info(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    logging.info(\"Finished training GNN model.\")\n",
    "    return model\n",
    "\n",
    "def predict_walkability(G, model):\n",
    "    logging.info(\"Predicting walkability with GNN...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    data = prepare_gnn_data(G)\n",
    "    data = data.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(data.x, data.edge_index, data.edge_attr).squeeze()\n",
    "\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    predictions = 1 / (1 + np.exp(-predictions))\n",
    "\n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    nodes_df['walkability_gnn'] = predictions\n",
    "    G._nodes = cudf.from_pandas(nodes_df)\n",
    "\n",
    "    logging.info(\"Finished predicting walkability with GNN.\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5406821f",
   "metadata": {},
   "source": [
    "Cell 11: Interactive Map Generation (create_interactive_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9d059c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_map(G, data):\n",
    "    logging.info(\"Generating interactive Kepler.gl map...\")\n",
    "    nodes_df = G._nodes.to_pandas()\n",
    "    neighborhoods_gdf = data['neighborhoods'].copy()\n",
    "\n",
    "    nodes_df['LIE_NAME'] = nodes_df['LIE_NAME'].astype(str).str.strip()\n",
    "    neighborhoods_gdf['LIE_NAME'] = neighborhoods_gdf['LIE_NAME'].astype(str).str.strip()\n",
    "\n",
    "    neighborhood_nodes = nodes_df[nodes_df['type'] == 'neighborhood'].copy()\n",
    "\n",
    "    nodes_lie_names = set(neighborhood_nodes['LIE_NAME'])\n",
    "    gdf_lie_names = set(neighborhoods_gdf['LIE_NAME'])\n",
    "    logging.info(f\"Neighborhood nodes count: {len(neighborhood_nodes)}\")\n",
    "    logging.info(f\"Neighborhoods_gdf count: {len(neighborhoods_gdf)}\")\n",
    "    logging.info(f\"Sample LIE_NAME in nodes_df: {list(nodes_lie_names)[:5]}\")\n",
    "    logging.info(f\"Sample LIE_NAME in neighborhoods_gdf: {list(gdf_lie_names)[:5]}\")\n",
    "    logging.info(f\"Common LIE_NAMEs: {len(nodes_lie_names & gdf_lie_names)}\")\n",
    "    logging.info(f\"Nodes LIE_NAMEs not in GDF: {list(nodes_lie_names - gdf_lie_names)}\")\n",
    "    logging.info(f\"GDF LIE_NAMEs not in nodes: {list(gdf_lie_names - nodes_lie_names)}\")\n",
    "    logging.info(f\"Nodes nulls: {neighborhood_nodes.isna().sum().to_dict()}\")\n",
    "    logging.info(f\"GDF geometry nulls: {neighborhoods_gdf['geometry'].isna().sum()}\")\n",
    "\n",
    "    map_data = neighborhoods_gdf[['LIE_NAME', 'geometry']].merge(\n",
    "        neighborhood_nodes[['LIE_NAME', 'walkability_score', 'walkability_gnn']],\n",
    "        on='LIE_NAME',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    map_data = map_data.drop_duplicates(subset=['LIE_NAME'], keep='first')\n",
    "\n",
    "    logging.info(f\"Merged map_data rows: {len(map_data)}\")\n",
    "    logging.info(f\"Walkability score nulls: {map_data['walkability_score'].isna().sum()}\")\n",
    "    logging.info(f\"Walkability GNN nulls: {map_data['walkability_gnn'].isna().sum()}\")\n",
    "\n",
    "    map_data['walkability_score'] = map_data['walkability_score'].fillna(0)\n",
    "    map_data['walkability_gnn'] = map_data['walkability_gnn'].fillna(0)\n",
    "\n",
    "    map_data = gpd.GeoDataFrame(map_data, geometry='geometry', crs='EPSG:3826')\n",
    "\n",
    "    map_data['geometry'] = map_data['geometry'].to_crs('EPSG:4326')\n",
    "    kepler_data = {\n",
    "        'neighborhoods': map_data[['LIE_NAME', 'walkability_score', 'walkability_gnn', 'geometry']].to_json()\n",
    "    }\n",
    "\n",
    "    config = {\n",
    "        \"version\": \"v1\",\n",
    "        \"config\": {\n",
    "            \"visState\": {\n",
    "                \"layers\": [\n",
    "                    {\n",
    "                        \"id\": \"neighborhoods\",\n",
    "                        \"type\": \"geojson\",\n",
    "                        \"config\": {\n",
    "                            \"dataId\": \"neighborhoods\",\n",
    "                            \"label\": \"Neighborhoods\",\n",
    "                            \"color\": [18, 147, 154],\n",
    "                            \"columns\": {\n",
    "                                \"geojson\": \"geometry\"\n",
    "                            },\n",
    "                            \"isVisible\": True,\n",
    "                            \"visConfig\": {\n",
    "                                \"opacity\": 0.7,\n",
    "                                \"strokeOpacity\": 0.9,\n",
    "                                \"thickness\": 1,\n",
    "                                \"strokeColor\": [255, 255, 255],\n",
    "                                \"colorRange\": {\n",
    "                                    \"name\": \"Global Warming\",\n",
    "                                    \"type\": \"sequential\",\n",
    "                                    \"colors\": [\n",
    "                                        \"#5A1846\", \"#900C3F\", \"#C70039\",\n",
    "                                        \"#E3611C\", \"#F1920E\", \"#FFC107\"\n",
    "                                    ]\n",
    "                                },\n",
    "                                \"strokeColorRange\": {\n",
    "                                    \"name\": \"Global Warming\",\n",
    "                                    \"type\": \"sequential\",\n",
    "                                    \"colors\": [\n",
    "                                        \"#5A1846\", \"#900C3F\", \"#C70039\",\n",
    "                                        \"#E3611C\", \"#F1920E\", \"#FFC107\"\n",
    "                                    ]\n",
    "                                },\n",
    "                                \"colorField\": {\n",
    "                                    \"name\": \"walkability_gnn\",\n",
    "                                    \"type\": \"real\"\n",
    "                                },\n",
    "                                \"colorScale\": \"quantile\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"visualChannels\": {\n",
    "                            \"colorField\": {\n",
    "                                \"name\": \"walkability_gnn\",\n",
    "                                \"type\": \"real\"\n",
    "                            },\n",
    "                            \"colorScale\": \"quantile\"\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"interactionConfig\": {\n",
    "                    \"tooltip\": {\n",
    "                        \"fieldsToShow\": {\n",
    "                            \"neighborhoods\": [\n",
    "                                {\"name\": \"LIE_NAME\", \"format\": None},\n",
    "                                {\"name\": \"walkability_score\", \"format\": \"{:.3f}\"},\n",
    "                                {\"name\": \"walkability_gnn\", \"format\": \"{:.3f}\"}\n",
    "                            ]\n",
    "                        },\n",
    "                        \"enabled\": True\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"mapState\": {\n",
    "                \"latitude\": 25.0330,\n",
    "                \"longitude\": 121.5654,\n",
    "                \"zoom\": 11\n",
    "            },\n",
    "            \"mapStyle\": {\n",
    "                \"styleType\": \"dark\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    map_1 = KeplerGl(height=800, data=kepler_data, config=config)\n",
    "    map_path = os.path.join(BASE_DIR, 'taipei_walkability_map.html')\n",
    "    map_1.save_to_html(file_name=map_path)\n",
    "    logging.info(f\"Interactive map generated and saved as {map_path}\")\n",
    "    print(f\"Map saved to {map_path}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b0683d",
   "metadata": {},
   "source": [
    "Cell 12: Main Execution (main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f13b11d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 18:14:31,494 - INFO - Ensured subgraph directory exists: /home/johnny/Iaacthesis/projects/Geojson/GNN_Read_data/subgraphs\n",
      "2025-04-17 18:14:31,495 - INFO - Stage 1: Loading and preparing data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting load_and_prepare_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:   0%|          | 0/8 [00:00<?, ?it/s]2025-04-17 18:14:31,581 - INFO - Loaded 456 neighborhoods (expected 456)\n",
      "2025-04-17 18:14:31,581 - INFO - Unique LIE_NAME values: 456\n",
      "Loading files:  25%|██▌       | 2/8 [00:01<00:03,  1.56it/s]2025-04-17 18:14:33,895 - INFO - Found 604 roads with missing 'class' values.\n",
      "Loading files:  75%|███████▌  | 6/8 [00:03<00:00,  2.01it/s]2025-04-17 18:14:35,153 - INFO - Loaded accidents with severity column.\n",
      "Loading files: 100%|██████████| 8/8 [00:03<00:00,  2.19it/s]\n",
      "2025-04-17 18:14:35,158 - INFO - Validating and fixing geometries...\n",
      "2025-04-17 18:14:36,336 - INFO - Performing spatial joins and aggregations...\n",
      "2025-04-17 18:14:36,337 - INFO - Computing accident counts per neighborhood with border sharing...\n"
     ]
    },
    {
     "ename": "IntCastingNaNError",
     "evalue": "Cannot convert non-finite values (NA or inf) to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIntCastingNaNError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28mprint\u001b[39m(G.edgelist.edgelist_df.to_pandas().head())\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforce_recompute_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(force_recompute_graph)\u001b[39m\n\u001b[32m      4\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEnsured subgraph directory exists: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSUBGRAPH_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting load_and_prepare_data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m data = \u001b[43mload_and_prepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFinished load_and_prepare_data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting compute_road_type_accident_correlation...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[92]\u001b[39m\u001b[32m, line 105\u001b[39m, in \u001b[36mload_and_prepare_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    103\u001b[39m shared_counts = accidents_near_borders.groupby(\u001b[33m'\u001b[39m\u001b[33mLIE_NAME\u001b[39m\u001b[33m'\u001b[39m)[\u001b[33m'\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m'\u001b[39m].sum().reindex(neighborhoods_gdf[\u001b[33m'\u001b[39m\u001b[33mLIE_NAME\u001b[39m\u001b[33m'\u001b[39m], fill_value=\u001b[32m0\u001b[39m)\n\u001b[32m    104\u001b[39m neighborhoods_gdf[\u001b[33m'\u001b[39m\u001b[33maccident_count\u001b[39m\u001b[33m'\u001b[39m] = neighborhoods_gdf[\u001b[33m'\u001b[39m\u001b[33maccident_count\u001b[39m\u001b[33m'\u001b[39m] + shared_counts\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m neighborhoods_gdf[\u001b[33m'\u001b[39m\u001b[33maccident_count\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mneighborhoods_gdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maccident_count\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m tree_counts = gpd.sjoin(neighborhoods_gdf, trees_gdf, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m, predicate=\u001b[33m'\u001b[39m\u001b[33mcontains\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    108\u001b[39m tree_counts = tree_counts.groupby(level=\u001b[32m0\u001b[39m).size().reindex(neighborhoods_gdf.index, fill_value=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rapids_wsl/lib/python3.12/site-packages/pandas/core/generic.py:6643\u001b[39m, in \u001b[36mNDFrame.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m   6637\u001b[39m     results = [\n\u001b[32m   6638\u001b[39m         ser.astype(dtype, copy=copy, errors=errors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()\n\u001b[32m   6639\u001b[39m     ]\n\u001b[32m   6641\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6642\u001b[39m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6643\u001b[39m     new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6644\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes)\n\u001b[32m   6645\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mastype\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rapids_wsl/lib/python3.12/site-packages/pandas/core/internals/managers.py:430\u001b[39m, in \u001b[36mBaseBlockManager.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[32m    428\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mastype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rapids_wsl/lib/python3.12/site-packages/pandas/core/internals/managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rapids_wsl/lib/python3.12/site-packages/pandas/core/internals/blocks.py:758\u001b[39m, in \u001b[36mBlock.astype\u001b[39m\u001b[34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[39m\n\u001b[32m    755\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan not squeeze with more than one column.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    756\u001b[39m     values = values[\u001b[32m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m new_values = \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m new_values = maybe_coerce_values(new_values)\n\u001b[32m    762\u001b[39m refs = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rapids_wsl/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:237\u001b[39m, in \u001b[36mastype_array_safe\u001b[39m\u001b[34m(values, dtype, copy, errors)\u001b[39m\n\u001b[32m    234\u001b[39m     dtype = dtype.numpy_dtype\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     new_values = \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rapids_wsl/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:182\u001b[39m, in \u001b[36mastype_array\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    179\u001b[39m     values = values.astype(dtype, copy=copy)\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     values = \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np.dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rapids_wsl/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:101\u001b[39m, in \u001b[36m_astype_nansafe\u001b[39m\u001b[34m(arr, dtype, copy, skipna)\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.ensure_string_array(\n\u001b[32m     97\u001b[39m         arr, skipna=skipna, convert_na_value=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     98\u001b[39m     ).reshape(shape)\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m np.issubdtype(arr.dtype, np.floating) \u001b[38;5;129;01mand\u001b[39;00m dtype.kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33miu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_astype_float_to_int_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m arr.dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# if we have a datetime/timedelta array of objects\u001b[39;00m\n\u001b[32m    105\u001b[39m     \u001b[38;5;66;03m# then coerce to datetime64[ns] and use DatetimeArray.astype\u001b[39;00m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m lib.is_np_dtype(dtype, \u001b[33m\"\u001b[39m\u001b[33mM\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rapids_wsl/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:145\u001b[39m, in \u001b[36m_astype_float_to_int_nansafe\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[33;03mastype with a check preventing converting NaN to an meaningless integer value.\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.isfinite(values).all():\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m IntCastingNaNError(\n\u001b[32m    146\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot convert non-finite values (NA or inf) to integer\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    147\u001b[39m     )\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype.kind == \u001b[33m\"\u001b[39m\u001b[33mu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    149\u001b[39m     \u001b[38;5;66;03m# GH#45151\u001b[39;00m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (values >= \u001b[32m0\u001b[39m).all():\n",
      "\u001b[31mIntCastingNaNError\u001b[39m: Cannot convert non-finite values (NA or inf) to integer"
     ]
    }
   ],
   "source": [
    "def main(force_recompute_graph=False):\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    os.makedirs(SUBGRAPH_DIR, exist_ok=True)\n",
    "    logging.info(f\"Ensured subgraph directory exists: {SUBGRAPH_DIR}\")\n",
    "\n",
    "    print(\"Starting load_and_prepare_data...\")\n",
    "    data = load_and_prepare_data()\n",
    "    print(\"Finished load_and_prepare_data\")\n",
    "\n",
    "    print(\"Starting compute_road_type_accident_correlation...\")\n",
    "    road_accident_summary = compute_road_type_accident_correlation(\n",
    "        data['roads'], data['neighborhoods'])\n",
    "    print(\"Finished compute_road_type_accident_correlation\")\n",
    "\n",
    "    print(\"Starting build_graph...\")\n",
    "    G = build_graph(data, force_recompute=force_recompute_graph)\n",
    "    print(\"Finished build_graph\")\n",
    "\n",
    "    print(\"Starting compute_walkability_scores...\")\n",
    "    G = compute_walkability_scores(G)\n",
    "    print(\"Finished compute_walkability_scores\")\n",
    "\n",
    "    print(\"Starting prepare_gnn_data...\")\n",
    "    data_gnn = prepare_gnn_data(G)\n",
    "    print(\"Finished prepare_gnn_data\")\n",
    "\n",
    "    print(\"Starting train_gnn_model...\")\n",
    "    model = train_gnn_model(data_gnn)\n",
    "    print(\"Finished train_gnn_model\")\n",
    "\n",
    "    print(\"Starting predict_walkability...\")\n",
    "    G = predict_walkability(G, model)\n",
    "    print(\"Finished predict_walkability\")\n",
    "\n",
    "    print(\"Starting create_interactive_map...\")\n",
    "    create_interactive_map(G, data)\n",
    "    print(\"Finished create_interactive_map\")\n",
    "\n",
    "    logging.info(\"Processing complete.\")\n",
    "    print(G.edgelist.edgelist_df.to_pandas().head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(force_recompute_graph=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
